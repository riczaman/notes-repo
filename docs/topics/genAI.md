**==Fundamentals of Large Language Models==**
##Basics of LLMs
   - `Language Model` = probablilstic model of text. Large LMs just refers to the number of parameters (large)
   - `Decoding` = Term for generating text from an LLM
   - `Prompting` = Affects the distribution of the LLMs vocabulary but it does not change the models parameters
   - `Training` = Affects the distribution and changes the models parameters. 

##LLM Architectures
   1. **==Encoders==** they are used for `embedding`
   2. **==Decoders==** they are used for ` text generation`
   - Capabilities can be embedding or text generation 
   - All models are built using the `Transformer` Architecture. 
   - `Embedding` = convert a sequence of word into a vector or sequence of vectors. Basically, embedding converts the text into a numerical representation of the text with meaning.
   - `Generation` = Generate text based on a sequence of input words. 

   