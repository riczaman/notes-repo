*The AI stack consists of: Generative AI, Deep Learning, Machine Learning, and Artificial Intelligence*

- ==**Artificial Intelligence**==: Programming machines to imitate human intelligence

- ==**Machine Learning**==: Subset of AI where alogorithms are used to learn from past data and predict outcomes on new data or identify trends

- ==**Deep Learning**==: A subset of machine learning where algorithms are modelled to learn from complex data using neural networks.

- ==**Generative AI**==: A type of AI that creates new content

---

##AI Foundations

- ==**Artificial General Intelligence**==: Is machines being able to replicate human intelligence capabilities like motor skills, learning, and intelligence. 
- When you apply AGI to specific and narrow objectives then you get ==**Artificial Intelligence**==

- 2 major reasons why we need AI:
   - `Automation & Decision Making`
   - `Creative Support` 

- Commonly Used AI Domains:

   1. Language: `Text-related AI tasks` use text as the input. `Generative AI tasks` the output text is generated by a model (ChatGPT). 
      - ==**Text as Data**==: Inherently Sequential = Sentences, multiple words = tokenization, varying sentence legnths = padding, and similair words = dot or cosine similarity and embedding.
      - `Language AI Models` = designed to understand, process, and generate natural language. (NLP)
      - Deep Learning Models that are used for NLP are: ==**Recurrent Neural Networks**== which process data sequentiall and stores hidden state, ==**Long Short-Term Memory**== which process data sequentially and can retain the context betther through use of gates, and ==**Transformers**== which process data in parallel by using concepts of self attention to better understand the context. 

   2. Audio & Speech: Can be either `Audio-Related` or `Generative AI`. 
      - ==**Audio & Speech as Data**==: Digitized snapshots in time like a sample rate, sampling rate of 44.1kHz, bit depth is the number of bits in each 44.1kHz of data. 
      - `Audio & Speech AI Models` = designed to process and manipulate audio and speech. 
      - Deep Learning models are: ==**Recurrent Neural Networks**, ==**Long Short-Term Memory**==, and ==**Transformers**==, ==**Variational Autoencoders**==, ==**Waveform Models**==, & ==**Siamese Networks**==

   3. Vision: Can be `Image Related` or `Generative AI`. 
      - ==**Image as Data**==: Images consist of pixels which can be grey scale or colour. 
      - `Vision AI models` = designed to process and understand visual information from images and videos
      - Deep Learning Models: ==**Convolutional Neural Networks**== which detect patterns in images, learning hierarchial representations of visual features and ==**YOLO**== which process the image and detects objects within the image, and ==**Generative Adversarial Network**== which generates real-looking images. 

###OCI AI Services
1. `Vision AI Services` allows us to do the following:
   - `Image Classification`: Upload an image which gets analyzed and labelled with confidence scores. 
   - `Object Detection`: Upload the image and then it detects objects with confidence scores.
   - `Text Detection`: upload an image and it extracts all the text from the image. 
   - `Document AI`: Upload a document and then it gives you the raw text and then assigns key value pairs and it extracts tables. 

2. `Language AI Services`:
   - `Text Analytics`: Analyzes a block of text and provides us language detection, text classfication, extracts entities, key phrase extractions and sentiment analysis. Also personal identifiable information
   - `Text Translation`: Translates text from one language to another. 

###AI vs ML vs DL
- Machine Learning Types:
   - `Supervised`: Extracting rules from labelled data. For example: Like credit card applications that use a rules engine. `Learning from labelled data`. 
   - `Unsupervised`: Extracting trends from unlabelled data. Grouping similair data into clusters like retail marketing and sales.
   - `Reinforcement`: Solving tasks by trial and error. 

- Deep Learning is used extracting features and rules from data and it uses neural networks with multiple layers. 

---

##Machine Learning Foundations

- ML provides statidstical tools to analyze, visualize, and make predictions from data like Netflix movie suggestions. 

- ML uses `input features` to describe what the `output label` should be. Train the model with the input features and then when the model is trained we can apply ==**inference**== which is the ability to predic the label. 

- Types of Machine Learning:
   - Supervised which uses labeled data, unsupervised where we just understand relationships and reinforcement which make decisions. 
   - Supervised examples include: disease detection, weather forecasting, stock price prediction, spam detection
   - Unsupervised examples: Fradulent transactions, outlier detection and targeted marketing campaigns 
   - Reinforcement: automated robots, autonomous cars, healthcare and video games

##==**Supervised Learning: Classification**==
- There are 2 types of output labels:
   - `Continous` - This leads to `Regression`
   - `Categorical` - This leads to `Classification` and these can be `binary` or `multi-class` (different types of the same thing like 3 different species of a flower)

- Classification = a supervised mL technique used to categorize or assign data points into predefined classes based on their features or attributes. 
- They train using a labelled data set

Machine Learning Algorithm for classfication is: ==**Logistic Regression**==: helps in predicting if something is true or false. Logistic regression uses an S-sjaped curve (sigmoid) for the data as opposed to linear regression.

##==**Supervised Learning: Regression**==
- Independent features are the labeled input and the dependent feature is the output label
- Uses linear regression 
- loss is a number indicating how far the predicted value is from the actual value

---

##Jupyter Notebooks
- `Anaconda` is an open source python and R for data science and machine learning. It helps with package management and deployment. Within Anaconda is `Jupyter Notebooks` which is an IDE that allows you to share documents. 
- This opens up a terminal view on localhost of your files. 

- Machine Level Process consists of: 
   1. Loading Data
   2. Preprocessing - this involves creating features and labels
   3. Training a Model
   4. Evaluating the Model
   5. Making Predictions 

- Important ML Libraries in Python from `Sklearn`
   1. `train_test_split` - this module is used to split the data into 2 sets one where you can train the model and other set to test
   2. `StandardScalar` - Process of transforming data so it has a mean of 0 and a standard deviation of 1 and it make sures all features use the same scale. if. square foot vs number of beds when predicting the price of a house. 
   3. `accuracy_score`- gives you an prediction of how strong the prediction is (classification)

- `Unsupervised Learning`:
   - There are no labelled outputs 
   - Algorithms learn the patterns in the data and group similair data items together. 

   - ==**Clustering**==: is the grouping of simialir data items
   - ==**Similarity**==: is how close two data points are to each other and is a value between 0 and 1 and this determines what cluster objects belong too.

   - Unsupervised Workflow:
      1. Prepare the data (remove missing values and normalize)
      2. Create similarity metrics
      3. Run the clustering algorithm (parition, density, hierarchial and distribution)
      4. Interpret results and adjust clusternig 

- `Reinforcement Learning`:
   - type of machine learning that enables an agent to learn from its interactions within the enviornment.
   - agent = interacts with the enviornment and takes action and learns from feedback
   - enviornment = external systems with which the agent interacts
   - state = representation of the current situation of the enviornment
   - action = possible moves or decisions that the agent can take
   - policy = mapping that the agent uses to devide which action to take

   - `optimal policy` = finding the policy that yields a lot of rewards. The algorithms used are: ==**Q learning or Deep Q learning**==

---

##Deep Learning Foundations
- A subset of machine learning that focuses on training ==**Artificial Neural Networks (ANNs)**== with multiple layers 

- ML needs us to specify features wheres in `Deep Learning extracts features from raw and complex data and DL algorithms allow parallel processing of data so it has better scalability and performance`. 
- The use of GPUs were needed for this complex learning and machine algorithms

- Types of Deep Learning algorithms can be broken down into two types: `Data` (Images, videos, text, audio) and the `applications` (image classification, face detection, NLP) 

1. Deep Learning Alorithm for `images` is ==**Convolutional Neural Networks (CNN)**==

2. For `text` we use ==**Transformers, Long-Short Term Memory (LSTM) or Recurrent Neural Networks (RNN)**==

3. For `images, audio and text generation` we can use: ==**Transformers, Difussion models, and Generative Adversarial networks (GAN)**==

- Building Blocks of ANN:
   1. `Layers` which are inputs and hidden layers 
   2. `Neurons` are computantional units that accept input and produce an output and applies ther activation function to generate output
   3. `Weights` which determine the strength of connection between neurons
   4. `Activation Function` work on the weighted sum of inputs to a neuron to produce an output
   5. `Bias`: is additional input to a neuron that allows a certain degree of flexibility 

- ANNs are trained using ==**Backpropgation Algorithm**== which is guessing and comparing and then measuring the error. Then the weights are adjusted and then weights are updated. Repeat and Learng Model training method

- Deep Learning Models for `Sequence Models`
   - Sequence models are input data in the form of sequences and the goal is to find patterns and make predictions. Like NLP, speech recognition, gesture recognition, etc

   -==**Recurrent Neural Networks (RNN)**== handle sequential data and there is a feedback loop and it can maintain a hidden state or memory and it updates as each element in the sequence is processed. So it can capture dependencies 
      -Architecture:
         1. `One to One`: used for non sequential data
         2. `One to Many`: music generation or sequence generation
         3. `Many to one`: sentiment analysis 
         4. `Many to Many`: machine translation and entity recognition. 
    
    -==**Long Short Term Memory**==: Works by using a specialized memory cell and gating mechanisms to capture long term dependencies which RNN is not good at. 
    - Input processing at step 1, then it recieves the previous memory hidden state value then there is a gating mechanism (input gate, forget gate, and output gate) then it updates the memory and then it produced output generation. 

    -==**Convolutional Neural Networks**==: 
       -Deep Learning Models: 
           1. `Feed Forward Neural Networks (FNN)` - multi layer perception MLP (simplest)
           2. `Convolutional Neural Networks (CNN)` - good for image and video 
           3. `Recurrent Neural Networks (RNN)` - good  for time series and sequential data
           4. `Autoencoders` - are unsupervised learning models used for feature extraction.
           5. `Long Short Term Memory` - specilized RNN for long term dependencies
           6. Generative Adversarial Network` (GAN) producing images and content 
           7. `Transformers` which is used for language processing 

        - CNN: processes grid like data like images and videos. CNN works good with 2D data by reducing images into an easier to process form. 

        - CNN Layers:
           1. Input Layer 
           2. Feature Extraction layers: This layer is to automatically learn and extract patterns from the input images
              -convolutional layers (uses small filters kernels)
              -activation function allows the network to learn more complex non linear data
              -pooling layer reduces computational complexity
              -fully connected layer
              -softmax layer
              -dropout layer
           3. Classification Layers

        - Limitations of CNN:
           - Computation: needs alot of data and compute 
           - Overfitting: happens with limited traning data
           -Interpretiability: black box models
           -Sensitivity: sensitive to input variations 

        - Main use of CNN is image classification, object detection, image segmentation, face recognition. 

---

##Generative AI & LLM Foundations
