
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Personal Technical Notes">
      
      
        <meta name="author" content="Ricky Zaman">
      
      
        <link rel="canonical" href="https://riczaman.github.io/notes-repo/topics/adf/">
      
      
        <link rel="prev" href="../scripts/">
      
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>ADF - Ricky's Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Raleway";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-orange" data-md-color-accent="green">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#copy-activity-need-your-data-set-a-connection-from-your-source-to-the-copy-activity-a-connection-from-your-copy-activity-to-the-destination-go-to-the-author-tab-data-flows-are-used-to-transform-data-power-query-can-also-be-used-to-transform-data-the-term-sink-destination-in-azure-data-factory-to-see-your-storage-make-sure-to-go-to-the-storage-account-and-then-click-on-data-storage-containers-click-on-pipelines-click-on-new-pipeline-search-for-the-copy-activity-if-your-data-is-in-the-storage-account-then-your-source-snd-sink-should-be-azure-data-lake-storage-2-and-then-select-the-type-of-data-so-in-this-case-its-csv-on-this-screen-you-can-choose-the-directory-of-where-the-file-resides-and-if-it-doesnt-already-exist-you-can-create-folders-by-supplying-a-name-you-also-need-to-have-a-previously-setup-linked-service-so-that-you-can-supply-the-linked-service-data-location-in-this-source-and-sink-fields-you-usually-select-none-for-the-schema-type-running-in-debug-mode-will-run-the-full-activity-which-will-copy-the-file-from-your-source-to-dest-you-monitor-the-pipeline-status-to-see-if-its-working-copying-data-from-an-apihttp-connection-to-data-lake-storage-account-first-thing-you-have-to-do-is-create-a-linked-service-to-get-data-from-your-apihttp-activity-so-when-you-create-a-linked-service-choose-http-rest-api-is-another-option-if-its-a-csv-on-github-make-sure-you-view-it-as-raw-and-then-provide-this-url-only-up-to-com-as-the-base-url-the-rest-of-the-url-is-the-data-set-since-github-repo-is-public-you-can-set-the-authentication-type-as-anonymous-then-go-create-the-new-pipeline-it-will-only-be-the-copy-activity-in-the-source-you-would-click-new-and-then-http-and-then-provide-the-github-linked-service-then-you-can-provide-the-relative-url-rest-of-the-url-after-the-base-and-then-one-good-thing-to-do-is-preview-data-to-see-if-it-can-actually-fetch-your-data-then-setup-the-sink-using-the-previous-linked-service-to-your-data-lake-you-can-edit-settings-by-clicking-on-the-open-option-beside-the-parameter-its-important-to-note-that-you-should-specify-the-file-name-or-it-will-copy-the-whole-github-repo-hierarchy-publishing-all-this-saves-all-your-work-get-metadata-activity-you-can-sort-files-into-different-containers-using-this-activity-use-case-would-be-to-create-a-dataset-that-contains-all-files-in-the-directory-that-you-want-to-sort-you-also-have-to-supply-the-field-list-child-items-will-cover-all-files-within-a-folder-after-you-run-the-activity-you-can-see-the-pipeline-in-the-bottom-under-the-successful-status-and-just-click-on-the-see-output-button-the-output-of-this-activity-is-an-array-if-condition-activity-allows-you-to-run-if-conditions-for-each-activity-allows-your-to-loop-through-data-sorting-files-in-a-container-and-moving-them-to-a-different-one-if-they-dont-match-each-activity-in-adf-has-4-options-connections-on-error-on-success-on-completion-this-is-how-you-connect-activities-to-each-other-start-with-a-get-metadata-activity-to-get-an-array-of-files-with-properties-then-you-need-a-for-loop-to-loop-through-each-file-in-the-array-to-get-the-output-variable-you-have-two-options-you-can-use-the-pipeline-expression-builder-to-build-out-code-or-in-the-activity-you-select-the-add-dynamic-content-and-this-has-a-list-of-a-built-in-outputs-that-you-can-retrieve-into-a-variable-so-in-our-case-it-would-be-the-get-output-metadata-but-you-need-to-make-sure-you-specify-that-you-just-want-the-child-items-because-you-dont-want-the-whole-object-just-the-child-items-array-to-perform-activities-within-the-foreach-you-click-on-the-for-each-go-to-the-activities-tab-then-click-on-the-pencil-button-anytime-you-want-to-use-output-from-another-activity-you-just-need-to-add-the-dynamic-content-pipeline-expression-builder-also-contains-a-bunch-of-functions-you-can-leverage-like-string-functions-within-the-for-each-you-need-to-start-with-an-if-activity-the-expression-for-this-should-be-dynamic-content-each-for-itemstartswith-parameterized-datasets-basically-in-our-copy-activity-for-each-file-that-meets-the-condition-it-needs-to-be-copied-but-in-our-current-setup-we-hardcode-the-file-name-which-wont-work-since-file-names-will-differ-so-we-need-to-capture-the-filename-in-a-dynamic-variable-parameter-to-do-this-when-setting-your-properties-click-on-advanced-and-then-click-on-open-dataset-from-here-you-can-see-the-tab-called-parameters-where-you-can-create-the-parameter-then-in-the-file-path-add-dynamic-content-and-select-the-parameter-you-created-when-referring-to-dynamic-variables-you-need-the-prefix-if-you-get-a-bad-request-when-trying-to-run-the-pipeline-that-means-you-have-an-error-somewhere-in-your-code-even-if-your-validation-passes-dataflow-activities-uses-spark-to-transform-data-within-your-pipeline-go-to-settings-then-setup-the-source-first-to-use-spark-you-need-to-enable-the-data-flow-debug-option-on-after-you-enable-the-cluster-but-turning-data-flow-debug-on-then-you-need-to-go-to-the-projection-tab-and-import-the-projection-which-is-the-schema-the-data-preview-shows-you-all-of-the-fields-in-your-table-and-their-type-you-click-on-the-small-plus-button-beside-the-source-activity-of-the-dataflow-activity-then-you-can-select-what-type-of-transformation-you-want-to-do-transformation-is-basically-like-doing-a-query-on-the-data-select-you-can-check-box-any-fields-you-want-to-remove-filter-use-the-expression-query-to-filter-out-data-you-dont-want-conditional-split-splits-data-based-on-the-condition-replacing-null-derived-column-and-then-select-the-column-and-in-the-expression-you-can-use-the-coalesce-function-which-filters-out-null-and-puts-in-the-second-string-argument-you-provide-group-by-aggregate-writing-the-data-select-the-sink-destination-as-the-last-activity-in-the-flow-before-writing-to-sink-you-should-always-use-the-alter-rows-activity-first-alter-condition-insert-if-will-only-insert-the-data-based-on-the-condition-use-case-take-your-data-select-it-all-but-remove-2-misc-columns-then-filter-out-cust-id-2-then-conditionally-split-on-payment-and-in-amex-get-rid-of-nulls-then-group-the-main-split-on-customer-id-and-find-the-highest-product-id-trigger-activities-on-the-main-pipeline-screen-just-click-on-add-trigger-when-setting-up-the-trigger-select-an-end-date-to-see-if-the-trigger-actually-got-kicked-off-go-to-the-left-hand-menu-and-click-on-the-managed-tab-then-click-on-triggers-under-the-monitor-tab-you-can-see-the-pipeline-runs-set-variable-activity-used-to-store-variables-that-can-be-used-somewhere-else-in-the-pipeline-downstream-you-can-set-variables-in-the-pipeline-by-clicking-on-the-blank-space-and-then-going-to-the-variables-tab-and-then-create-them-here-then-you-use-the-set-variable-activity-to-set-the-value-pipeline-return-value-types-are-variables-that-can-be-used-in-other-pipelines-storage-events-trigger-a-trigger-based-on-an-even-as-opposed-to-in-a-timely-fashion-storage-accounts-specifically-get-triggered-by-changes-in-the-data-storage-repo-you-provide-if-the-file-stays-in-the-same-initial-repo-the-trigger-will-keep-firing-off-so-you-need-to-delete-the-file-after-the-trigger-runs-delete-activity-to-use-this-storage-event-trigger-you-need-to-register-this-by-managing-your-subscription-subscriptions-resource-providers-microsofteventgrid-execute-pipeline-this-activity-lets-you-execute-multiple-pipelines-together" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Ricky&#39;s Notes" class="md-header__button md-logo" aria-label="Ricky's Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ricky's Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ADF
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/riczaman/notes-repo" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Ricky&#39;s Notes" class="md-nav__button md-logo" aria-label="Ricky's Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Ricky's Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/riczaman/notes-repo" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Topics
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Topics
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jenkins/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Jenkins
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Docker
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../actions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    GitHub Actions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../aiFoundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Oracle AI Foundations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../genAI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Oracle Generative AI Professional
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scripts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Scripts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    ADF
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    ADF
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#copy-activity-need-your-data-set-a-connection-from-your-source-to-the-copy-activity-a-connection-from-your-copy-activity-to-the-destination-go-to-the-author-tab-data-flows-are-used-to-transform-data-power-query-can-also-be-used-to-transform-data-the-term-sink-destination-in-azure-data-factory-to-see-your-storage-make-sure-to-go-to-the-storage-account-and-then-click-on-data-storage-containers-click-on-pipelines-click-on-new-pipeline-search-for-the-copy-activity-if-your-data-is-in-the-storage-account-then-your-source-snd-sink-should-be-azure-data-lake-storage-2-and-then-select-the-type-of-data-so-in-this-case-its-csv-on-this-screen-you-can-choose-the-directory-of-where-the-file-resides-and-if-it-doesnt-already-exist-you-can-create-folders-by-supplying-a-name-you-also-need-to-have-a-previously-setup-linked-service-so-that-you-can-supply-the-linked-service-data-location-in-this-source-and-sink-fields-you-usually-select-none-for-the-schema-type-running-in-debug-mode-will-run-the-full-activity-which-will-copy-the-file-from-your-source-to-dest-you-monitor-the-pipeline-status-to-see-if-its-working-copying-data-from-an-apihttp-connection-to-data-lake-storage-account-first-thing-you-have-to-do-is-create-a-linked-service-to-get-data-from-your-apihttp-activity-so-when-you-create-a-linked-service-choose-http-rest-api-is-another-option-if-its-a-csv-on-github-make-sure-you-view-it-as-raw-and-then-provide-this-url-only-up-to-com-as-the-base-url-the-rest-of-the-url-is-the-data-set-since-github-repo-is-public-you-can-set-the-authentication-type-as-anonymous-then-go-create-the-new-pipeline-it-will-only-be-the-copy-activity-in-the-source-you-would-click-new-and-then-http-and-then-provide-the-github-linked-service-then-you-can-provide-the-relative-url-rest-of-the-url-after-the-base-and-then-one-good-thing-to-do-is-preview-data-to-see-if-it-can-actually-fetch-your-data-then-setup-the-sink-using-the-previous-linked-service-to-your-data-lake-you-can-edit-settings-by-clicking-on-the-open-option-beside-the-parameter-its-important-to-note-that-you-should-specify-the-file-name-or-it-will-copy-the-whole-github-repo-hierarchy-publishing-all-this-saves-all-your-work-get-metadata-activity-you-can-sort-files-into-different-containers-using-this-activity-use-case-would-be-to-create-a-dataset-that-contains-all-files-in-the-directory-that-you-want-to-sort-you-also-have-to-supply-the-field-list-child-items-will-cover-all-files-within-a-folder-after-you-run-the-activity-you-can-see-the-pipeline-in-the-bottom-under-the-successful-status-and-just-click-on-the-see-output-button-the-output-of-this-activity-is-an-array-if-condition-activity-allows-you-to-run-if-conditions-for-each-activity-allows-your-to-loop-through-data-sorting-files-in-a-container-and-moving-them-to-a-different-one-if-they-dont-match-each-activity-in-adf-has-4-options-connections-on-error-on-success-on-completion-this-is-how-you-connect-activities-to-each-other-start-with-a-get-metadata-activity-to-get-an-array-of-files-with-properties-then-you-need-a-for-loop-to-loop-through-each-file-in-the-array-to-get-the-output-variable-you-have-two-options-you-can-use-the-pipeline-expression-builder-to-build-out-code-or-in-the-activity-you-select-the-add-dynamic-content-and-this-has-a-list-of-a-built-in-outputs-that-you-can-retrieve-into-a-variable-so-in-our-case-it-would-be-the-get-output-metadata-but-you-need-to-make-sure-you-specify-that-you-just-want-the-child-items-because-you-dont-want-the-whole-object-just-the-child-items-array-to-perform-activities-within-the-foreach-you-click-on-the-for-each-go-to-the-activities-tab-then-click-on-the-pencil-button-anytime-you-want-to-use-output-from-another-activity-you-just-need-to-add-the-dynamic-content-pipeline-expression-builder-also-contains-a-bunch-of-functions-you-can-leverage-like-string-functions-within-the-for-each-you-need-to-start-with-an-if-activity-the-expression-for-this-should-be-dynamic-content-each-for-itemstartswith-parameterized-datasets-basically-in-our-copy-activity-for-each-file-that-meets-the-condition-it-needs-to-be-copied-but-in-our-current-setup-we-hardcode-the-file-name-which-wont-work-since-file-names-will-differ-so-we-need-to-capture-the-filename-in-a-dynamic-variable-parameter-to-do-this-when-setting-your-properties-click-on-advanced-and-then-click-on-open-dataset-from-here-you-can-see-the-tab-called-parameters-where-you-can-create-the-parameter-then-in-the-file-path-add-dynamic-content-and-select-the-parameter-you-created-when-referring-to-dynamic-variables-you-need-the-prefix-if-you-get-a-bad-request-when-trying-to-run-the-pipeline-that-means-you-have-an-error-somewhere-in-your-code-even-if-your-validation-passes-dataflow-activities-uses-spark-to-transform-data-within-your-pipeline-go-to-settings-then-setup-the-source-first-to-use-spark-you-need-to-enable-the-data-flow-debug-option-on-after-you-enable-the-cluster-but-turning-data-flow-debug-on-then-you-need-to-go-to-the-projection-tab-and-import-the-projection-which-is-the-schema-the-data-preview-shows-you-all-of-the-fields-in-your-table-and-their-type-you-click-on-the-small-plus-button-beside-the-source-activity-of-the-dataflow-activity-then-you-can-select-what-type-of-transformation-you-want-to-do-transformation-is-basically-like-doing-a-query-on-the-data-select-you-can-check-box-any-fields-you-want-to-remove-filter-use-the-expression-query-to-filter-out-data-you-dont-want-conditional-split-splits-data-based-on-the-condition-replacing-null-derived-column-and-then-select-the-column-and-in-the-expression-you-can-use-the-coalesce-function-which-filters-out-null-and-puts-in-the-second-string-argument-you-provide-group-by-aggregate-writing-the-data-select-the-sink-destination-as-the-last-activity-in-the-flow-before-writing-to-sink-you-should-always-use-the-alter-rows-activity-first-alter-condition-insert-if-will-only-insert-the-data-based-on-the-condition-use-case-take-your-data-select-it-all-but-remove-2-misc-columns-then-filter-out-cust-id-2-then-conditionally-split-on-payment-and-in-amex-get-rid-of-nulls-then-group-the-main-split-on-customer-id-and-find-the-highest-product-id-trigger-activities-on-the-main-pipeline-screen-just-click-on-add-trigger-when-setting-up-the-trigger-select-an-end-date-to-see-if-the-trigger-actually-got-kicked-off-go-to-the-left-hand-menu-and-click-on-the-managed-tab-then-click-on-triggers-under-the-monitor-tab-you-can-see-the-pipeline-runs-set-variable-activity-used-to-store-variables-that-can-be-used-somewhere-else-in-the-pipeline-downstream-you-can-set-variables-in-the-pipeline-by-clicking-on-the-blank-space-and-then-going-to-the-variables-tab-and-then-create-them-here-then-you-use-the-set-variable-activity-to-set-the-value-pipeline-return-value-types-are-variables-that-can-be-used-in-other-pipelines-storage-events-trigger-a-trigger-based-on-an-even-as-opposed-to-in-a-timely-fashion-storage-accounts-specifically-get-triggered-by-changes-in-the-data-storage-repo-you-provide-if-the-file-stays-in-the-same-initial-repo-the-trigger-will-keep-firing-off-so-you-need-to-delete-the-file-after-the-trigger-runs-delete-activity-to-use-this-storage-event-trigger-you-need-to-register-this-by-managing-your-subscription-subscriptions-resource-providers-microsofteventgrid-execute-pipeline-this-activity-lets-you-execute-multiple-pipelines-together" class="md-nav__link">
    <span class="md-ellipsis">
      
        Copy Activity Need your data set A connection from your source to the copy activity A connection from your copy activity to the destination Go to the Author Tab Data flows are used to transform data Power Query can also be used to transform data The term Sink = Destination in Azure Data Factory To see your storage make sure to go to the storage account and then click on data storage &gt; containers Click on Pipelines Click on new pipeline Search for the copy activity If your data is in the storage account then your source snd sink should be azure data lake storage 2 and then select the type of data so in this case its csv On this screen you can choose the directory of where the file resides and if it doesn’t already exist you can create folders by supplying a name You also need to have a previously setup linked service so that you can supply the linked service data location in this source and sink fields You usually select none for the schema type Running in debug mode will run the full activity which will copy the file from your source to dest. You monitor the pipeline status to see if its working Copying Data from an API/HTTP Connection to Data Lake (Storage Account) First thing you have to do is create a linked service to get data from your api/http activity So when you create a linked service choose HTTP (rest api is another option) If its a csv on GitHub make sure you view it as Raw and then provide this URL (only up to .com) as the base URL The rest of the URL is the data set Since GitHub repo is public you can set the authentication type as anonymous Then go create the new pipeline it will only be the copy activity In the source you would click new and then HTTP and then provide the GitHub linked service Then you can provide the relative URL (rest of the URL after the base) and then one good thing to do is preview data to see if it can actually fetch your data Then setup the sink using the previous linked service to your data lake You can edit settings by clicking on the open option beside the parameter. Its important to note that you should specify the file name or it will copy the whole GitHub repo hierarchy Publishing All This saves all your work Get Metadata Activity You can sort files into different containers using this activity Use case would be to create a dataset that contains all files in the directory that you want to sort You also have to supply the field list Child items will cover all files within a folder After you run the activity you can see the pipeline in the bottom under the successful status and just click on the see output button The output of this activity is an array IF condition activity Allows you to run if conditions For Each Activity Allows your to loop through data Sorting files in a container and moving them to a different one if they don’t match Each activity in ADF has 4 options (Connections - on error, on success, on completion) This is how you connect activities to each other Start with a get metadata activity to get an array of files with properties Then you need a for loop to loop through each file in the array To get the output variable you have two options you can use the pipeline expression builder to build out code or in the activity you select the add dynamic content and this has a list of a built in outputs that you can retrieve into a variable. So in our case it would be the get output metadata but you need to make sure you specify that you just want the child items because you don’t want the whole object just the child items array To perform activities within the foreach you click on the for each go to the activities tab then click on the pencil button Anytime you want to use output from another activity you just need to add the dynamic content Pipeline expression builder also contains a bunch of functions you can leverage like string functions Within the for each you need to start with an if activity The expression for this should be dynamic content (each for item).startswith() Parameterized DataSets Basically in our copy activity for each file that meets the condition it needs to be copied but in our current setup we hardcode the file name which won’t work since file names will differ so we need to capture the filename in a dynamic variable = parameter To do this: When setting your properties click on Advanced and then click on open dataset From here you can see the tab called parameters where you can create the parameter Then in the file path add dynamic content and select the parameter you created When referring to dynamic variables - you need the @ prefix If you get a bad request when trying to run the pipeline that means you have an error somewhere in your code even if your validation passes. Dataflow Activities: Uses spark to transform data within your pipeline Go to settings then setup the source first To use spark you need to enable the data flow debug option on After you enable the cluster but turning data flow debug on then you need to go to the projection tab and import the projection which is the schema The data preview shows you all of the fields in your table and their type You click on the small plus button beside the source activity of the dataflow activity then you can select what type of transformation you want to do. Transformation is basically like doing a query on the data. SELECT you can check box any fields you want to remove FILTER use the expression query to filter out data you don’t want Conditional Split - splits data based on the condition Replacing NULL: Derived Column and then select the column and in the expression you can use the coalesce function which filters out NULL and puts in the second string argument you provide Group By = Aggregate Writing the Data: Select the sink destination as the last activity in the flow Before writing to sink you should always use the ALTER Rows activity first. Alter condition = insert if will only insert the data based on the condition Use case: take your data select it all but remove 2 misc columns then filter out cust id 2 then conditionally split on payment and in amex get rid of nulls then group the main split on customer id and find the highest product id Trigger Activities On the main pipeline screen just click on “Add Trigger” When setting up the trigger select an end date To see if the trigger actually got kicked off go to the left hand menu and click on the Managed Tab then click on triggers Under the monitor tab you can see the pipeline runs Set Variable Activity Used to store variables that can be used somewhere else in the pipeline downstream You can set variables in the pipeline by clicking on the blank space and then going to the variables tab and then create them here Then you use the set variable activity to set the value Pipeline return value types are variables that can be used in other pipelines Storage Events Trigger A trigger based on an even as opposed to in a timely fashion Storage accounts specifically get triggered by changes in the data storage repo you provide If the file stays in the same initial repo the trigger will keep firing off so you need to delete the file after the trigger runs DELETE Activity To use this storage event trigger you need to register this by managing your subscription Subscriptions &gt; resource providers &gt; Microsoft.eventgrid Execute Pipeline This activity lets you execute multiple pipelines together
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#copy-activity-need-your-data-set-a-connection-from-your-source-to-the-copy-activity-a-connection-from-your-copy-activity-to-the-destination-go-to-the-author-tab-data-flows-are-used-to-transform-data-power-query-can-also-be-used-to-transform-data-the-term-sink-destination-in-azure-data-factory-to-see-your-storage-make-sure-to-go-to-the-storage-account-and-then-click-on-data-storage-containers-click-on-pipelines-click-on-new-pipeline-search-for-the-copy-activity-if-your-data-is-in-the-storage-account-then-your-source-snd-sink-should-be-azure-data-lake-storage-2-and-then-select-the-type-of-data-so-in-this-case-its-csv-on-this-screen-you-can-choose-the-directory-of-where-the-file-resides-and-if-it-doesnt-already-exist-you-can-create-folders-by-supplying-a-name-you-also-need-to-have-a-previously-setup-linked-service-so-that-you-can-supply-the-linked-service-data-location-in-this-source-and-sink-fields-you-usually-select-none-for-the-schema-type-running-in-debug-mode-will-run-the-full-activity-which-will-copy-the-file-from-your-source-to-dest-you-monitor-the-pipeline-status-to-see-if-its-working-copying-data-from-an-apihttp-connection-to-data-lake-storage-account-first-thing-you-have-to-do-is-create-a-linked-service-to-get-data-from-your-apihttp-activity-so-when-you-create-a-linked-service-choose-http-rest-api-is-another-option-if-its-a-csv-on-github-make-sure-you-view-it-as-raw-and-then-provide-this-url-only-up-to-com-as-the-base-url-the-rest-of-the-url-is-the-data-set-since-github-repo-is-public-you-can-set-the-authentication-type-as-anonymous-then-go-create-the-new-pipeline-it-will-only-be-the-copy-activity-in-the-source-you-would-click-new-and-then-http-and-then-provide-the-github-linked-service-then-you-can-provide-the-relative-url-rest-of-the-url-after-the-base-and-then-one-good-thing-to-do-is-preview-data-to-see-if-it-can-actually-fetch-your-data-then-setup-the-sink-using-the-previous-linked-service-to-your-data-lake-you-can-edit-settings-by-clicking-on-the-open-option-beside-the-parameter-its-important-to-note-that-you-should-specify-the-file-name-or-it-will-copy-the-whole-github-repo-hierarchy-publishing-all-this-saves-all-your-work-get-metadata-activity-you-can-sort-files-into-different-containers-using-this-activity-use-case-would-be-to-create-a-dataset-that-contains-all-files-in-the-directory-that-you-want-to-sort-you-also-have-to-supply-the-field-list-child-items-will-cover-all-files-within-a-folder-after-you-run-the-activity-you-can-see-the-pipeline-in-the-bottom-under-the-successful-status-and-just-click-on-the-see-output-button-the-output-of-this-activity-is-an-array-if-condition-activity-allows-you-to-run-if-conditions-for-each-activity-allows-your-to-loop-through-data-sorting-files-in-a-container-and-moving-them-to-a-different-one-if-they-dont-match-each-activity-in-adf-has-4-options-connections-on-error-on-success-on-completion-this-is-how-you-connect-activities-to-each-other-start-with-a-get-metadata-activity-to-get-an-array-of-files-with-properties-then-you-need-a-for-loop-to-loop-through-each-file-in-the-array-to-get-the-output-variable-you-have-two-options-you-can-use-the-pipeline-expression-builder-to-build-out-code-or-in-the-activity-you-select-the-add-dynamic-content-and-this-has-a-list-of-a-built-in-outputs-that-you-can-retrieve-into-a-variable-so-in-our-case-it-would-be-the-get-output-metadata-but-you-need-to-make-sure-you-specify-that-you-just-want-the-child-items-because-you-dont-want-the-whole-object-just-the-child-items-array-to-perform-activities-within-the-foreach-you-click-on-the-for-each-go-to-the-activities-tab-then-click-on-the-pencil-button-anytime-you-want-to-use-output-from-another-activity-you-just-need-to-add-the-dynamic-content-pipeline-expression-builder-also-contains-a-bunch-of-functions-you-can-leverage-like-string-functions-within-the-for-each-you-need-to-start-with-an-if-activity-the-expression-for-this-should-be-dynamic-content-each-for-itemstartswith-parameterized-datasets-basically-in-our-copy-activity-for-each-file-that-meets-the-condition-it-needs-to-be-copied-but-in-our-current-setup-we-hardcode-the-file-name-which-wont-work-since-file-names-will-differ-so-we-need-to-capture-the-filename-in-a-dynamic-variable-parameter-to-do-this-when-setting-your-properties-click-on-advanced-and-then-click-on-open-dataset-from-here-you-can-see-the-tab-called-parameters-where-you-can-create-the-parameter-then-in-the-file-path-add-dynamic-content-and-select-the-parameter-you-created-when-referring-to-dynamic-variables-you-need-the-prefix-if-you-get-a-bad-request-when-trying-to-run-the-pipeline-that-means-you-have-an-error-somewhere-in-your-code-even-if-your-validation-passes-dataflow-activities-uses-spark-to-transform-data-within-your-pipeline-go-to-settings-then-setup-the-source-first-to-use-spark-you-need-to-enable-the-data-flow-debug-option-on-after-you-enable-the-cluster-but-turning-data-flow-debug-on-then-you-need-to-go-to-the-projection-tab-and-import-the-projection-which-is-the-schema-the-data-preview-shows-you-all-of-the-fields-in-your-table-and-their-type-you-click-on-the-small-plus-button-beside-the-source-activity-of-the-dataflow-activity-then-you-can-select-what-type-of-transformation-you-want-to-do-transformation-is-basically-like-doing-a-query-on-the-data-select-you-can-check-box-any-fields-you-want-to-remove-filter-use-the-expression-query-to-filter-out-data-you-dont-want-conditional-split-splits-data-based-on-the-condition-replacing-null-derived-column-and-then-select-the-column-and-in-the-expression-you-can-use-the-coalesce-function-which-filters-out-null-and-puts-in-the-second-string-argument-you-provide-group-by-aggregate-writing-the-data-select-the-sink-destination-as-the-last-activity-in-the-flow-before-writing-to-sink-you-should-always-use-the-alter-rows-activity-first-alter-condition-insert-if-will-only-insert-the-data-based-on-the-condition-use-case-take-your-data-select-it-all-but-remove-2-misc-columns-then-filter-out-cust-id-2-then-conditionally-split-on-payment-and-in-amex-get-rid-of-nulls-then-group-the-main-split-on-customer-id-and-find-the-highest-product-id-trigger-activities-on-the-main-pipeline-screen-just-click-on-add-trigger-when-setting-up-the-trigger-select-an-end-date-to-see-if-the-trigger-actually-got-kicked-off-go-to-the-left-hand-menu-and-click-on-the-managed-tab-then-click-on-triggers-under-the-monitor-tab-you-can-see-the-pipeline-runs-set-variable-activity-used-to-store-variables-that-can-be-used-somewhere-else-in-the-pipeline-downstream-you-can-set-variables-in-the-pipeline-by-clicking-on-the-blank-space-and-then-going-to-the-variables-tab-and-then-create-them-here-then-you-use-the-set-variable-activity-to-set-the-value-pipeline-return-value-types-are-variables-that-can-be-used-in-other-pipelines-storage-events-trigger-a-trigger-based-on-an-even-as-opposed-to-in-a-timely-fashion-storage-accounts-specifically-get-triggered-by-changes-in-the-data-storage-repo-you-provide-if-the-file-stays-in-the-same-initial-repo-the-trigger-will-keep-firing-off-so-you-need-to-delete-the-file-after-the-trigger-runs-delete-activity-to-use-this-storage-event-trigger-you-need-to-register-this-by-managing-your-subscription-subscriptions-resource-providers-microsofteventgrid-execute-pipeline-this-activity-lets-you-execute-multiple-pipelines-together" class="md-nav__link">
    <span class="md-ellipsis">
      
        Copy Activity Need your data set A connection from your source to the copy activity A connection from your copy activity to the destination Go to the Author Tab Data flows are used to transform data Power Query can also be used to transform data The term Sink = Destination in Azure Data Factory To see your storage make sure to go to the storage account and then click on data storage &gt; containers Click on Pipelines Click on new pipeline Search for the copy activity If your data is in the storage account then your source snd sink should be azure data lake storage 2 and then select the type of data so in this case its csv On this screen you can choose the directory of where the file resides and if it doesn’t already exist you can create folders by supplying a name You also need to have a previously setup linked service so that you can supply the linked service data location in this source and sink fields You usually select none for the schema type Running in debug mode will run the full activity which will copy the file from your source to dest. You monitor the pipeline status to see if its working Copying Data from an API/HTTP Connection to Data Lake (Storage Account) First thing you have to do is create a linked service to get data from your api/http activity So when you create a linked service choose HTTP (rest api is another option) If its a csv on GitHub make sure you view it as Raw and then provide this URL (only up to .com) as the base URL The rest of the URL is the data set Since GitHub repo is public you can set the authentication type as anonymous Then go create the new pipeline it will only be the copy activity In the source you would click new and then HTTP and then provide the GitHub linked service Then you can provide the relative URL (rest of the URL after the base) and then one good thing to do is preview data to see if it can actually fetch your data Then setup the sink using the previous linked service to your data lake You can edit settings by clicking on the open option beside the parameter. Its important to note that you should specify the file name or it will copy the whole GitHub repo hierarchy Publishing All This saves all your work Get Metadata Activity You can sort files into different containers using this activity Use case would be to create a dataset that contains all files in the directory that you want to sort You also have to supply the field list Child items will cover all files within a folder After you run the activity you can see the pipeline in the bottom under the successful status and just click on the see output button The output of this activity is an array IF condition activity Allows you to run if conditions For Each Activity Allows your to loop through data Sorting files in a container and moving them to a different one if they don’t match Each activity in ADF has 4 options (Connections - on error, on success, on completion) This is how you connect activities to each other Start with a get metadata activity to get an array of files with properties Then you need a for loop to loop through each file in the array To get the output variable you have two options you can use the pipeline expression builder to build out code or in the activity you select the add dynamic content and this has a list of a built in outputs that you can retrieve into a variable. So in our case it would be the get output metadata but you need to make sure you specify that you just want the child items because you don’t want the whole object just the child items array To perform activities within the foreach you click on the for each go to the activities tab then click on the pencil button Anytime you want to use output from another activity you just need to add the dynamic content Pipeline expression builder also contains a bunch of functions you can leverage like string functions Within the for each you need to start with an if activity The expression for this should be dynamic content (each for item).startswith() Parameterized DataSets Basically in our copy activity for each file that meets the condition it needs to be copied but in our current setup we hardcode the file name which won’t work since file names will differ so we need to capture the filename in a dynamic variable = parameter To do this: When setting your properties click on Advanced and then click on open dataset From here you can see the tab called parameters where you can create the parameter Then in the file path add dynamic content and select the parameter you created When referring to dynamic variables - you need the @ prefix If you get a bad request when trying to run the pipeline that means you have an error somewhere in your code even if your validation passes. Dataflow Activities: Uses spark to transform data within your pipeline Go to settings then setup the source first To use spark you need to enable the data flow debug option on After you enable the cluster but turning data flow debug on then you need to go to the projection tab and import the projection which is the schema The data preview shows you all of the fields in your table and their type You click on the small plus button beside the source activity of the dataflow activity then you can select what type of transformation you want to do. Transformation is basically like doing a query on the data. SELECT you can check box any fields you want to remove FILTER use the expression query to filter out data you don’t want Conditional Split - splits data based on the condition Replacing NULL: Derived Column and then select the column and in the expression you can use the coalesce function which filters out NULL and puts in the second string argument you provide Group By = Aggregate Writing the Data: Select the sink destination as the last activity in the flow Before writing to sink you should always use the ALTER Rows activity first. Alter condition = insert if will only insert the data based on the condition Use case: take your data select it all but remove 2 misc columns then filter out cust id 2 then conditionally split on payment and in amex get rid of nulls then group the main split on customer id and find the highest product id Trigger Activities On the main pipeline screen just click on “Add Trigger” When setting up the trigger select an end date To see if the trigger actually got kicked off go to the left hand menu and click on the Managed Tab then click on triggers Under the monitor tab you can see the pipeline runs Set Variable Activity Used to store variables that can be used somewhere else in the pipeline downstream You can set variables in the pipeline by clicking on the blank space and then going to the variables tab and then create them here Then you use the set variable activity to set the value Pipeline return value types are variables that can be used in other pipelines Storage Events Trigger A trigger based on an even as opposed to in a timely fashion Storage accounts specifically get triggered by changes in the data storage repo you provide If the file stays in the same initial repo the trigger will keep firing off so you need to delete the file after the trigger runs DELETE Activity To use this storage event trigger you need to register this by managing your subscription Subscriptions &gt; resource providers &gt; Microsoft.eventgrid Execute Pipeline This activity lets you execute multiple pipelines together
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>ADF</h1>

<h2 id="copy-activity-need-your-data-set-a-connection-from-your-source-to-the-copy-activity-a-connection-from-your-copy-activity-to-the-destination-go-to-the-author-tab-data-flows-are-used-to-transform-data-power-query-can-also-be-used-to-transform-data-the-term-sink-destination-in-azure-data-factory-to-see-your-storage-make-sure-to-go-to-the-storage-account-and-then-click-on-data-storage-containers-click-on-pipelines-click-on-new-pipeline-search-for-the-copy-activity-if-your-data-is-in-the-storage-account-then-your-source-snd-sink-should-be-azure-data-lake-storage-2-and-then-select-the-type-of-data-so-in-this-case-its-csv-on-this-screen-you-can-choose-the-directory-of-where-the-file-resides-and-if-it-doesnt-already-exist-you-can-create-folders-by-supplying-a-name-you-also-need-to-have-a-previously-setup-linked-service-so-that-you-can-supply-the-linked-service-data-location-in-this-source-and-sink-fields-you-usually-select-none-for-the-schema-type-running-in-debug-mode-will-run-the-full-activity-which-will-copy-the-file-from-your-source-to-dest-you-monitor-the-pipeline-status-to-see-if-its-working-copying-data-from-an-apihttp-connection-to-data-lake-storage-account-first-thing-you-have-to-do-is-create-a-linked-service-to-get-data-from-your-apihttp-activity-so-when-you-create-a-linked-service-choose-http-rest-api-is-another-option-if-its-a-csv-on-github-make-sure-you-view-it-as-raw-and-then-provide-this-url-only-up-to-com-as-the-base-url-the-rest-of-the-url-is-the-data-set-since-github-repo-is-public-you-can-set-the-authentication-type-as-anonymous-then-go-create-the-new-pipeline-it-will-only-be-the-copy-activity-in-the-source-you-would-click-new-and-then-http-and-then-provide-the-github-linked-service-then-you-can-provide-the-relative-url-rest-of-the-url-after-the-base-and-then-one-good-thing-to-do-is-preview-data-to-see-if-it-can-actually-fetch-your-data-then-setup-the-sink-using-the-previous-linked-service-to-your-data-lake-you-can-edit-settings-by-clicking-on-the-open-option-beside-the-parameter-its-important-to-note-that-you-should-specify-the-file-name-or-it-will-copy-the-whole-github-repo-hierarchy-publishing-all-this-saves-all-your-work-get-metadata-activity-you-can-sort-files-into-different-containers-using-this-activity-use-case-would-be-to-create-a-dataset-that-contains-all-files-in-the-directory-that-you-want-to-sort-you-also-have-to-supply-the-field-list-child-items-will-cover-all-files-within-a-folder-after-you-run-the-activity-you-can-see-the-pipeline-in-the-bottom-under-the-successful-status-and-just-click-on-the-see-output-button-the-output-of-this-activity-is-an-array-if-condition-activity-allows-you-to-run-if-conditions-for-each-activity-allows-your-to-loop-through-data-sorting-files-in-a-container-and-moving-them-to-a-different-one-if-they-dont-match-each-activity-in-adf-has-4-options-connections-on-error-on-success-on-completion-this-is-how-you-connect-activities-to-each-other-start-with-a-get-metadata-activity-to-get-an-array-of-files-with-properties-then-you-need-a-for-loop-to-loop-through-each-file-in-the-array-to-get-the-output-variable-you-have-two-options-you-can-use-the-pipeline-expression-builder-to-build-out-code-or-in-the-activity-you-select-the-add-dynamic-content-and-this-has-a-list-of-a-built-in-outputs-that-you-can-retrieve-into-a-variable-so-in-our-case-it-would-be-the-get-output-metadata-but-you-need-to-make-sure-you-specify-that-you-just-want-the-child-items-because-you-dont-want-the-whole-object-just-the-child-items-array-to-perform-activities-within-the-foreach-you-click-on-the-for-each-go-to-the-activities-tab-then-click-on-the-pencil-button-anytime-you-want-to-use-output-from-another-activity-you-just-need-to-add-the-dynamic-content-pipeline-expression-builder-also-contains-a-bunch-of-functions-you-can-leverage-like-string-functions-within-the-for-each-you-need-to-start-with-an-if-activity-the-expression-for-this-should-be-dynamic-content-each-for-itemstartswith-parameterized-datasets-basically-in-our-copy-activity-for-each-file-that-meets-the-condition-it-needs-to-be-copied-but-in-our-current-setup-we-hardcode-the-file-name-which-wont-work-since-file-names-will-differ-so-we-need-to-capture-the-filename-in-a-dynamic-variable-parameter-to-do-this-when-setting-your-properties-click-on-advanced-and-then-click-on-open-dataset-from-here-you-can-see-the-tab-called-parameters-where-you-can-create-the-parameter-then-in-the-file-path-add-dynamic-content-and-select-the-parameter-you-created-when-referring-to-dynamic-variables-you-need-the-prefix-if-you-get-a-bad-request-when-trying-to-run-the-pipeline-that-means-you-have-an-error-somewhere-in-your-code-even-if-your-validation-passes-dataflow-activities-uses-spark-to-transform-data-within-your-pipeline-go-to-settings-then-setup-the-source-first-to-use-spark-you-need-to-enable-the-data-flow-debug-option-on-after-you-enable-the-cluster-but-turning-data-flow-debug-on-then-you-need-to-go-to-the-projection-tab-and-import-the-projection-which-is-the-schema-the-data-preview-shows-you-all-of-the-fields-in-your-table-and-their-type-you-click-on-the-small-plus-button-beside-the-source-activity-of-the-dataflow-activity-then-you-can-select-what-type-of-transformation-you-want-to-do-transformation-is-basically-like-doing-a-query-on-the-data-select-you-can-check-box-any-fields-you-want-to-remove-filter-use-the-expression-query-to-filter-out-data-you-dont-want-conditional-split-splits-data-based-on-the-condition-replacing-null-derived-column-and-then-select-the-column-and-in-the-expression-you-can-use-the-coalesce-function-which-filters-out-null-and-puts-in-the-second-string-argument-you-provide-group-by-aggregate-writing-the-data-select-the-sink-destination-as-the-last-activity-in-the-flow-before-writing-to-sink-you-should-always-use-the-alter-rows-activity-first-alter-condition-insert-if-will-only-insert-the-data-based-on-the-condition-use-case-take-your-data-select-it-all-but-remove-2-misc-columns-then-filter-out-cust-id-2-then-conditionally-split-on-payment-and-in-amex-get-rid-of-nulls-then-group-the-main-split-on-customer-id-and-find-the-highest-product-id-trigger-activities-on-the-main-pipeline-screen-just-click-on-add-trigger-when-setting-up-the-trigger-select-an-end-date-to-see-if-the-trigger-actually-got-kicked-off-go-to-the-left-hand-menu-and-click-on-the-managed-tab-then-click-on-triggers-under-the-monitor-tab-you-can-see-the-pipeline-runs-set-variable-activity-used-to-store-variables-that-can-be-used-somewhere-else-in-the-pipeline-downstream-you-can-set-variables-in-the-pipeline-by-clicking-on-the-blank-space-and-then-going-to-the-variables-tab-and-then-create-them-here-then-you-use-the-set-variable-activity-to-set-the-value-pipeline-return-value-types-are-variables-that-can-be-used-in-other-pipelines-storage-events-trigger-a-trigger-based-on-an-even-as-opposed-to-in-a-timely-fashion-storage-accounts-specifically-get-triggered-by-changes-in-the-data-storage-repo-you-provide-if-the-file-stays-in-the-same-initial-repo-the-trigger-will-keep-firing-off-so-you-need-to-delete-the-file-after-the-trigger-runs-delete-activity-to-use-this-storage-event-trigger-you-need-to-register-this-by-managing-your-subscription-subscriptions-resource-providers-microsofteventgrid-execute-pipeline-this-activity-lets-you-execute-multiple-pipelines-together"><div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>Copy Activity
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>Need your data set
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>A connection from your source to the copy activity 
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>A connection from your copy activity to the destination 
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>Go to the Author Tab
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>Data flows are used to transform data
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>Power Query can also be used to transform data
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>The term Sink = Destination in Azure Data Factory
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>To see your storage make sure to go to the storage account and then click on data storage &gt; containers
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>Click on Pipelines
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>Click on new pipeline
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>Search for the copy activity
</span><span id="__span-0-13"><a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>If your data is in the storage account then your source snd sink should be azure data lake storage 2 and then select the type of data so in this case its csv
</span><span id="__span-0-14"><a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>On this screen you can choose the directory of where the file resides and if it doesn’t already exist you can create folders by supplying a name
</span><span id="__span-0-15"><a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>You also need to have a previously setup linked service so that you can supply the linked service data location in this source and sink fields
</span><span id="__span-0-16"><a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>You usually select none for the schema type
</span><span id="__span-0-17"><a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>Running in debug mode will run the full activity which will copy the file from your source to dest. You monitor the pipeline status to see if its working 
</span><span id="__span-0-18"><a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>
</span><span id="__span-0-19"><a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>Copying Data from an API/HTTP Connection to Data Lake (Storage Account)
</span><span id="__span-0-20"><a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>First thing you have to do is create a linked service to get data from your api/http activity
</span><span id="__span-0-21"><a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>So when you create a linked service choose HTTP (rest api is another option)
</span><span id="__span-0-22"><a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>If its a csv on GitHub make sure you view it as Raw and then provide this URL (only up to .com) as the base URL
</span><span id="__span-0-23"><a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>The rest of the URL is the data set
</span><span id="__span-0-24"><a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>Since GitHub repo is public you can set the authentication type as anonymous 
</span><span id="__span-0-25"><a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>Then go create the new pipeline it will only be the copy activity
</span><span id="__span-0-26"><a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>In the source you would click new and then HTTP and then provide the GitHub linked service
</span><span id="__span-0-27"><a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>Then you can provide the relative URL (rest of the URL after the base) and then one good thing to do is preview data to see if it can actually fetch your data
</span><span id="__span-0-28"><a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>Then setup the sink using the previous linked service to your data lake
</span><span id="__span-0-29"><a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>You can edit settings by clicking on the open option beside the parameter. Its important to note that you should specify the file name or it will copy the whole GitHub repo hierarchy
</span><span id="__span-0-30"><a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>
</span><span id="__span-0-31"><a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>Publishing All
</span><span id="__span-0-32"><a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>This saves all your work 
</span><span id="__span-0-33"><a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>
</span><span id="__span-0-34"><a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a>Get Metadata Activity
</span><span id="__span-0-35"><a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>You can sort files into different containers using this activity 
</span><span id="__span-0-36"><a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a>Use case would be to create a dataset that contains all files in the directory that you want to sort
</span><span id="__span-0-37"><a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a>You also have to supply the field list
</span><span id="__span-0-38"><a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a>Child items will cover all files within a folder
</span><span id="__span-0-39"><a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a>After you run the activity you can see the pipeline in the bottom under the successful status and just click on the see output button
</span><span id="__span-0-40"><a id="__codelineno-0-40" name="__codelineno-0-40" href="#__codelineno-0-40"></a>The output of this activity is an array
</span><span id="__span-0-41"><a id="__codelineno-0-41" name="__codelineno-0-41" href="#__codelineno-0-41"></a>
</span><span id="__span-0-42"><a id="__codelineno-0-42" name="__codelineno-0-42" href="#__codelineno-0-42"></a>IF condition activity 
</span><span id="__span-0-43"><a id="__codelineno-0-43" name="__codelineno-0-43" href="#__codelineno-0-43"></a>Allows you to run if conditions 
</span><span id="__span-0-44"><a id="__codelineno-0-44" name="__codelineno-0-44" href="#__codelineno-0-44"></a>
</span><span id="__span-0-45"><a id="__codelineno-0-45" name="__codelineno-0-45" href="#__codelineno-0-45"></a>For Each Activity
</span><span id="__span-0-46"><a id="__codelineno-0-46" name="__codelineno-0-46" href="#__codelineno-0-46"></a>Allows your to loop through data 
</span><span id="__span-0-47"><a id="__codelineno-0-47" name="__codelineno-0-47" href="#__codelineno-0-47"></a>
</span><span id="__span-0-48"><a id="__codelineno-0-48" name="__codelineno-0-48" href="#__codelineno-0-48"></a>Sorting files in a container and moving them to a different one if they don’t match
</span><span id="__span-0-49"><a id="__codelineno-0-49" name="__codelineno-0-49" href="#__codelineno-0-49"></a>Each activity in ADF has 4 options (Connections - on error, on success, on completion)
</span><span id="__span-0-50"><a id="__codelineno-0-50" name="__codelineno-0-50" href="#__codelineno-0-50"></a>This is how you connect activities to each other
</span><span id="__span-0-51"><a id="__codelineno-0-51" name="__codelineno-0-51" href="#__codelineno-0-51"></a>Start with a get metadata activity to get an array of files with properties
</span><span id="__span-0-52"><a id="__codelineno-0-52" name="__codelineno-0-52" href="#__codelineno-0-52"></a>Then you need a for loop to loop through each file in the array 
</span><span id="__span-0-53"><a id="__codelineno-0-53" name="__codelineno-0-53" href="#__codelineno-0-53"></a>To get the output variable you have two options you can use the pipeline expression builder to build out code or in the activity you select the add dynamic content and this has a list of a built in outputs that you can retrieve into a variable. So in our case it would be the get output metadata but you need to make sure you specify that you just want the child items because you don’t want the whole object just the child items array
</span><span id="__span-0-54"><a id="__codelineno-0-54" name="__codelineno-0-54" href="#__codelineno-0-54"></a>To perform activities within the foreach you click on the for each go to the activities tab then click on the pencil button 
</span><span id="__span-0-55"><a id="__codelineno-0-55" name="__codelineno-0-55" href="#__codelineno-0-55"></a>Anytime you want to use output from another activity you just need to add the dynamic content 
</span><span id="__span-0-56"><a id="__codelineno-0-56" name="__codelineno-0-56" href="#__codelineno-0-56"></a>Pipeline expression builder also contains a bunch of functions you can leverage like string functions
</span><span id="__span-0-57"><a id="__codelineno-0-57" name="__codelineno-0-57" href="#__codelineno-0-57"></a>Within the for each you need to start with an if activity
</span><span id="__span-0-58"><a id="__codelineno-0-58" name="__codelineno-0-58" href="#__codelineno-0-58"></a>The expression for this should be dynamic content (each for item).startswith()
</span><span id="__span-0-59"><a id="__codelineno-0-59" name="__codelineno-0-59" href="#__codelineno-0-59"></a>
</span><span id="__span-0-60"><a id="__codelineno-0-60" name="__codelineno-0-60" href="#__codelineno-0-60"></a>Parameterized DataSets
</span><span id="__span-0-61"><a id="__codelineno-0-61" name="__codelineno-0-61" href="#__codelineno-0-61"></a>Basically in our copy activity for each file that meets the condition it needs to be copied but in our current setup we hardcode the file name which won’t work since file names will differ so we need to capture the filename in a dynamic variable = parameter
</span><span id="__span-0-62"><a id="__codelineno-0-62" name="__codelineno-0-62" href="#__codelineno-0-62"></a>To do this: When setting your properties click on Advanced and then click on open dataset
</span><span id="__span-0-63"><a id="__codelineno-0-63" name="__codelineno-0-63" href="#__codelineno-0-63"></a>From here you can see the tab called parameters where you can create the parameter 
</span><span id="__span-0-64"><a id="__codelineno-0-64" name="__codelineno-0-64" href="#__codelineno-0-64"></a>Then in the file path add dynamic content and select the parameter you created
</span><span id="__span-0-65"><a id="__codelineno-0-65" name="__codelineno-0-65" href="#__codelineno-0-65"></a>When referring to dynamic variables - you need the @ prefix
</span><span id="__span-0-66"><a id="__codelineno-0-66" name="__codelineno-0-66" href="#__codelineno-0-66"></a>If you get a bad request when trying to run the pipeline that means you have an error somewhere in your code even if your validation passes.
</span><span id="__span-0-67"><a id="__codelineno-0-67" name="__codelineno-0-67" href="#__codelineno-0-67"></a>
</span><span id="__span-0-68"><a id="__codelineno-0-68" name="__codelineno-0-68" href="#__codelineno-0-68"></a>Dataflow Activities:
</span><span id="__span-0-69"><a id="__codelineno-0-69" name="__codelineno-0-69" href="#__codelineno-0-69"></a>Uses spark to transform data within your pipeline
</span><span id="__span-0-70"><a id="__codelineno-0-70" name="__codelineno-0-70" href="#__codelineno-0-70"></a>Go to settings then setup the source first
</span><span id="__span-0-71"><a id="__codelineno-0-71" name="__codelineno-0-71" href="#__codelineno-0-71"></a>To use spark you need to enable the data flow debug option on
</span><span id="__span-0-72"><a id="__codelineno-0-72" name="__codelineno-0-72" href="#__codelineno-0-72"></a>After you enable the cluster but turning data flow debug on then you need to go to the projection tab and import the projection which is the schema
</span><span id="__span-0-73"><a id="__codelineno-0-73" name="__codelineno-0-73" href="#__codelineno-0-73"></a>The data preview shows you all of the fields in your table and their type
</span><span id="__span-0-74"><a id="__codelineno-0-74" name="__codelineno-0-74" href="#__codelineno-0-74"></a>You click on the small plus button beside the source activity of the dataflow activity then you can select what type of transformation you want to do. Transformation is basically like doing a query on the data.
</span><span id="__span-0-75"><a id="__codelineno-0-75" name="__codelineno-0-75" href="#__codelineno-0-75"></a>SELECT you can check box any fields you want to remove
</span><span id="__span-0-76"><a id="__codelineno-0-76" name="__codelineno-0-76" href="#__codelineno-0-76"></a>FILTER use the expression query to filter out data you don’t want
</span><span id="__span-0-77"><a id="__codelineno-0-77" name="__codelineno-0-77" href="#__codelineno-0-77"></a>Conditional Split - splits data based on the condition
</span><span id="__span-0-78"><a id="__codelineno-0-78" name="__codelineno-0-78" href="#__codelineno-0-78"></a>Replacing NULL: Derived Column and then select the column and in the expression you can use the coalesce function which filters out NULL and puts in the second string argument you provide 
</span><span id="__span-0-79"><a id="__codelineno-0-79" name="__codelineno-0-79" href="#__codelineno-0-79"></a>Group By = Aggregate
</span><span id="__span-0-80"><a id="__codelineno-0-80" name="__codelineno-0-80" href="#__codelineno-0-80"></a>Writing the Data: Select the sink destination as the last activity in the flow
</span><span id="__span-0-81"><a id="__codelineno-0-81" name="__codelineno-0-81" href="#__codelineno-0-81"></a>Before writing to sink you should always use the ALTER Rows activity first. 
</span><span id="__span-0-82"><a id="__codelineno-0-82" name="__codelineno-0-82" href="#__codelineno-0-82"></a>Alter condition = insert if will only insert the data based on the condition 
</span><span id="__span-0-83"><a id="__codelineno-0-83" name="__codelineno-0-83" href="#__codelineno-0-83"></a>Use case: take your data select it all but remove 2 misc columns then filter out cust id 2 then conditionally split on payment and in amex get rid of nulls then group the main split on customer id and find the highest product id
</span><span id="__span-0-84"><a id="__codelineno-0-84" name="__codelineno-0-84" href="#__codelineno-0-84"></a>
</span><span id="__span-0-85"><a id="__codelineno-0-85" name="__codelineno-0-85" href="#__codelineno-0-85"></a>Trigger Activities 
</span><span id="__span-0-86"><a id="__codelineno-0-86" name="__codelineno-0-86" href="#__codelineno-0-86"></a>On the main pipeline screen just click on “Add Trigger”
</span><span id="__span-0-87"><a id="__codelineno-0-87" name="__codelineno-0-87" href="#__codelineno-0-87"></a>When setting up the trigger select an end date
</span><span id="__span-0-88"><a id="__codelineno-0-88" name="__codelineno-0-88" href="#__codelineno-0-88"></a>To see if the trigger actually got kicked off go to the left hand menu and click on the Managed Tab then click on triggers
</span><span id="__span-0-89"><a id="__codelineno-0-89" name="__codelineno-0-89" href="#__codelineno-0-89"></a>Under the monitor tab you can see the pipeline runs
</span><span id="__span-0-90"><a id="__codelineno-0-90" name="__codelineno-0-90" href="#__codelineno-0-90"></a>
</span><span id="__span-0-91"><a id="__codelineno-0-91" name="__codelineno-0-91" href="#__codelineno-0-91"></a>Set Variable Activity 
</span><span id="__span-0-92"><a id="__codelineno-0-92" name="__codelineno-0-92" href="#__codelineno-0-92"></a>Used to store variables that can be used somewhere else in the pipeline downstream
</span><span id="__span-0-93"><a id="__codelineno-0-93" name="__codelineno-0-93" href="#__codelineno-0-93"></a>You can set variables in the pipeline by clicking on the blank space and then going to the variables tab and then create them here
</span><span id="__span-0-94"><a id="__codelineno-0-94" name="__codelineno-0-94" href="#__codelineno-0-94"></a>Then you use the set variable activity to set the value
</span><span id="__span-0-95"><a id="__codelineno-0-95" name="__codelineno-0-95" href="#__codelineno-0-95"></a>Pipeline return value types are variables that can be used in other pipelines
</span><span id="__span-0-96"><a id="__codelineno-0-96" name="__codelineno-0-96" href="#__codelineno-0-96"></a>
</span><span id="__span-0-97"><a id="__codelineno-0-97" name="__codelineno-0-97" href="#__codelineno-0-97"></a>Storage Events Trigger
</span><span id="__span-0-98"><a id="__codelineno-0-98" name="__codelineno-0-98" href="#__codelineno-0-98"></a>A trigger based on an even as opposed to in a timely fashion 
</span><span id="__span-0-99"><a id="__codelineno-0-99" name="__codelineno-0-99" href="#__codelineno-0-99"></a>Storage accounts specifically get triggered by changes in the data storage repo you provide 
</span><span id="__span-0-100"><a id="__codelineno-0-100" name="__codelineno-0-100" href="#__codelineno-0-100"></a>If the file stays in the same initial repo the trigger will keep firing off so you need to delete the file after the trigger runs 
</span><span id="__span-0-101"><a id="__codelineno-0-101" name="__codelineno-0-101" href="#__codelineno-0-101"></a>DELETE Activity
</span><span id="__span-0-102"><a id="__codelineno-0-102" name="__codelineno-0-102" href="#__codelineno-0-102"></a>To use this storage event trigger you need to register this by managing your subscription
</span><span id="__span-0-103"><a id="__codelineno-0-103" name="__codelineno-0-103" href="#__codelineno-0-103"></a>Subscriptions &gt; resource providers &gt; Microsoft.eventgrid 
</span><span id="__span-0-104"><a id="__codelineno-0-104" name="__codelineno-0-104" href="#__codelineno-0-104"></a>
</span><span id="__span-0-105"><a id="__codelineno-0-105" name="__codelineno-0-105" href="#__codelineno-0-105"></a>Execute Pipeline
</span><span id="__span-0-106"><a id="__codelineno-0-106" name="__codelineno-0-106" href="#__codelineno-0-106"></a>This activity lets you execute multiple pipelines together
</span></code></pre></div></h2>
<div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>ADF with GitHub Actions as the CD
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>Within ADF you can setup your git connection from either the live mode option at the top or from the management tab
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>Just need to provide the base GitHub repo name and then it will connect and you select what branch you want to start off with. 
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>You can create a new branch from within ADF 
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>Also allows you to create PRs from ADF
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>In Azure Portal
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>Go to managed identities 
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>Create a new one
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>In the resource group go to the IAM (access control) and add a role assignment and go to job function roles and search for data factory contributor.
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>In role assignments you can see the different roles in the resource group
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>On the new identity you created click into it 
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>Then go to federated credentials
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>Within here select the configure a GitHub issued token to impersonate this app and deploy to azure
</span><span id="__span-1-14"><a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>Creating the CI/CD 
</span><span id="__span-1-15"><a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>In the GitHub repo
</span><span id="__span-1-16"><a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>Create a folder called 
</span><span id="__span-1-17"><a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>Build
</span><span id="__span-1-18"><a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>Then within here create a package.json
</span><span id="__span-1-19"><a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>Here is the script copied from the Microsoft documentation specifically for ADF
</span><span id="__span-1-20"><a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a>Within the GitHub repo go to settings to setup the credentials to connect to Azure
</span><span id="__span-1-21"><a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>Go to security and then secrets and variables
</span><span id="__span-1-22"><a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a>Then go to Actions
</span><span id="__span-1-23"><a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a>Create the new repository secrets 
</span><span id="__span-1-24"><a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a>You need to add the azure subscription id to connect the deployments with action and your azure tenant
</span><span id="__span-1-25"><a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a>The Azure subscription ID is provided from the managed ID
</span><span id="__span-1-26"><a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a>Azure client ID is also within the managed ID
</span><span id="__span-1-27"><a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a>Azure Tenant ID is found by searching for the entra ID within azure portal
</span><span id="__span-1-28"><a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a>Go to GitHub Actions
</span><span id="__span-1-29"><a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a>Setup the workflow - if you do it through the UI it will create the empty yml file for you if not then you will need to create a repo called workflows within the .github folder (don’t forget the period) and then create a main.yml file
</span><span id="__span-1-30"><a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a>Then create the workflow and embed the secrets that you need - you can get this from Microsoft documentation 
</span><span id="__span-1-31"><a id="__codelineno-1-31" name="__codelineno-1-31" href="#__codelineno-1-31"></a>One of the first steps need to be to create the ARM template as that is what we want to use to do the deployments using actions to azure. 
</span><span id="__span-1-32"><a id="__codelineno-1-32" name="__codelineno-1-32" href="#__codelineno-1-32"></a>Then go to GitHub actions and run the build for the first time
</span><span id="__span-1-33"><a id="__codelineno-1-33" name="__codelineno-1-33" href="#__codelineno-1-33"></a>The deployment will deploy onto another ADF resource usually the one you create for PAT/PROD
</span><span id="__span-1-34"><a id="__codelineno-1-34" name="__codelineno-1-34" href="#__codelineno-1-34"></a>
</span><span id="__span-1-35"><a id="__codelineno-1-35" name="__codelineno-1-35" href="#__codelineno-1-35"></a>Azure App Services
</span><span id="__span-1-36"><a id="__codelineno-1-36" name="__codelineno-1-36" href="#__codelineno-1-36"></a>Fully managed service to deploy web apps and APIs without the need to manage infrastructure
</span><span id="__span-1-37"><a id="__codelineno-1-37" name="__codelineno-1-37" href="#__codelineno-1-37"></a>PaaS
</span><span id="__span-1-38"><a id="__codelineno-1-38" name="__codelineno-1-38" href="#__codelineno-1-38"></a>Your web app is stored in a index file and folder and this is served to the end user by the app service plan
</span><span id="__span-1-39"><a id="__codelineno-1-39" name="__codelineno-1-39" href="#__codelineno-1-39"></a>App service plan vs app service
</span><span id="__span-1-40"><a id="__codelineno-1-40" name="__codelineno-1-40" href="#__codelineno-1-40"></a>The plan provides the infra that the app service needs. The app service is your actual application
</span><span id="__span-1-41"><a id="__codelineno-1-41" name="__codelineno-1-41" href="#__codelineno-1-41"></a>In Azure Portal
</span><span id="__span-1-42"><a id="__codelineno-1-42" name="__codelineno-1-42" href="#__codelineno-1-42"></a>Create app service plan first and here you will use the resource group you have for app services
</span><span id="__span-1-43"><a id="__codelineno-1-43" name="__codelineno-1-43" href="#__codelineno-1-43"></a>Now go to app services within azure portal and create a new app service
</span><span id="__span-1-44"><a id="__codelineno-1-44" name="__codelineno-1-44" href="#__codelineno-1-44"></a>There’s. Web app, static web app, web app and data base ,and Wordpress
</span><span id="__span-1-45"><a id="__codelineno-1-45" name="__codelineno-1-45" href="#__codelineno-1-45"></a>After you input the base info you can go ahead and create it 
</span><span id="__span-1-46"><a id="__codelineno-1-46" name="__codelineno-1-46" href="#__codelineno-1-46"></a>Once the app service resource is created click on it
</span><span id="__span-1-47"><a id="__codelineno-1-47" name="__codelineno-1-47" href="#__codelineno-1-47"></a>Then you can see the default domain as well as other high level info
</span><span id="__span-1-48"><a id="__codelineno-1-48" name="__codelineno-1-48" href="#__codelineno-1-48"></a>Within the rescue search advanced tools and then click on go
</span><span id="__span-1-49"><a id="__codelineno-1-49" name="__codelineno-1-49" href="#__codelineno-1-49"></a>It launches the KUDU environment and here is where you can use the console to add your code
</span><span id="__span-1-50"><a id="__codelineno-1-50" name="__codelineno-1-50" href="#__codelineno-1-50"></a>Go to debug console and go to CMD and then from within here go to home\site and go to the wwwroot folder and then copy all of your code files into thes repo 
</span><span id="__span-1-51"><a id="__codelineno-1-51" name="__codelineno-1-51" href="#__codelineno-1-51"></a>Go back to overview and check the default domain 
</span><span id="__span-1-52"><a id="__codelineno-1-52" name="__codelineno-1-52" href="#__codelineno-1-52"></a>
</span><span id="__span-1-53"><a id="__codelineno-1-53" name="__codelineno-1-53" href="#__codelineno-1-53"></a>Kafka
</span><span id="__span-1-54"><a id="__codelineno-1-54" name="__codelineno-1-54" href="#__codelineno-1-54"></a>When your micro service architecture can’t handle load is where we introduce the need for Kafka
</span><span id="__span-1-55"><a id="__codelineno-1-55" name="__codelineno-1-55" href="#__codelineno-1-55"></a>Type coupling occurs when one service depends on other services so if it one goes down it causes the other services to fail
</span><span id="__span-1-56"><a id="__codelineno-1-56" name="__codelineno-1-56" href="#__codelineno-1-56"></a>Kafka sits in the middle of the services and collects events from services and then makes these events available to the other services that need them
</span><span id="__span-1-57"><a id="__codelineno-1-57" name="__codelineno-1-57" href="#__codelineno-1-57"></a>Events are key value pairs and have metadata in them
</span><span id="__span-1-58"><a id="__codelineno-1-58" name="__codelineno-1-58" href="#__codelineno-1-58"></a>Producers create the events
</span><span id="__span-1-59"><a id="__codelineno-1-59" name="__codelineno-1-59" href="#__codelineno-1-59"></a>When producers create events they give them to Kafka and these events get saved in topics which group the same type of events together
</span><span id="__span-1-60"><a id="__codelineno-1-60" name="__codelineno-1-60" href="#__codelineno-1-60"></a>You create the topics and decide how to group the events together
</span><span id="__span-1-61"><a id="__codelineno-1-61" name="__codelineno-1-61" href="#__codelineno-1-61"></a>Microservices subscribe to topics so Kafka will notify subscribed micro services to updates that happen in topics. 
</span><span id="__span-1-62"><a id="__codelineno-1-62" name="__codelineno-1-62" href="#__codelineno-1-62"></a>When a service updates into the database and produces an event and this event and status is captured by Kafka
</span><span id="__span-1-63"><a id="__codelineno-1-63" name="__codelineno-1-63" href="#__codelineno-1-63"></a>One event creates a chain of events
</span><span id="__span-1-64"><a id="__codelineno-1-64" name="__codelineno-1-64" href="#__codelineno-1-64"></a>Kafka also allows for real time analytics 
</span><span id="__span-1-65"><a id="__codelineno-1-65" name="__codelineno-1-65" href="#__codelineno-1-65"></a>Kafka uses stream APIs for real time analytics
</span><span id="__span-1-66"><a id="__codelineno-1-66" name="__codelineno-1-66" href="#__codelineno-1-66"></a>Kafka has partition capabilities that allow for scaling possible especially for large amounts of data
</span><span id="__span-1-67"><a id="__codelineno-1-67" name="__codelineno-1-67" href="#__codelineno-1-67"></a>Partitions add more workers per topic to help process 
</span><span id="__span-1-68"><a id="__codelineno-1-68" name="__codelineno-1-68" href="#__codelineno-1-68"></a>Producers can write into partitions and consumer groups can all consume from Kafka partitions 
</span><span id="__span-1-69"><a id="__codelineno-1-69" name="__codelineno-1-69" href="#__codelineno-1-69"></a>Data in topics are saved on Kafka servers called brokers 
</span><span id="__span-1-70"><a id="__codelineno-1-70" name="__codelineno-1-70" href="#__codelineno-1-70"></a>Regular message queues delete messages after message consumption but in Kafka you can store messages for as long as you need for later analysis 
</span><span id="__span-1-71"><a id="__codelineno-1-71" name="__codelineno-1-71" href="#__codelineno-1-71"></a>Kafka uses zookeeper 
</span><span id="__span-1-72"><a id="__codelineno-1-72" name="__codelineno-1-72" href="#__codelineno-1-72"></a>But now they use Kraft for centralized control and coordination 
</span><span id="__span-1-73"><a id="__codelineno-1-73" name="__codelineno-1-73" href="#__codelineno-1-73"></a>
</span><span id="__span-1-74"><a id="__codelineno-1-74" name="__codelineno-1-74" href="#__codelineno-1-74"></a>TIBCO
</span><span id="__span-1-75"><a id="__codelineno-1-75" name="__codelineno-1-75" href="#__codelineno-1-75"></a>Used for file transfers
</span><span id="__span-1-76"><a id="__codelineno-1-76" name="__codelineno-1-76" href="#__codelineno-1-76"></a>Platform server allows you to transfer files between different server environments 
</span><span id="__span-1-77"><a id="__codelineno-1-77" name="__codelineno-1-77" href="#__codelineno-1-77"></a>Also has event driven interactions
</span><span id="__span-1-78"><a id="__codelineno-1-78" name="__codelineno-1-78" href="#__codelineno-1-78"></a>Internet Server allows for file transfers in and out of the organization 
</span><span id="__span-1-79"><a id="__codelineno-1-79" name="__codelineno-1-79" href="#__codelineno-1-79"></a>Command Center is the centralized control over everything that is happening in regards to file transfers
</span></code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["content.code.copy", "content.code.select", "content.code.annotate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/extra.js"></script>
      
    
  </body>
</html>