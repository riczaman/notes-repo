{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcda My Technical Notes","text":"<p>Welcome to my personal knowledge base. Here, I document new technologies, tools, and concepts I\u2019m learning to grow as a software engineer.</p>"},{"location":"#purpose","title":"Purpose","text":"<ul> <li>Keep a clear, organized record of what I learn.</li> <li>Create a quick reference I can revisit anytime.</li> <li>Share knowledge in a structured, accessible way.</li> </ul>"},{"location":"#topics-covered","title":"Topics Covered","text":"<ul> <li>Programming languages and frameworks</li> <li>DevOps tools and automation</li> <li>Cloud platforms</li> <li>Software design and architecture</li> </ul>"},{"location":"#how-to-navigate","title":"How to Navigate","text":"<p>Use the menu on the left to explore topics. Each section contains: - Overview of the technology - Step-by-step examples - Screenshots or diagrams - Key takeaways</p>"},{"location":"#how-i-learn","title":"How I learn","text":""},{"location":"#first-topic","title":"First Topic","text":"<p>Start with Jenkins \u2014 an automation server for building, testing, and deploying software.</p>"},{"location":"topics/actions/","title":"GitHub Actions","text":"<p>Platform to automate Developer Workflows. CI/CD pipelines is just one of the many workflows that GitHub Actions can be used for. - Any GitHub event that occurs can trigger a workflow which is an automated execution of tasks = GitHub Action</p>"},{"location":"topics/actions/#cicd-pipelines","title":"CI&amp;CD Pipelines","text":"<ol> <li>commit code</li> <li>test code</li> <li>build code</li> <li>push artifact</li> <li>deploy artifact on server</li> </ol>"},{"location":"topics/actions/#github-action-cicd-example","title":"GitHub Action CI/CD Example","text":"<ul> <li> <p>GitHub Actions already has a bunch of workflow templates for applications. 3 types of workflows: Deployment, continious integration build and test, and continious integration publish workflows </p> </li> <li> <p>If you go to <code>Actions</code> on your repo and then select a <code>workflow</code> as a template than it will automatically create the following:</p> </li> <li><code>.github/workflows</code>: Creates the folder that holds the Action workflow</li> <li><code>*.yml</code>: The yaml file that contains the instuctions</li> </ul>"},{"location":"topics/actions/#workflow-syntax","title":"Workflow Syntax","text":"<p>Main Parameters include: <pre><code>   - `name`: This is just the name of the Action\n   - `on`: This is the section that describes the event you can also do multiple events like *on and pull request*\n        `push`: \n            `branches`: [Master]\n\n    - `jobs`: A set of actions that get executed \n\n        -  `runs-on`: ubuntu-latest\n\n        - `steps`: \n           - `uses`: actions/checkout@v2 - This is a predefined action that handles your repo checkout that was made my GitHub. If you go to github.com/actions you can see the full list of pre-defined actions \n\n           - name: Set up JDK 1.8\n                uses: actions/setup-java@v1\n                with:\n                    java-version: 1.8\n\n             - name: Grant execute permission for gradlew\n                run: chmod +x gradlew    #Whenver we use an action we use the *uses* keyword but when we need to run a specific command use *run*\n</code></pre>     - When you push to master or make a PR to master that above action will automatically start</p> <pre><code>- The code is run on GitHub servers which means you do not have to manage them. Each new job runs on a new virtual enviornment. In the above example, we only put one job but you can put multiple jobs and they will all run on different virtual machines.\n   - **By default the jobs run in parallel but you can overwrite this by using the keyword needs under the second jobs like `needs: firstjob`**\n\n- The runs on command determines what OS to run the build on (ubuntu, macOS, or Windows). Using the `matrix` keyword is needed when you want to test on multiple OS then you switch the runs-on to use {{matrix.os}}\n</code></pre>"},{"location":"topics/actions/#build-a-docker-image-from-the-artifact-generated","title":"Build a Docker Image from the Artifact Generated","text":"<ul> <li>You define this also within the workflow action as another step</li> </ul> <p><pre><code>    - name: Build and push Docker Image\n        run: You can put all of the commands required to build and then bush the docker image and if you need multiple commands then you use the pipe syntax. ie. |\n        docker login cred\n        docker build\n            - On Ubuntu machines docker is already pre-installed so you don't have to setup docker to use the commands\n            - but instead of using the run command there is an action for the docker build and push and these are found in the *GitHub Actions Marketplace*\n</code></pre> - For credentials you can store them in GitHub as secrets</p>"},{"location":"topics/aiFoundations/","title":"Oracle AI Foundations","text":"<p>The AI stack consists of: Generative AI, Deep Learning, Machine Learning, and Artificial Intelligence</p> <ul> <li> <p>Artificial Intelligence: Programming machines to imitate human intelligence</p> </li> <li> <p>Machine Learning: Subset of AI where alogorithms are used to learn from past data and predict outcomes on new data or identify trends</p> </li> <li> <p>Deep Learning: A subset of machine learning where algorithms are modelled to learn from complex data using neural networks.</p> </li> <li> <p>Generative AI: A type of AI that creates new content</p> </li> </ul>"},{"location":"topics/aiFoundations/#ai-foundations","title":"AI Foundations","text":"<ul> <li>Artificial General Intelligence: Is machines being able to replicate human intelligence capabilities like motor skills, learning, and intelligence. </li> <li> <p>When you apply AGI to specific and narrow objectives then you get Artificial Intelligence</p> </li> <li> <p>2 major reasons why we need AI:</p> </li> <li><code>Automation &amp; Decision Making</code></li> <li> <p><code>Creative Support</code> </p> </li> <li> <p>Commonly Used AI Domains:</p> </li> <li> <p>Language: <code>Text-related AI tasks</code> use text as the input. <code>Generative AI tasks</code> the output text is generated by a model (ChatGPT). </p> <ul> <li>Text as Data: Inherently Sequential = Sentences, multiple words = tokenization, varying sentence legnths = padding, and similair words = dot or cosine similarity and embedding.</li> <li><code>Language AI Models</code> = designed to understand, process, and generate natural language. (NLP)</li> <li>Deep Learning Models that are used for NLP are: Recurrent Neural Networks which process data sequentiall and stores hidden state, Long Short-Term Memory which process data sequentially and can retain the context betther through use of gates, and Transformers which process data in parallel by using concepts of self attention to better understand the context. </li> </ul> </li> <li> <p>Audio &amp; Speech: Can be either <code>Audio-Related</code> or <code>Generative AI</code>. </p> <ul> <li>Audio &amp; Speech as Data: Digitized snapshots in time like a sample rate, sampling rate of 44.1kHz, bit depth is the number of bits in each 44.1kHz of data. </li> <li><code>Audio &amp; Speech AI Models</code> = designed to process and manipulate audio and speech. </li> <li>Deep Learning models are: Recurrent Neural Networks, ==Long Short-Term Memory, and Transformers, Variational Autoencoders, Waveform Models, &amp; Siamese Networks</li> </ul> </li> <li> <p>Vision: Can be <code>Image Related</code> or <code>Generative AI</code>. </p> <ul> <li>Image as Data: Images consist of pixels which can be grey scale or colour. </li> <li><code>Vision AI models</code> = designed to process and understand visual information from images and videos</li> <li>Deep Learning Models: Convolutional Neural Networks which detect patterns in images, learning hierarchial representations of visual features and YOLO which process the image and detects objects within the image, and Generative Adversarial Network which generates real-looking images. </li> </ul> </li> </ul>"},{"location":"topics/aiFoundations/#oci-ai-services","title":"OCI AI Services","text":"<ol> <li><code>Vision AI Services</code> allows us to do the following:</li> <li><code>Image Classification</code>: Upload an image which gets analyzed and labelled with confidence scores. </li> <li><code>Object Detection</code>: Upload the image and then it detects objects with confidence scores.</li> <li><code>Text Detection</code>: upload an image and it extracts all the text from the image. </li> <li> <p><code>Document AI</code>: Upload a document and then it gives you the raw text and then assigns key value pairs and it extracts tables. </p> </li> <li> <p><code>Language AI Services</code>:</p> </li> <li><code>Text Analytics</code>: Analyzes a block of text and provides us language detection, text classfication, extracts entities, key phrase extractions and sentiment analysis. Also personal identifiable information</li> <li><code>Text Translation</code>: Translates text from one language to another. </li> </ol>"},{"location":"topics/aiFoundations/#ai-vs-ml-vs-dl","title":"AI vs ML vs DL","text":"<ul> <li>Machine Learning Types:</li> <li><code>Supervised</code>: Extracting rules from labelled data. For example: Like credit card applications that use a rules engine. <code>Learning from labelled data</code>. </li> <li><code>Unsupervised</code>: Extracting trends from unlabelled data. Grouping similair data into clusters like retail marketing and sales.</li> <li> <p><code>Reinforcement</code>: Solving tasks by trial and error. </p> </li> <li> <p>Deep Learning is used extracting features and rules from data and it uses neural networks with multiple layers. </p> </li> </ul>"},{"location":"topics/aiFoundations/#machine-learning-foundations","title":"Machine Learning Foundations","text":"<ul> <li> <p>ML provides statidstical tools to analyze, visualize, and make predictions from data like Netflix movie suggestions. </p> </li> <li> <p>ML uses <code>input features</code> to describe what the <code>output label</code> should be. Train the model with the input features and then when the model is trained we can apply inference which is the ability to predic the label. </p> </li> <li> <p>Types of Machine Learning:</p> </li> <li>Supervised which uses labeled data, unsupervised where we just understand relationships and reinforcement which make decisions. </li> <li>Supervised examples include: disease detection, weather forecasting, stock price prediction, spam detection</li> <li>Unsupervised examples: Fradulent transactions, outlier detection and targeted marketing campaigns </li> <li>Reinforcement: automated robots, autonomous cars, healthcare and video games</li> </ul>"},{"location":"topics/aiFoundations/#supervised-learning-classification","title":"Supervised Learning: Classification","text":"<ul> <li>There are 2 types of output labels:</li> <li><code>Continous</code> - This leads to <code>Regression</code></li> <li> <p><code>Categorical</code> - This leads to <code>Classification</code> and these can be <code>binary</code> or <code>multi-class</code> (different types of the same thing like 3 different species of a flower)</p> </li> <li> <p>Classification = a supervised mL technique used to categorize or assign data points into predefined classes based on their features or attributes. </p> </li> <li>They train using a labelled data set</li> </ul> <p>Machine Learning Algorithm for classfication is: Logistic Regression: helps in predicting if something is true or false. Logistic regression uses an S-sjaped curve (sigmoid) for the data as opposed to linear regression.</p>"},{"location":"topics/aiFoundations/#supervised-learning-regression","title":"Supervised Learning: Regression","text":"<ul> <li>Independent features are the labeled input and the dependent feature is the output label</li> <li>Uses linear regression </li> <li>loss is a number indicating how far the predicted value is from the actual value</li> </ul>"},{"location":"topics/aiFoundations/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<ul> <li><code>Anaconda</code> is an open source python and R for data science and machine learning. It helps with package management and deployment. Within Anaconda is <code>Jupyter Notebooks</code> which is an IDE that allows you to share documents. </li> <li> <p>This opens up a terminal view on localhost of your files. </p> </li> <li> <p>Machine Level Process consists of: </p> </li> <li>Loading Data</li> <li>Preprocessing - this involves creating features and labels</li> <li>Training a Model</li> <li>Evaluating the Model</li> <li> <p>Making Predictions </p> </li> <li> <p>Important ML Libraries in Python from <code>Sklearn</code></p> </li> <li><code>train_test_split</code> - this module is used to split the data into 2 sets one where you can train the model and other set to test</li> <li><code>StandardScalar</code> - Process of transforming data so it has a mean of 0 and a standard deviation of 1 and it make sures all features use the same scale. if. square foot vs number of beds when predicting the price of a house. </li> <li> <p><code>accuracy_score</code>- gives you an prediction of how strong the prediction is (classification)</p> </li> <li> <p><code>Unsupervised Learning</code>:</p> </li> <li>There are no labelled outputs </li> <li> <p>Algorithms learn the patterns in the data and group similair data items together. </p> </li> <li> <p>Clustering: is the grouping of simialir data items</p> </li> <li> <p>Similarity: is how close two data points are to each other and is a value between 0 and 1 and this determines what cluster objects belong too.</p> </li> <li> <p>Unsupervised Workflow:</p> <ol> <li>Prepare the data (remove missing values and normalize)</li> <li>Create similarity metrics</li> <li>Run the clustering algorithm (parition, density, hierarchial and distribution)</li> <li>Interpret results and adjust clusternig </li> </ol> </li> <li> <p><code>Reinforcement Learning</code>:</p> </li> <li>type of machine learning that enables an agent to learn from its interactions within the enviornment.</li> <li>agent = interacts with the enviornment and takes action and learns from feedback</li> <li>enviornment = external systems with which the agent interacts</li> <li>state = representation of the current situation of the enviornment</li> <li>action = possible moves or decisions that the agent can take</li> <li> <p>policy = mapping that the agent uses to devide which action to take</p> </li> <li> <p><code>optimal policy</code> = finding the policy that yields a lot of rewards. The algorithms used are: Q learning or Deep Q learning</p> </li> </ul>"},{"location":"topics/aiFoundations/#deep-learning-foundations","title":"Deep Learning Foundations","text":"<ul> <li> <p>A subset of machine learning that focuses on training Artificial Neural Networks (ANNs) with multiple layers </p> </li> <li> <p>ML needs us to specify features wheres in <code>Deep Learning extracts features from raw and complex data and DL algorithms allow parallel processing of data so it has better scalability and performance</code>. </p> </li> <li> <p>The use of GPUs were needed for this complex learning and machine algorithms</p> </li> <li> <p>Types of Deep Learning algorithms can be broken down into two types: <code>Data</code> (Images, videos, text, audio) and the <code>applications</code> (image classification, face detection, NLP) </p> </li> <li> <p>Deep Learning Alorithm for <code>images</code> is Convolutional Neural Networks (CNN)</p> </li> <li> <p>For <code>text</code> we use Transformers, Long-Short Term Memory (LSTM) or Recurrent Neural Networks (RNN)</p> </li> <li> <p>For <code>images, audio and text generation</code> we can use: Transformers, Difussion models, and Generative Adversarial networks (GAN)</p> </li> <li> <p>Building Blocks of ANN:</p> </li> <li><code>Layers</code> which are inputs and hidden layers </li> <li><code>Neurons</code> are computantional units that accept input and produce an output and applies ther activation function to generate output</li> <li><code>Weights</code> which determine the strength of connection between neurons</li> <li><code>Activation Function</code> work on the weighted sum of inputs to a neuron to produce an output</li> <li> <p><code>Bias</code>: is additional input to a neuron that allows a certain degree of flexibility </p> </li> <li> <p>ANNs are trained using Backpropgation Algorithm which is guessing and comparing and then measuring the error. Then the weights are adjusted and then weights are updated. Repeat and Learng Model training method</p> </li> <li> <p>Deep Learning Models for <code>Sequence Models</code></p> </li> <li>Sequence models are input data in the form of sequences and the goal is to find patterns and make predictions. Like NLP, speech recognition, gesture recognition, etc</li> </ul> <p>-Recurrent Neural Networks (RNN) handle sequential data and there is a feedback loop and it can maintain a hidden state or memory and it updates as each element in the sequence is processed. So it can capture dependencies        -Architecture:          1. <code>One to One</code>: used for non sequential data          2. <code>One to Many</code>: music generation or sequence generation          3. <code>Many to one</code>: sentiment analysis           4. <code>Many to Many</code>: machine translation and entity recognition. </p> <pre><code>-==**Long Short Term Memory**==: Works by using a specialized memory cell and gating mechanisms to capture long term dependencies which RNN is not good at. \n- Input processing at step 1, then it recieves the previous memory hidden state value then there is a gating mechanism (input gate, forget gate, and output gate) then it updates the memory and then it produced output generation.\n\n-==**Convolutional Neural Networks**==: \n   -Deep Learning Models: \n       1. `Feed Forward Neural Networks (FNN)` - multi layer perception MLP (simplest)\n       2. `Convolutional Neural Networks (CNN)` - good for image and video \n       3. `Recurrent Neural Networks (RNN)` - good  for time series and sequential data\n       4. `Autoencoders` - are unsupervised learning models used for feature extraction.\n       5. `Long Short Term Memory` - specilized RNN for long term dependencies\n       6. Generative Adversarial Network` (GAN) producing images and content \n       7. `Transformers` which is used for language processing\n\n    - CNN: processes grid like data like images and videos. CNN works good with 2D data by reducing images into an easier to process form.\n\n    - CNN Layers:\n       1. Input Layer \n       2. Feature Extraction layers: This layer is to automatically learn and extract patterns from the input images\n          -convolutional layers (uses small filters kernels)\n          -activation function allows the network to learn more complex non linear data\n          -pooling layer reduces computational complexity\n          -fully connected layer\n          -softmax layer\n          -dropout layer\n       3. Classification Layers\n\n    - Limitations of CNN:\n       - Computation: needs alot of data and compute \n       - Overfitting: happens with limited traning data\n       -Interpretiability: black box models\n       -Sensitivity: sensitive to input variations\n\n    - Main use of CNN is image classification, object detection, image segmentation, face recognition.\n</code></pre>"},{"location":"topics/aiFoundations/#generative-ai-llm-foundations","title":"Generative AI &amp; LLM Foundations","text":""},{"location":"topics/aiFoundations/#intro-to-genai","title":"Intro to GenAI","text":"<ul> <li>Subset of Deep learning where the models are trained to generate output on their own.</li> <li>GenAI models learn the underlying patterns in a given data set and uses that knowledge to create a new data set. </li> </ul> <p>-ML identifies patterns to recognize and classify patterns. -<code>inference</code> is the ability to predict based on the training that was done before.  -ML focuses on learning the relationship between data and the label -In GenAI it learns patterns in an unstructured content and it doesn't need labelled data to train.  -The output of ML is a label whereas in GenAI it is New content. </p> <ul> <li>2 types of Gen AI Models</li> <li><code>Text-Based</code>: models generate text, code, dialogue and they learn from large collections of text data</li> <li><code>Multimodal</code>: process multiple modalities like text, images, videos, etc. </li> </ul>"},{"location":"topics/aiFoundations/#intro-to-llm","title":"Intro to LLM","text":"<ul> <li>A language model (LM) is a probalistic model of text. </li> <li>it helps determine what the next word will be in a sentence it gives a probabilitity to every word in its vocab. </li> <li>Large in LLM stands for the nuumber of parameters. -<code>EOS</code> stands for end of sentence or end of sequence. </li> </ul> <p>LLM Features:    - Based on Transformers this allows them to play attention to specific parts and gives them enhanced contextual understanding.     - Deep Neural networks that are trained on a large set of text    - Paremeters are adustable weights in the models neural network.     - Model size is the memory required to store the models parameters</p> <p>Transformers - <code>Recurrent Neural Network</code> handle sequential data like a sentence and they have a feedback loop that allows them to store and maintain a hidden state but RNN has trouble with <code>Long-range dependencies</code>.     - As the length of the sentence grows it leads to <code>Vanishing Gradient</code> which means it loses context of the entire sequence. </p> <ul> <li>Transformers Architecture: </li> <li>They understand the relationship between all the words in a sentence at the same time and understand how they relate to each other. </li> <li> <p>Attention Mechanism (Self Attention) is used by transformers that adds context to the text and this also helps with long range dependencies </p> </li> <li> <p>Transformer has 2 main parts:</p> </li> <li>Encoder: reads the input text aand encodes it into meanings using attention mechanism. Used for semantic search. </li> <li>Decoder uses these embeddings to generate the output text of the next word (token). Decoders only generate a single token at a time. Used for text generation.</li> <li> <p><code>tokens</code>: LLM understand tokens instead of workds. Tokens can be a part of a word, an entire word or a punctuation</p> </li> <li> <p><code>Embeddings</code>: numerical representation of a piece of text converted to number sequences. They can also be used in semantic search in a vector database</p> </li> <li> <p>Words get converted into tokens then into embeddings (vector data)</p> </li> <li> <p><code>Encoder-Decoder</code>: encoder encodes a sequence of words to a set of vectors and the decoder generates the output sequence from the set of vectors. Here the decoder has a self referential loop as it keeps generating all of the tokens in the sequence. <code>Used for machine translation</code></p> </li> </ul>"},{"location":"topics/aiFoundations/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>Prompt: the input or the initial text provided to the model </li> <li>Prompt Engineering: the process of iteratively refining a prompt for the purpose of eliciting a particular style of a response. </li> <li><code>instruction tuning</code> is a critical step in LLM alignment and it involves fine tuning a pre trained LLM on a varied set of instructions, each paired iwth a desired output.</li> <li> <p>Reinforcement Learning From Human Feedback is used to fine tune LLMs to follow a broad class of written instructions. </p> </li> <li> <p><code>In-context Learning</code>: prompting an LLM with instructions and or demonstrations of the task it is meant to complete</p> </li> <li> <p><code>k-shot prompting</code>: explicitly providing k examples of the intended task in the prompt </p> </li> <li> <p><code>Chain of Thought Prompting</code>: provide examples in a prompt to show responses that include a reasoning step and describes the calculation logic to get to the final answer before giving the final answer. </p> </li> <li> <p><code>Hallucination</code>: model generated text that is non factual and ungrounded.     -<code>retrieval-augmentation</code> have less hallucination then 0-shot prompting. </p> </li> </ul>"},{"location":"topics/aiFoundations/#customizing-llms-with-your-data","title":"Customizing LLMs with your Data","text":"<ol> <li><code>Prompt engineering</code> is the easiest to start</li> <li>If you need more context than use <code>Retrieval-Augmented Generation (RAG)</code></li> <li>More instructions require <code>Fine-tuning</code></li> <li> <p>Usually need to use all of them</p> </li> <li> <p>Retrieval Augmented Generation (RAG): languge queries enterprise knowledge bases (databases, wikis, vector database) to provide grounded responses. RAG does not require fine-tuning </p> </li> <li> <p>Augmented Generation = providing a more concrete answer using the acquired information. </p> </li> <li> <p>LLM Fine-tuning &amp; Inference: take pre-trained foundational model and provide additional training using custom data    -<code>inference</code>: model recieves new text as input and generates output based on what it learning during pre-training and fine tuning.     -benefits: model performance on specific tasks, and improve model efficiency </p> </li> <li> <p>Generative AI creates new content without making predictions</p> </li> <li>Sequence models are indeed well-suited for tasks involving sequentially ordered data points or events, such as time series analysis, natural language processing, speech recognition, and language translation. However, for image classification and object recognition, traditional machine learning models and convolutional neural networks (CNNs) are more commonly used.</li> </ol>"},{"location":"topics/aiFoundations/#oci-ai-portfolio","title":"OCI AI Portfolio","text":"<ul> <li>Data -&gt; Infrastructure -&gt; AI Services -&gt; SaaS Apps</li> <li> <p>No infrastructure needs to be managed. </p> </li> <li> <p>Ways to access Oracle Cloud Infra:</p> </li> <li><code>OCI Console</code> is a browser based for all the features needed for data science likes notebook</li> <li><code>Rest API</code> </li> <li><code>Language SDKs</code>: Provides programming language SDKs</li> <li><code>Command Line Interface</code>: provides quick access and full functionality without scripting</li> </ul>"},{"location":"topics/aiFoundations/#overview-of-ai-services","title":"Overview of AI Services:","text":"<ol> <li><code>Language</code>: Text analysis at scale using pretrained models and custom models </li> <li><code>Vision</code>: upload images to detect and classify object using pretained and custom models</li> <li><code>Speech</code>: convert media files into readable text</li> <li><code>Document Understanding</code>: upload documents to detect and classify text.</li> <li><code>Digital Assistant</code>: Platform used to create and deploy digital assistants using natural language conversations.</li> </ol>"},{"location":"topics/aiFoundations/#overview-of-ml-services","title":"Overview of ML Services:","text":"<ul> <li>3 core principles of OCI Data Science:</li> <li><code>Accelerated</code>: allows data scientists to work the way they want without needed to manage the infra</li> <li><code>Collaborative</code>: allows them to work together using <code>Projects</code> (notebook sessions) and uses the <code>Conda</code> enviornments </li> <li> <p><code>Enterprise-Grade</code>: Fully managed infra and updates and security</p> </li> <li> <p>OCI Data science is used to build, train, and deploy ML models and it serves Data Scientists. </p> </li> <li> <p>Accelerated Data Science (ADS) SDK are given to data scientst that give them libraries to help data scientists </p> </li> <li> <p><code>Model Catalog</code> is a centralized repo where model artifacts are stored. </p> </li> <li>Model Deployments (to an HTTP Web app) -&gt; Jobs</li> </ul>"},{"location":"topics/aiFoundations/#ai-infrastructure","title":"AI Infrastructure","text":"<ul> <li><code>GPU</code>: Hardware that performs simple operations and allows many processes to run with parallel computing.</li> </ul>"},{"location":"topics/aiFoundations/#gpu-superclusters-in-oci","title":"GPU &amp; Superclusters in OCI","text":"<ul> <li><code>RDMA</code>: Remote Direct Memory Access - Its a technology that allows for network communication without any cpu interference which allows GPUs to communicate with low latency and this is the core that their database services are built upon</li> <li><code>ROCKY</code> = RDMA converged ethernet </li> <li>The partnered with NVDIA because there is a high demand for high compute GPUs that can run within a single RDMA network (ie. a Supercluster)</li> </ul> <p><code>RDMA Supercluster</code> which is designed to support a large number of GPUs.    -GPU node connects to the network fabrics and any GPU can talk to any GPU through the fabric    - Supercluster = it just means its much larger than a typical cluster and it has 2 blocks. One block uses a <code>Clos Fabric</code> and it uses a 3-tier network using silicon chips and buffers they counteract the latency that might occur with a large number of GPUs supercluster = Lossless    - Using <code>placement</code> they are able to balance scalability and latency    - OCI AI Superclusters are specifically designed to handle demanding AI workloads that require significant computational power and scalability. They are optimized to provide high performance for complex tasks like training large machine learning models, deep learning, and other compute-intensive AI tasks.    - Dedicated AI Clusters provide GPU-based compute resources required to fine-tune a pre-trained model for specific tasks like customer support.</p>"},{"location":"topics/aiFoundations/#responsible-ai","title":"Responsible AI","text":"<ul> <li>Guiding Principles for AI to be trustworthy:</li> <li>AI should follow applicable laws</li> <li>AI should be ethical: Human Ethics and AI Ethics - used to help humans, prevent harm, and fairness and explicable</li> <li>AI should be robust </li> </ul> <p>-Responsible AI Requirements;    1. Set up goverance    2. Develop policies and procedures    3. Ensure compliance  - roles: developers, deployers, and end users </p>"},{"location":"topics/aiFoundations/#oci-generative-ai-services","title":"OCI Generative AI Services","text":"<ul> <li>Fully mangaged service that provides a set of customizable LLM available through APIs</li> <li><code>Choice of Models</code> - high performing models from Meta and Cohere</li> <li><code>Flexible Fine-Tuning</code> to create custom models by fine tuning models using your own data set</li> <li> <p><code>Dedicated AI Clusters</code> that host your workloads</p> </li> <li> <p>2 types of Pretrained Foundational Models:    1.Chat Models: Such as Command-r-plus, command-r-16k, and llamma 3-70b-instruct: Llama is made by meta and the first two by cohere. R plus is more expensive and its used for more complex scenarios. They work by asking questions and get <code>conversational repsonses</code> aka go through <code>instruction tuning</code></p> </li> <li> <p>Embedding Models such as: embed-english-v3.0, embed-multilingual-v3.0: Text converted to <code>vector embeddings</code> used for semantic search and allows for multilingual models.</p> </li> <li> <p>Fine tuning is used when a pretrained model isn't working or if you want to teach it something new.</p> </li> <li><code>T-Few Fine Tuning</code> is what Cohere uses and it enables fast and efficient customizations - it introduces new base layers and only updates a fraction of the model so you dont have to fine tune everything which takes longer and costs more </li> </ul> <p>-Preamble just changs the behaviour of the model but it is not finetuning </p>"},{"location":"topics/aiFoundations/#vector-search","title":"Vector Search","text":"<ul> <li>AI Vector search is built into the <code>Oracle Database 23ai</code></li> <li>Works on structured and unstructured data</li> <li>Uses SQL support for vector generation, Vector Data type, indexes, and uses syntax similair to SQL.</li> <li> <p>Process: Load images as blob -&gt; Vector Embededding -&gt; store in DB -&gt; Vector Search for similair matches </p> </li> <li> <p><code>Vector Datatype</code>: You can use the dimension format or not this is optional (ie. int, float, etc).  The VECTOR datatype in Oracle Database 23ai is specifically designed to store embeddings for AI Vector Search. This datatype allows efficient storage and retrieval of high-dimensional numerical representations of data, enabling similarity searches for AI and machine learning applications.</p> </li> <li><code>Vector Distance Function</code> shows the similairty between vectors. Vectors that have a small distance are more similair. </li> <li><code>Vector Search SQL</code> used to find top k closes matches to a given query item that uses (vector_distance)</li> <li><code>Vector Index</code> are used not only for performance but it also controls the accuracy using the <code>organization</code> and <code>distance</code> parameters. Organization is if it will fit in memory. If it will fit in memory use <code>inmemory neighbour graph</code> and if it doesnt use <code>neighbour partitions</code></li> <li><code>Target accuracy</code> is a clause added to indicate the default accuracy the index should provide for similairty queries </li> <li><code>Approximate</code> keyword indicates that the user wants to perform a similarity search using a vector index. </li> <li>You can also perform similarity search over joins</li> <li>Allows you to efficiently orchestrate Gen-AI pipelines.</li> <li>Model endpoints allow deployed models to be accessed via an API for real-time inference, making them available for AI applications.</li> </ul>"},{"location":"topics/aiFoundations/#select-ai","title":"Select AI","text":"<ul> <li>use your language to query the data (autonomous database) you dont need to know where the data is or how to access the database</li> <li>It takes the natural language question to form a SQL query </li> </ul>"},{"location":"topics/aiFoundations/#oci-ai-services_1","title":"OCI AI Services","text":"<ul> <li> <p><code>OCI Language</code> detects the language of your text, identifies entities in your text, identifies setinment for each aspect of text, identifies key phrases that represent important ideas or subjects, and it classifies general topic from list of 600 categories</p> </li> <li> <p><code>OCI Speech</code> converts speech to text using deep learning techniques. It also uses SRT closed caption support. </p> </li> <li>It also normalizes text to more concise versions of the text. (words to numbers) </li> <li> <p>Has profanity filtering: removing, mask (removes but leaves the first letter) and tags.</p> </li> <li> <p><code>OCI Vision</code> works on images and provides image analysis and document AI    Image Analysis: <code>Object Detection</code> where it detects objects inside an image with a bounding box and a label. It can also detect text. <code>Image Classification</code> labels the scene and you can retrain data for specific needs. </p> </li> <li> <p><code>Document Understanding</code> its used for understanding document images    -features: Text recognition (OCR) from images, document classification based on visual appearance, language detection, table extraction, and key value extraction </p> </li> </ul>"},{"location":"topics/aiFoundations/#additional-notes","title":"Additional Notes","text":"<ul> <li>Model training = establishing a relationship between input and output parameters</li> <li> <p>Gen AI aims to understand underlying data distribution and creates new examples. </p> </li> <li> <p>Recommendations are given based on the user's past choices or similar user or product choices. Hence it is an example of a Supervised Machine Learning.</p> </li> <li>OCI Speech s SRT file support is the best choice. This allows captions to be added easily to videos in industry-standard format.</li> <li>Predicting a house price which is a numerical value is an example of a supervised machine learning, more specifically Regression algorithm.</li> <li>The GB200 GPU is a next-generation Grace Blackwell GPU designed for exascale AI and HPC workloads, making it more suitable for massive-scale AI training rather than standard large-scale AI workloads.</li> <li>Once a model is trained, it needs to be deployed for real-time inferencing using OCI Data Science and GPU Compute. This allows the model to process new data efficiently.</li> <li>Tokens are the fundamental units of text that Large Language Models (LLMs) process. A token can be a word, subword, or character, depending on the tokenization method used. The model interprets and generates text based on these tokens rather than entire sentences or paragraphs at once.</li> <li>Hidden layers , take the input from input or other hidden layer and multiples it through weights and activations. Input layer accepts input and output layer outputs the final result.</li> <li>Predicting a next note in music needs context of prior notes. For this RNN is well suited.</li> <li>OCI AI Infrastructure includes NVIDIA GPUs, OCI Storage, and RDMA Networking for high-performance AI and ML workloads. However, OCI Vault is primarily used for securing and managing cryptographic keys and secrets, not AI infrastructure.</li> <li>Oracle Database 23ai allows ONNX models to be loaded into the database, enabling vector embedding generation and similarity searches.</li> <li>K-Nearest Neighbors (KNN) is considered a non-parametric algorithm: Unlike parametric models (e.g., linear regression, neural networks), KNN doesn't have any parameters that need to be learned from the data. The only parameter to tune is the number of nearest neighbors (K).</li> <li>Organize text based on \"politics\" , \"sports\" or \"news\" = Text Classification</li> </ul> <p>-The NVIDIA A100 GPU is widely used for small to medium-scale AI training and inference workloads, offering high-performance compute capabilities, tensor cores, and scalability. While H200 is a newer high-memory variant, the A100 remains a strong choice for efficient AI workloads. - Object detection is implemented using Deep Learning. Hence the answer is Deep Learning. - To retain the words but mark them, tagging is the correct choice. This method leaves the words in place while adding labels to indicate profanity. - Loss function checks what is the difference between actual value and predicted value. - Generative AI models do not require labeled data in the pre-training stage. Instead, they learn patterns from vast amounts of unstructured data, enabling them to generate new, unique outputs. - Spam detection is a supervised machine learning problem and NOT a unsupervised learning example. - Detecting pedestrians and making lane changes is similar to a human behaviour. Hence the answer is Artificial Intelligence. - Prediction of a next word given a sequence of words needs to use a context of prior words in a sequence. RNN is well suited for this. - The target variable refers to desired outcome. It could be a numerical value or a label. e.g. spam or not spam or predicted rainfall in milimeters. - Select AI translates natural language into SQL by leveraging large language models (LLMs) to infer intent and construct the required SQL query. - Detecting spam is a classification problem. Hence Machine Learning can be used for Supervised Machine Learning.</p>"},{"location":"topics/docker/","title":"Docker","text":"<p>Docker is an open platform designed for developing, shipping, and running applications using containerization. At a high level, it enables users to package an application and all its dependencies (code, libraries, system tools, runtime, and configurations) into a standardized unit called a container. </p>"},{"location":"topics/docker/#deployment-before-containers","title":"Deployment before Containers","text":"<ul> <li>Each developer needs to install the services and dependencies locally on their OS on their laptop (Redis, Database, Messaging, etc)</li> <li>Installation on different OS (Mac vs Windows) is different</li> <li>Prone to error as the setup can be difficult especially if your application is complex</li> </ul>"},{"location":"topics/docker/#deployment-with-containers","title":"Deployment with Containers","text":"<ul> <li>With containers like Docker you don't have to install all of the services independently on your OS instead these services come packaged with the application in an isolated enviornment</li> <li>Docker also allows you to run different versions of an application without any conflicts (ie. Redis 4.1, Redis 4.2, Redis 4.3, etc)</li> </ul>"},{"location":"topics/docker/#vm-vs-docker","title":"VM vs Docker","text":"<ul> <li>Docker only contains and deals with the OS application layer</li> <li>VM has the container layer and the OS Kernel</li> <li>As a result Docker packages are a lot smaller since they only have to implement one layer of the OS and they are faster to start up since VMS have to boot up the OS Kernel</li> <li>VMs are compatible with all OS but Docker is only compatible with Linux OS for example if you have Linux Docker file it can run on a Windows machine because the Windows machine has the Windos OS Kernel</li> <li><code>Docker Desktop</code> now lets Linux Containers work with Mac or Windows OS. Docker Desktop uses a hypervisor layer which has the Linux kernel that is needed. </li> </ul>"},{"location":"topics/docker/#docker-images-vs-docker-containers","title":"Docker Images vs Docker Containers","text":"<ul> <li> <p><code>Docker image</code> is the artifact that we create that has the application and all of it envionrment dependencies that we can then upload to an artifact repository so another server/person can download and run it &amp; it is executable</p> </li> <li> <p><code>Docker Container</code> is a running instance of an image. Basically the server or host or machine that runs the image becomes the Docker container              - You can run multiple containers from one image</p> </li> <li> <p><code>docker images</code>: Command to see all the images that you have in your docker instance</p> </li> <li><code>docker ps</code>: See your running containers</li> </ul>"},{"location":"topics/docker/#docker-registry","title":"Docker Registry","text":"<ul> <li>Registries are places that have premade docker images stored</li> <li><code>Docker Hub</code> is Docker's officially registry: https://hub.docker.com/</li> </ul>"},{"location":"topics/docker/#image-versioning","title":"Image Versioning","text":"<ul> <li>New versions of the application require new versions of the docker image and these versions are called Tags</li> <li>All images have a tag called <code>latest</code> which indicates it the newest version of that image. If you don't choose a specific version you get latest by default</li> </ul>"},{"location":"topics/docker/#getting-images","title":"Getting Images","text":"<ol> <li>Search for the image (package) that you need from a Registry</li> <li>Then run <code>docker pull {name}:tag</code></li> <li>You didn't have to specify the registry here because DockerHub is the default location where it searches. </li> </ol>"},{"location":"topics/docker/#running-images","title":"Running Images","text":"<ol> <li>To run images you just need to use this commmand: <code>docker run imageName:tagnNumber</code></li> <li> <p>Running the container using this command runs it in the foreground which means your terminal is now blocked</p> </li> <li> <p>Using <code>-d</code> flag will run the container in the background and it will give you the full process ID</p> </li> <li> <p><code>docker run -d name:tag</code> </p> </li> <li> <p>To see Logs while running containers in the backround: <code>docker logs containerID</code> (you get this from docker ps)</p> </li> <li> <p>You can skip the docker pull command and just run the docker run for an image that you don't have locally as long as that image is found in DockerHub</p> </li> </ol>"},{"location":"topics/docker/#port-binding","title":"Port Binding","text":"<ul> <li>When you run a container like the above without mentioning a port then you can't access it since its running on the enclosed Docker network </li> <li> <p>We need to expose the container port to the host that is running the container </p> </li> <li> <p><code>Port Binding</code>: is when you bind the containers port to the host port so that you can access the container. For example, nginx runs on port 80 and that is standard for this application.</p> </li> <li> <p>Stopping Containers = <code>docker stop containerID</code></p> </li> <li> <p>To bind the port: <code>docker run -d -p hostPort:containerPort name:tag</code></p> </li> <li>the -p is what is used to bind the port and it uses {host port: container port}. Now if you go to localhost for example on the host port you define you will see the running container </li> <li> <p>Only one service can be tied to a port if you try to run another docker run for a different service on the same port you will get an error </p> </li> <li> <p>To stop a container you run: <code>docker stop containerID</code></p> </li> </ul> <p>-You should match the host port to the same one that the container uses</p>"},{"location":"topics/docker/#all-containers","title":"All Containers","text":"<ul> <li>Even though you stopped containers they still exist and <code>docker run</code> does not reuse containers it creates a new one to see a list of all containers running and not running you can run: <code>docker ps -a</code></li> <li> <p>the -a flag means all</p> </li> <li> <p>Restarting a Container that was previously stopped:</p> </li> <li> <p><code>docker start containerID</code></p> </li> <li> <p>Using Container Names - When running these docker commands we reference the docker ID but we can also use the <code>container name</code> which is usually auto-generated by docker but you can change this value:</p> </li> <li>to manually name the container you need to run <code>docker run --name giveName -d -p 8080:80 name:tag</code></li> <li>the --name is what you use to give it a specific name</li> </ul>"},{"location":"topics/docker/#private-docker-registeries","title":"Private Docker Registeries","text":"<ul> <li>These are images that are created in-house by companies which they do not want to share with people outside of the company</li> <li> <p>You need to <code>authenticate</code> before you can access the registry and all of the big cloud providers have their own docker registries</p> </li> <li> <p>Docker Registry versus Docker Repository</p> </li> <li>A registry is a service for providing storage and its a collection of repositories </li> <li>A repository is a collection of related images with the same name but different versions </li> </ul>"},{"location":"topics/docker/#building-custom-docker-images","title":"Building Custom Docker Images","text":"<ul> <li> <p><code>Dockerfile</code> is the file that contains all the commands used to assemble an image from your app</p> </li> <li> <p>In the root of the application create a new file called: <code>Dockerfile</code></p> </li> <li> <p>Anything that you need to run your app is what you also need to include as a dependency or env variable in your dockerfile. For example, you have a backend server file that needs node js for example to run it locally you need to do node server.js then node is something that your app depends on</p> </li> <li> <p>So each dockerfile starts off with a Base Image and you choose the base image based on the tools that your application needs             - node based app you should use a node base image             - python based app you should use a python base image</p> </li> </ul> <p>Structure of a Docker file</p> <ul> <li> <p>You can pull base images from public or private registeries so in this example we are going to use a base image from the Dockerhub and we get base images by using the FROM keyword</p> </li> <li> <p>Basically you need to map all the commands you need to run the app locally in your docker file so that this image when it goes to another OS can also be built successfully. </p> </li> <li>For example: If you have package.json that means you have node packages that need to install so you need to define this in your dockerfile</li> <li>You can run any commands and since alpine is a linux based image you can run linux command but you need to prefix it with the <code>RUN</code> keyword.</li> <li>You also need to copy all of the files that are required to run the application to the container so you need a <code>COPY</code> directive </li> <li>To change the directory of where you want to run commands in your docker container you use <code>WORKDIR</code></li> <li>The last command that is run in a dockerfile uses the command <code>CMD</code> and it uses [] where you provide the command and the parameters</li> </ul>"},{"location":"topics/docker/#from-node19-alpine-copy-packagejson-app-copy-src-app-workdir-app-run-npm-install-cmd-node-serverjs","title":"<pre><code>FROM node:19-alpine\n\nCOPY package.json /app/\nCOPY src /app/\n\nWORKDIR /app\n\nRUN npm install\n\nCMD [\"node\", \"server.js\"]\n</code></pre>","text":""},{"location":"topics/docker/#building-the-docker-image","title":"Building the Docker Image","text":"<ol> <li>Once the dockerfile is complete then inside the project where the dockerfile resides we can build the image:</li> <li>the -t is the tag parameter that lets you name the image using the {name: version}</li> <li>the last parameter is the location of the dockerfile</li> <li>` docker build - t web-app:1.0 . <ul> <li>the dot represents the current folder </li> </ul> </li> </ol>"},{"location":"topics/genAI/","title":"Oracle Generative AI Professional","text":""},{"location":"topics/genAI/#fundamentals-of-large-language-models","title":"Fundamentals of Large Language Models","text":""},{"location":"topics/genAI/#basics-of-llms","title":"Basics of LLMs","text":"<ul> <li><code>Language Model</code> = probablilstic model of text. Large LMs just refers to the number of parameters (large)</li> <li><code>Decoding</code> = Term for generating text from an LLM</li> <li><code>Prompting</code> = Affects the distribution of the LLMs vocabulary but it does not change the models parameters</li> <li><code>Training</code> = Affects the distribution and changes the models parameters. </li> </ul>"},{"location":"topics/genAI/#llm-architectures","title":"LLM Architectures","text":"<ol> <li>Encoders they are used for <code>embedding</code></li> <li>Decoders they are used for <code>text generation</code></li> <li>Capabilities can be embedding or text generation </li> <li>All models are built using the <code>Transformer</code> Architecture. </li> <li><code>Embedding</code> = convert a sequence of word into a vector or sequence of vectors. Basically, embedding converts the text into a numerical representation of the text with meaning.</li> <li> <p><code>Generation</code> = Generate text based on a sequence of input words. </p> </li> <li> <p>Encoders and Decoders can come in all sizes. Sizes is defined by the number of trainable parameters it has. <code>Decoder models are larger than encoders</code> but you can make encoders big but its not needed. </p> </li> <li> <p><code>Encoders</code>: Semantic search is the primary use where you can store an input snippet into an index and then use a group of documents to find the one that is most similair </p> </li> <li> <p><code>Decoders</code>: take a sequence of tokens and generate the next word in the sequence. They only produce a single token at a time. </p> </li> <li> <p><code>Encoder-Decoder</code>: encodes a sequence of words and uses the encoded to output the next word. Used for machine translation. </p> </li> </ol>"},{"location":"topics/genAI/#prompting-prompt-engineering","title":"Prompting &amp; Prompt Engineering","text":"<ul> <li>LLMs typically only involve decoder only models</li> </ul> <p>Prompting</p> <ul> <li> <p>Altering the content or structure of the input that you pass to the model: text provided to an LLM as input, sometimes containing instructions and/or examples.</p> </li> <li> <p>Decoders have pre-training which is where they are trained on a large set of input</p> </li> </ul> <p>Prompt Engineering - The process of iteratively refining a prompt for the purpose of eliciting a particular style of response. </p> <ol> <li> <p>In-context learning: prompting an LLM with instructions and or demonstrations of the task its meant to complete. </p> </li> <li> <p>k-shot prompting: explicitly providing k examples of the intended task in the prompt.</p> </li> <li> <p>0-shot prompting means not providing any examples</p> </li> <li> <p>Chain-of-Thought prompting: prompt the LLM to emit intermediate reasoning steps </p> </li> <li> <p>Least-to-Most prompting: prompt the LLM to decompose the problem and solve, easy first then hardest. </p> </li> <li> <p>Step-Back prompting: prompt the LLM to identify high-level concepts pertinent to a specific task.</p> </li> </ol> <p><code>Issues with Prompting</code>: 1. Prompt Injection: deliberately provide an LLM with input that attempts to cause it to ignore instructions, cause harm, or behave contrary to deployment expectations. </p> <ol> <li>Memorization: after answering, repeat the original prompt (leaked prompt) or data from a previous prompt answer</li> </ol> <p>Training: - prompting alone may be inappropriate when: training data exists or domain adaption is required because small changes in the prompt can lead to huge changes in the probablity of the next output. </p> <ul> <li> <p>Domain-Adaption: adapting a model through training to enhance its performance outside of the domain/subject area it was trained on. </p> </li> <li> <p>Training is the process of giving a model an input then having the model guess the output and then based on the answer alter the parameters of the model so next time it generates something closer to the answer. </p> </li> </ul> <p>Training Styles: 1. Fine-Tuning: take a pre-training model and train the model on custom data but its very expensive  2. Param Efficient Fine Tuning: Isolate a small set of the models parameters to train - Cheaper (LORA) 3. Soft Prompting: Adding parameters to the prompt or specialized words in the prompt - learning and only uses a few parameters 4. Pre-Training: changes all parameters but it uses unlabeled data </p> <ul> <li>Pre-training is more expensive then fine tuning</li> </ul>"},{"location":"topics/genAI/#decoding","title":"Decoding","text":"<ul> <li> <p>The process of generating text with an LLM</p> </li> <li> <p>It happens 1 word at a time and it is iterative </p> </li> <li> <p>Word is appended to the input and the new revised input is fed into the model to be decoded for the next word. </p> </li> </ul> <p>Types of Decoding:</p> <ol> <li> <p>Greedy Decoding: pick the highest probability word at each step. </p> </li> <li> <p>Non-Deterministic Decoding: Pick randomly among high probability candidates at each step. Tempature = controls the sharpness or smoothness of this probability distribution. A low temperature value results in a sharper distribution, meaning that the model is more confident in its predictions and tends to select the most likely word with higher probability. Conversely, a higher temperature value smooths out the distribution, making it more likely for lower probability words to be chosen, leading to more diverse and varied output.</p> </li> <li>When temperature is decreased, the distribution is more peaked around the most likely word. = Greedy Decoding</li> <li>When temperature is increased, the distribution is flattened over all words. </li> <li> <p>But the probability in terms of the highest and lowest rated ones will always state that way. == Ordering of words is unaffected by temperature.</p> </li> <li> <p>Nucleus Sampling Decoding: Precise what portion of the distrubtion words you can sample from</p> </li> <li> <p>Beam Search: Multiple similar sequences simultaneously and it is not greedy but outputs sequences with higher probability than greedy decoding. </p> </li> </ol>"},{"location":"topics/genAI/#hallucination","title":"Hallucination","text":"<ul> <li>generated text that is non-factual and or ungrounded. </li> </ul> <p>Reducing Hallucinations (no way to eliminate): - Grounded: generated text is grounded in a document if the document supports the text. Attribution/Grounding </p>"},{"location":"topics/genAI/#llm-applications","title":"LLM Applications","text":"<ol> <li>Retrieval Augmented Generation (RAG): a system where input is turned into a query that has access to support documents which will generate a correct answer and this can reduce hallucination. </li> <li> <p>Non-parametric way of improving the model because you just add more documents but not the model itself</p> </li> <li> <p>Code Models: Are LLMs training on code and comments instead of written language </p> </li> <li> <p>Mult-Modal Models: Are trained on multiple modalities like languages, images, etc. </p> </li> <li> <p>Language Agents: Models that are intended for sequential decision making scenarios like playing chess and take actions iteratively in response to their enviornment. </p> </li> <li>ReAct: Prompt the model for thoughts, then acts, and observes the results </li> <li>Toolformer: strings are replaced with calls to tools</li> <li>Bootstrapped reasoningL emit rationalization of intermediate steps</li> </ol>"},{"location":"topics/genAI/#oci-generative-services","title":"OCI Generative Services","text":""},{"location":"topics/genAI/#chat-models","title":"Chat Models","text":"<ul> <li>Tokens: Language models understand tokens instead of characters and tokens can be a part of a word, an entire word or punctuation. </li> <li>For example a sentence with 10 words can have 15 tokens </li> </ul> <p>Pretrained Chat Models</p> <ol> <li> <p>command r-plus: used for q&amp;a, info retrieval and sentiment analysis</p> </li> <li> <p>command r-16k: This is the smaller and more fast version of r and it used when speed and cost is important. </p> </li> <li> <p>llama-3.1-405b/70b instruct: largest publically available LLM</p> </li> </ol>"},{"location":"topics/genAI/#chat-model-parameters","title":"Chat Model Parameters","text":"<ol> <li> <p>Maximum Output Tokens: The max number of tokens model generates per response. </p> </li> <li> <p>Premble Override: Initial guideline message that can change the models overall chat behaviour and conversation style. </p> </li> <li> <p>Temperature: Controls the randomness of the output. To generate the same output for a prompt you use 0 (highest probability answer). Lower values are more correct and used for Q&amp;A and higher values are more random and used for creative. </p> </li> <li> <p>Top k: Top K tells the model to pick the next token from the top 'k' tokens in the list sorted by probability</p> </li> <li> <p>Top p: Pick top tokens based on the sum of their probabilities (finds the combination of p tokens that yields the highes probabililty)</p> </li> <li> <p>Frequency Penalty: Used to get rid of repetition in your outputs. Frequency Penalty penalizes tokens that have already appeared in the preeceding text. </p> </li> <li> <p>Presence Penalty: Also used to get rid of repetition by applying the penalty regardless of frequency so if the token has appeared once it will be penalized. </p> </li> </ol>"},{"location":"topics/genAI/#generative-ai-inference-api","title":"Generative AI Inference API","text":"<ul> <li> <p>Within the Oracle cloud if you go to the Generative AI module you can actually copy the code that is developed from the playground and then take that python code and run it in a Jupyter notebook.</p> </li> <li> <p>inference API is the basically the endpoint you use within the Python script </p> </li> <li> <p>To setup the config file within the Oracle cloud you go to My Profile and within your details you will see <code>API keys</code>. When you create an API key make sure you download the private key file as well as adding the config file</p> </li> </ul>"},{"location":"topics/genAI/#embedding-models","title":"Embedding Models","text":"<ul> <li> <p>Translation is a sequence to sequence task</p> </li> <li> <p>Word Embeddings: Capture properties of the word. For example, the word is an animal so some properties could be size and age. But actual embeddings represent more properties than just two </p> </li> <li> <p>Semantic Similarity: Cosine and Dot product similairity can be used to compute numerical similarility</p> </li> <li> <p>embeddings that are numberically similair are also semantically similair</p> </li> <li> <p>Sentence Embeddings: Associates every sentence with a vector of numbers. </p> </li> </ul> <p>Embeddings use case:</p> <ul> <li>Retrieval Augmented Generation (RAG): take a large document and generate the embeddings of each paragraph and put it into a vector database to allow you to get semantic search. </li> </ul> <p>Embedding Models in GenAI</p> <ol> <li>cohere.embed-english</li> <li>cohere.ember-english-light</li> <li>cohere.emberd.multilingual    -use cases: semantic search, text classification, and text clustering </li> <li> <p>1024-dimensional vector for eaxch embedding and max 512 tokens. The light version only uses a 384 dimensionsal vector.</p> </li> <li> <p>As you compress the embeddings to lower dimensions the information retained is less </p> </li> </ol>"},{"location":"topics/genAI/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li> <p>LLMs are next word predictors they attempt to produce the next series of words that are most likely to follow from the previous text. </p> </li> <li> <p>Reinforcement Learning from Human Feedback (RLHF) is used to fine-tune LLMs to follow a broad class of written instructions.</p> </li> </ul> <p>Prompt Formats:</p> <ul> <li>large language models are trained on a specific prompt format. </li> <li> <p>Llama2 Prompt formatting: They use a beginning and end [INST] tag. Instruction tags</p> </li> <li> <p>Zero Shot Chain-of-Thought: apply chain of thought but you don't provide example so you just ask it a phrase like lets think step by step as opposed to chain of thought were you provide it the examples for reasoning. </p> </li> </ul>"},{"location":"topics/genAI/#customizing-llms-with-data","title":"Customizing LLMs with Data","text":"<p>Training LLMs from scratch Cons:</p> <ol> <li>Cost: Very expensive to train </li> <li>Data: A lot of data is needed and you need to annotated data (labelled)</li> <li>Expertise: Pretraining is hard and you need to understand what model performance means</li> </ol> <p>3 Options to Customize LLMs:</p> <ol> <li>In-context Learning/Few Shot Prompting</li> <li>Chain of Thought Prompting: Breaking a model down into smaller chunks and give reasoning.</li> <li> <p>Limitation of in context learning: Model Context length (which is the number of tokens it can process)</p> </li> <li> <p>Fine-tuning a pretrained model: optimizing a model on a smaller domain-specific dataset</p> </li> <li>Benefits: a) Improve the model performance on specific tasks</li> <li> <p>b) Improve the model efficiency - reduce the number of tokens needed for your model to perform well on your tasks. </p> </li> <li> <p>Retrieval Augmented Generation (RAG): language model is able to query enterprise knowledge bases and its grounded. These do not require Fine Tuning.</p> </li> <li> <p>Few shot prompting its simple and no training cost but the con is it adds latency to each model. Fine Tuning is used when the LLM does not perform well on a particular task and its more efficient and better performance but the con is it requires a labelled dataset (expensive and time consuming.) RAG is used when the data changes rapidly and it accesses the latest data and grounds results but its complez to setup. </p> </li> <li> <p>Look at <code>LLM Optimization</code> vs <code>Context Optimization</code>:</p> </li> <li>context is what the model needs to know and optimization is how the model needs to act. </li> <li>Always start with prompt engineering and then if its a context issue you do RAG but if its an optimization issue then you do fine tuning. </li> </ol>"},{"location":"topics/genAI/#fine-tuning-and-inference-in-oci-gen-ai","title":"Fine Tuning and Inference in OCI Gen AI","text":"<ul> <li> <p>Inference is using a trained ML to make predictions or decisions based on new input data.</p> </li> <li> <p>Custom Model is on that you create by using a pretrained model as a base and using your own data set to fine-tune the model. </p> </li> </ul> <p>Fine-Tuning Workflow:     1. Create a Dedicated AI Cluster for Fine Tuning    2. Gather training data    3. Kickstart fine tuning    4. Fine-tuned (custom) Model is created</p> <ul> <li>Model Endpoint: is a designated point on an AI Cluster where a LLM can accept user request and send back responses </li> </ul> <p>Inference Worflow:    1. Create Dedicated AI Cluster for Hosting    2. Create Endpoint    3. Serve the Model</p> <p>T-Few Fine Tuning - regular fine-tuning involves updating the weights of all layers in the model which takes longer training time and has more cost. - T-Few only updates a fraction of the models weights. (Few-Shot Parameter Efficient Fine Tuning = PEFT) and this reduce the training time and the cost. ~0.01% of the baselines model size</p> <p>Reducing Inference Costs: - usually inference is expensive  - Each hosting cluster can have 1 base model endpoint and N Fine-tuned custom models. So they share the same GPU resources.  - GPU memory is limited so if you switch between models it can cause alot of overhead since you have to reload the full GPU memory.  - <code>Parameter Sharing</code> reduces the total amount of memory and has minimal overheard. </p>"},{"location":"topics/genAI/#dedicated-ai-clusters","title":"Dedicated AI Clusters","text":"<ul> <li> <p>A single-tenant deployment where the GPU model is used to only host your custom models and since the endpoint is not shared the throughput is consistent</p> </li> <li> <p>2 types:</p> </li> <li>Fine-Tuning</li> <li>Hositng</li> </ul> <p>Dedicated AI Clusters Sizing &amp; Pricing:</p> <p>Different Cluster Unit Types: 1. Large Cohere Dedicated: You can do both fine-tuning and hosting but its limited to the cohere R command family.</p> <ol> <li> <p>Small Cohere Dedicated: Same as above but a smaller count</p> </li> <li> <p>Embed Cohere Dedicated: Used for embedding but no fine-tuning but you can still host. </p> </li> <li> <p>Large Meta Dedicated: Used for both finetuning and hosting but uses Meta Llama models. </p> </li> </ol> <p>Unit Sizing:    1. Cohere Command R+ - Does not support Fine tuning and needs 2 units of large cohere dedicated units for hosting.</p> <ol> <li> <p>Cohere Command R - Supports fine tuning and needs 8 units of small cohere dedicated and for hosting it needs 1 unit of small cohere dedicated.</p> </li> <li> <p>Meta Llmama - It neeeds 4 units of large meta dedicated for fine tuning and 1 unit large meta for hosting. </p> </li> <li> <p>Cohere English Embed - doesnt support fine tuning and needs 1 unit of embed dedicated for hosting. </p> </li> </ol> <p>Pricing Example:    - Cohere Command R 08-2024    - Min hosting commitment = 744unit-hours/cluster    - Fine tuning commitment = 1 unit-hour/fine tuning    - So for hosting you have to pay for the month but fine tuning is on a per hour basis</p>"},{"location":"topics/genAI/#fine-tuning-configuration","title":"Fine-Tuning Configuration","text":"<p>2 Training Methods:   - T-Few   - LoRA: Low rank adaptation    - Both of them are PEFT (parameter efficient fine tuning) ie. they fine tune only a subset parameters.</p> <p>Hyperparameters:    1. Total Training Epochs: the number of times the model is trained using the entire dataset. </p> <ol> <li> <p>Total Batch Size: number of samples processed before updating the model parameters. Large batches speed up learning</p> </li> <li> <p>Learning Rate: How fast the model adjusts it settings. </p> </li> <li> <p>Early Stopping Threshold: When the machine should stop training if its not improving fast enough </p> </li> <li> <p>Early Stopping Patience: How long the machine waits before its not learning.</p> </li> <li> <p>Log model metrics interval in steps: Determines how frequently to log model metrics. </p> </li> </ol> <p>Evaluating Fine-Tuning:</p> <ol> <li> <p>Accuracy: Measures whether the generated tokens match the annotated tokens (labelled output)</p> </li> <li> <p>Loss: Tells you how many predictions the model got wrong. Loss decreases as the model icreases. A loss of 0 means all output was perfect. If the context is simialir then loss is low.  -Loss is the preferred metric because Gen AI doesn't always know what is right</p> </li> <li> <p>Model training set needs to keys: the prompt and the completion.</p> </li> </ol>"},{"location":"topics/genAI/#oci-ai-generative-security","title":"OCI AI Generative Security","text":"<ul> <li> <p>GPUs allocated for a customers gen AI task are isolated from other GPUs</p> </li> <li> <p>dedicated GPU cluster only handles your base and fine-tuned models within your set of GPUs so there is data isolation.</p> </li> <li> <p>Customer data is restricted within the customers tenancy so one customers data cant be seen by another customer. </p> </li> <li> <p>Also uses <code>OCI IAM</code> for authentication and authorization. </p> </li> <li> <p><code>OCI Key Management</code> is used for secrets.</p> </li> <li> <p><code>OCI Object Storage buckets</code> for customer fine tuned models.</p> </li> </ul> <p>Question about periods as a stop sequence: The model stops generating text once it reaches the end of the first sentence, even if the token limit is much higher. Explanation: Stop sequences, in the context of text generation, are special tokens or symbols used to signal the end of the generated text. These sequences serve as markers for the model to halt its generation process. Common stop sequences include punctuation marks such as periods (.), question marks (?), and exclamation marks (!), as they typically denote the end of sentences in natural language.</p>"},{"location":"topics/genAI/#-the-main-advantage-of-using-few-shot-model-prompting-to-customize-a-large-language-model-llm-is-its-ability-to-adapt-the-model-quickly-and-effectively-to-new-tasks-or-domains-with-only-a-small-amount-of-training-data-instead-of-retraining-the-entire-model-from-scratch-which-can-be-time-consuming-and-resource-intensive-few-shot-prompting-leverages-the-models-pre-existing-knowledge","title":"- The main advantage of using few-shot model prompting to customize a Large Language Model (LLM) is its ability to adapt the model quickly and effectively to new tasks or domains with only a small amount of training data. Instead of retraining the entire model from scratch, which can be time-consuming and resource-intensive, few-shot prompting leverages the model's pre-existing knowledge.","text":""},{"location":"topics/genAI/#rag-using-genai-service-oracle-23-ai-vector-search","title":"RAG using GenAI Service &amp; Oracle 23 ai Vector Search","text":""},{"location":"topics/genAI/#oci-genai-integrations","title":"OCI GenAI Integrations","text":"<ul> <li>LangChain provides a wrapper class for using OCI GenAI with LangChain</li> <li>LangChain: is a framework for developing apps powered by language models. </li> </ul> <p>To create a chatbot you need: 1. LLM 2. Prompts 3. Memory 4. Chains 5. Vector Stores 6. Document Loaders </p> <p>2 Main Tyes of LangChain Models:</p> <ol> <li><code>LLMs</code>: Pure text completion models</li> <li><code>Chatbots</code>: tuned specific for having conversations. </li> <li>the core element of a language model is the model</li> </ol> <p>LangChain Prompt Templates: - LangChain prompts can be created using 2 types of langchain prompt templates</p> <ol> <li>String Prompt Template: created from a formatted python string and can have any number of variables and the output is a string. </li> <li> <p>Used for Generation Models</p> </li> <li> <p>Chat Prompt Template: this type of prompt template supports a list of messages and is used for Chat Models. </p> </li> </ol> <p>LangChain Chains: - provides frameworks for creating chains of components including LLMs and other types. </p> <p>2 Ways to create Chains: 1. LCEL: this creates chains declaratively (LangChain Expression Language). This is preferred. </p> <ol> <li> <p>Legacy: creates chains using python classes like LLM Chain</p> </li> <li> <p>Using Chains is how you link between getting user input to generating the response </p> </li> </ol> <p>LangChain Memory - ability to store information about past interactions = <code>memory</code> - Chain interacts with the memory to pass the information along with the question.  -Oracle 23ai can be used a vector store and LangChain can use these vector stores. </p> <ul> <li>Oracle AI Vector Search offers vector utilities to automatically generate vector embeddings from unstructure data</li> <li>Oracle Select AI can generate sql from natural language. </li> </ul>"},{"location":"topics/genAI/#rag","title":"RAG","text":"<ul> <li>Mitigate bias in training data because its getting information from an external data source. </li> <li>Can overcome model limitations by feeding in only the top results from the document instead of the full document</li> <li>handle queries without new training.</li> </ul> <p>RAG Pipeline: 1. Ingestion: Where you have documents that get broken into chunks and then they are turned into embeddings and then indexed into a database.     - <code>Document Loaders</code> are reponsible for loading documents from a variety of sources    - <code>Chunking</code> - Text Splitters take a document and split it into chunks:</p> <p>a. Chunk Size - Many LLMs have max input size constraints so splitting allows us to process documents that would otherwise exceed the limits. But if chunks are too small they wont be semantically useful AND if its too big then it wont be semantically specific. </p> <ul> <li>Embeddings capture semantic relationship</li> <li>Embeddings of similair words are close in the multi-dimensional space</li> <li>Vector embeddings can be generated outside Oracle 23ai db by using 3rd party embedding models</li> <li>Vector embeddings generated inside oracle 23ai use ONNX format</li> </ul> <p>b. Chunk Overlap - is the number of overlapping characters between adjacent chunks. This helps preserve context between chunks. </p> <p>c. Splitting Method - split block of text based on seperators like new line. It tries to retain paragraphs and sentences. </p> <ol> <li>Retrieval: There is a query which then gets searched within the index database and then the system selects the top K most relevant results. </li> <li>A users natural language question is encoded as a vector and sent to AI Vector Search </li> <li>Vector search uses dot product and cosine product (only cares about angle). </li> <li>Less angle means more similarity</li> <li>Vector Indexes Are used for larger sets of data to speed up vector simialirty searches. Uses clustering, partioning, and neighbour graphs to group simialir vectors together. </li> <li> <p>AI Vector search supports HNSW and IVF</p> </li> <li> <p>Generation: Using the top K results a response to the user is generated. </p> </li> </ol>"},{"location":"topics/genAI/#conversational-rag","title":"Conversational RAG","text":"<ul> <li>RAGs and the Chatbot both need to be conversational.</li> <li> <p>Dependence on memory which holds prior chat history to be inserted to the prompt as additional context. </p> </li> <li> <p>In the LangChain framework, memory serves as a dynamic repository for retaining and managing information throughout the system's operation. It allows the framework to maintain state and context, enabling chains to access, reference, and utilize past interactions and information in their decision-making processes.</p> </li> <li> <p>Large Language Models (LLMs) without Retrieval Augmented Generation (RAG) primarily rely on internal knowledge learned during pretraining on a large text corpus. These models are trained on vast amounts of text data, which enables them to learn complex patterns, structures, and relationships within language.</p> </li> </ul>"},{"location":"topics/genAI/#chatbot-using-generative-ai-agent-service","title":"Chatbot using Generative AI Agent Service","text":""},{"location":"topics/genAI/#oracle-generative-ai-agent-service","title":"Oracle Generative AI Agent Service","text":"<ul> <li>Agents Fully managed service that combines LLM with an intelligent retrieval answers from a knowledge base</li> </ul> <p>Architecture: 1. Interface: This is the starting point and this is where the user interacts with the AI agent like a web app or a chatbox</p> <ol> <li> <p>Inputs fed into LLM: Can be <code>Short/Long Term Memory</code>, <code>Tools</code>, and then <code>prompts</code> and there is a knowledge database</p> </li> <li> <p>Response Generation</p> </li> </ol> <p>Agent Concepts:</p> <ul> <li> <p>Generative AI Model: This is the LLM trained on large data </p> </li> <li> <p>Agent: Autonomous system based off the LLM that understands and generates human like text with high answerability and groundness </p> </li> <li> <p>Knowledge Base: Agent connexts to a knowledge base which is vector based</p> </li> <li> <p>Data Source: Data source provide connection to the data store</p> </li> <li> <p>Data Ingestion: extract data from data source and convert it to a structure format and then store in a knowledege base. </p> </li> <li> <p>Session: A series of exchanges where the user sends queries or prompts and the agent responds with relevant information </p> </li> <li> <p>Agent Endpoint: Specific points of access in a network that agents use to interact with other systems or services </p> </li> <li> <p>Trace: Tracks and display the conversation history both the original and generated response </p> </li> <li> <p>Citation: Source of information that the agent uses to respond (ie. document id, page number, etc)</p> </li> <li> <p>Content Moderation: Feature to help detect or filter out certain toxic, violent or abusive phrases from the generated responses and from prompts. </p> </li> </ul> <p>Object Storage Guideline: - Data sources: data for gen AI agents must be uploaded as files to an object storage bucket  - Only one bucket can be allowed per data source - Only pdf and txt files are supported no larger than 100mb - PDF files can have images and charts  - No need to format charts since they are already 2D - You can also use reference tables - Hyperlinks are also shown as clickable links in the chat reponses</p> <p>Oracle Database Guidelines:  - Gen AI Agents dont manage databases so they have to be created ahead of time and within the database you need the following: 1. DOCID 2. Body - is the actual content that you want the agent to search  3. Vector</p> <p>Optional: 4. CHUNKID 5. URL 6. Title 7. Page_Numbers </p> <ul> <li>The embedding model used in the query has to be the same as the embedding model that was used for the database. </li> </ul>"},{"location":"topics/genAI/#chatbot-using-object-store","title":"Chatbot using Object Store","text":"<p>Creating Agents process: 1. Create the knowledge Base    - Data Storage type can be:       a. Object Storage       b. OCI OpenSearch       c. Oracle AI Vector Search</p> <ul> <li>lexical search is key word</li> <li>semantic search is based on context. </li> <li>When restarting the job it will ignore previously read in storage and will only focus on the new content.</li> <li>You can only delete a knowledge base if it is not being used by an agent. </li> <li> <p>Data sources can be deleted at any time.</p> </li> <li> <p>Create the Agent</p> </li> <li> <p>Within the endpoints option is where you create the endpoint to the knowledge base. </p> </li> <li> <p>Test the agent</p> </li> </ul>"},{"location":"topics/genAI/#chatbot-using-oracle-23ai","title":"Chatbot using Oracle 23ai","text":"<ul> <li> <p>Database Tools is used to create a connection to a database and then you can use this tool with the agent that you set up. </p> </li> <li> <p>When you restart a previously run ingestion job, the pipeline detects files that were successfully ingested earlier and skips them. It only ingests files that failed previously and have since been updated.</p> </li> <li> <p>3 is the number of endpoints you can create for each agent.</p> </li> <li> <p>If your data isn't available yet, create an empty folder for the data source and populate it later. This way, you can ingest data into the source after the folder is populated.</p> </li> </ul>"},{"location":"topics/genAI/#additional-notes","title":"Additional Notes","text":"<ul> <li>OCI Generative AI Service = ~40% of the exam</li> <li>Increasing temperature flattens the distribution allowing for more varied words. </li> </ul>"},{"location":"topics/genAI/#sample-question-study-guide","title":"Sample Question Study Guide:","text":"<ol> <li> <p>The OCI Generative AI Agents service retains customer-provided queries and retrieved context used for chatting with Large Language Models (LLMs) during the user's session. However, this data isn't stored beyond the session. Also, the service doesn't use customer chat data or knowledge base data for model training. = Permanently Deleted </p> </li> <li> <p>When a model is deprecated, it remains available for use temporarily. The company should plan to migrate to another model before the deprecation period ends and the model is retired.</p> </li> <li> <p>The OnDemandServingMode is used to configure the Generative AI model to handle requests on-demand, which is suitable for use cases where requests are sporadic or less frequent.</p> </li> <li> <p>Cosine distance (or cosine similarity) measures the angle between two vectors in a high-dimensional space. A cosine distance of 0 (which corresponds to a cosine similarity of 1) means that the vectors are identical in direction, indicating strong semantic similarity between the two embeddings. This is crucial in vector search and retrieval systems, where similar meanings are identified based on direction rather than magnitude.</p> </li> <li> <p>Multi-modal parsing is used to parse and include information from charts and graphs in the documents.</p> </li> <li> <p>In Retrieval-Augmented Generation (RAG), Groundedness ensures that the model s response is factually correct and traceable to retrieved sources, minimizing hallucinations. Answer Relevance, on the other hand, evaluates how well the response aligns with the user s query, ensuring that the retrieved and generated content is contextually appropriate rather than just factually correct. Both are essential for high-quality AI-generated responses.</p> </li> <li> <p>The endpoint variable stores the URL where requests to Oracle's Generative AI inference service are sent. This endpoint acts as the gateway to communicate with the AI model hosted in the specified region.</p> </li> <li> <p>You can only delete knowledge bases that aren't used by agents. Before you delete a knowledge base, you must delete the data sources in that knowledge base and delete the agents using that data source. The delete action permanently deletes the knowledge base. This action can't be undone.</p> </li> <li> <p>If a seed value is provided, the model generates deterministic responses (same input leads to the same output). However, if no seed is specified, the model behaves non-deterministically, producing diverse responses each time it processes the same input.</p> </li> <li> <p>In Retrieval-Augmented Generation (RAG), the ranker evaluates and prioritizes the retrieved information to ensure that the most relevant and high-quality data is passed to the generator. It refines the initial retrieval results by scoring and reordering them based on relevance, improving the accuracy and contextual appropriateness of the generated response. This step is crucial for minimizing irrelevant or misleading outputs.</p> </li> <li> <p>Fine-tuning helps improve model efficiency by adapting a pre-trained model to a specific task or domain, allowing it to generate more relevant responses with fewer input tokens. This reduces computational costs and inference time while maintaining or improving accuracy. By refining the model's knowledge, fine-tuning enhances performance without requiring excessive amounts of new training data.</p> </li> <li> <p>The seed parameter ensures that the model generates deterministic outputs. By setting a fixed seed value (e.g., 123), the model will consistently produce the same response for the same input prompt and parameters. Leaving it as None allows the model to generate varied responses each time. temperature, frequency_penalty, and top_p control aspects of the text generation process, but they do not enforce consistency across multiple runs.</p> </li> <li> <p>Oracle Database typically uses port 1521 for SQL*Net (also known as Oracle Net) connections, which facilitate communication between clients and the database server. Some configurations may also use port 1522 for additional services or failover. When setting up ingress rules in an OCI subnet security list, allowing traffic over ports 1521-1522 ensures that Oracle Database can be accessed properly within the Generative AI Agents environment.</p> </li> <li> <p>Retrieval-Augmented Generation (RAG) is a non-parametric approach because it retrieves relevant information from external data sources at inference time rather than relying solely on pre-trained parameters. This allows RAG to dynamically answer questions based on any corpus without requiring a separate fine-tuned model for each dataset.</p> </li> <li> <p>The cohere.command-r-08-2024 model supports the T-Few and LoRA fine-tuning methods. NOT Vanilla.</p> </li> <li> <p>For on-demand inferencing, the total number of billable characters is the sum of the prompt and response length.</p> </li> <li> <p>In the LangChain framework, a chain typically interacts with memory at two specific points during a run. The first interaction occurs after user input but before chain execution begins. At this stage, the chain may access and retrieve relevant information or context stored in memory to inform its processing or decision-making process. The second interaction with memory occurs after the core logic of the chain has been executed but before generating the final output. At this stage, the chain may update or modify the memory based on the results of its processing, storing any relevant information or intermediate results for future reference or use.</p> </li> <li> <p>Soft prompting involves learning a small set of continuous embeddings that guide the model's behavior without modifying its original parameters. Unlike traditional fine-tuning, soft prompts require no task-specific training of the full model and are efficient in adapting LLMs to different tasks with minimal computational overhead. This makes it ideal for scenarios where full fine-tuning is impractical but some level of customization is needed.</p> </li> <li> <p>The totalTrainingSteps parameter is calculated as: totalTrainingSteps = (totalTrainingEpochs * size(trainingDataset)) / trainingBatchSize This formula determines the total number of training steps based on the number of epochs, the size of the training dataset, and the batch size.</p> </li> <li> <p>A hosting dedicated AI cluster can have up to 50 endpoints.</p> </li> <li> <p>A notification indicates that the endpoint resource is moved to the new compartment successfully. You might notice that the endpoint status changes to Updating. After the move is successful, the endpoint status changes back to Active.</p> </li> </ol>"},{"location":"topics/jenkins/","title":"Jenkins","text":"<p>Automation platform that lets you build, test, and deploy automations using pipelines.</p>"},{"location":"topics/jenkins/#jenkins-infrastructure","title":"Jenkins Infrastructure","text":"<ul> <li>Master Server = Controls the pipelines &amp; schedules builds</li> <li>Agents/Minions Servers = Run the builds </li> </ul>"},{"location":"topics/jenkins/#jenkins-build-types","title":"Jenkins Build Types","text":"<ol> <li>Freestyle Builds - shell scripts that are run on servers based on specific events.</li> <li>Pipeline Builds - Use Jenkins files to define declaritively on how to deploy the build in different stages.</li> </ol>"},{"location":"topics/jenkins/#jenkins-gui","title":"Jenkins GUI","text":"<ul> <li>Manage Jenkins: This contains all of the settings that you need for your Jenins instance such as plugins, global settings, etc.</li> <li><code>System Configuration/Configure System</code></li> <li><code>System Configuration/Manage Plugins</code></li> <li><code>System Configuration/Manage Nodes &amp; Cloud</code>: This is where you setup agents</li> <li><code>Security/Manage Credentials</code>: This where you store <code>SSH keys</code> or <code>API tokens</code></li> <li><code>Tools and Actions/Prepare for Shutdown</code>: You need to use this when you are performing and upgrade or maintanence if you just shutdown the server without doing this step then you will interrupt jobs that are running. </li> </ul>"},{"location":"topics/jenkins/#setting-up-freestyle-jenkins-projects","title":"Setting up Freestyle Jenkins Projects","text":"<ol> <li> <p>Go to the Jenkins Dashboard then click on  New Item. The two most popular types of projects are <code>freestyle</code> and <code>pipelines</code></p> </li> <li> <p>Pick the type of job and give it a name. Make sure to not to put spaces in the name</p> </li> <li> <p>From the build options: </p> <ul> <li> <p><code>Source Code Management</code> is usually always <code>Git</code> and Jenkins will pull that repo that is specified here. You also will mention any branches if you need specific ones. </p> </li> <li> <p>Then we have <code>Build Triggers</code>: You would usually using GitHub webhooks but you need to make sure the firewall or port is open on the Jenkins server so that it can work with the webhook. <code>Build Periodically</code> is used to build jobs on a schedule using cron jobs. </p> </li> <li> <p><code>Build Enviornments</code>: Good to select the 'Delete Workspace' option to clean up any artifacts from previous runs</p> </li> </ul> </li> <li> <p><code>Build</code>:</p> <ul> <li>Most common option is <code>Execute Shell</code></li> </ul> </li> <li> <p><code>Post-Build</code>: Like email notifications</p> </li> <li> <p>After clicking and saving on the build - from the job dashboard you can click <code>Build Now</code></p> </li> </ol> <p></p> <ol> <li> <p>Click on <code>Configure</code> from the Build Homepage to change any settings</p> </li> <li> <p><code>Enviornment Variable</code>:</p> <ul> <li>To see what env variables your build has access to go to <code>Configure</code> on the build and then scroll to Build Steps and check the <code>Execute Shell</code> step and click on \"See the list of available environment variables\"</li> <li><code>Main Enviornment Variables</code>:         - <code>BUILD_ID</code>: Gives you the current build ID and you can use this for docker images         - <code>BUILD_URL</code></li> <li>To see use enviornment variables <code>${VAR_NAME}</code> in your shell script</li> </ul> </li> <li> <p>Reading Console Output:</p> <ul> <li>Any line in the console that is prefixed with a plus sign <code>+</code> means that is a command that is being run</li> </ul> </li> <li> <p><code>Workspace</code>: If there are files that are created and managed by your build they will show up in Jenkins on the Project homepage under the Workspace folder</p> </li> </ol>"},{"location":"topics/jenkins/#jenkins-filesystem","title":"Jenkins Filesystem","text":"<ul> <li><code>cd /var/jenkins_home</code> this will take you to the home directory of Jenkins on the Master Server</li> <li>Within this folder you can navigate to the <code>workspace</code> directory and this is the directory that will contain all of your builds with their job names as the folder name. Within the build folders will be any artifacts of the build.</li> <li>Within the jenkins_home repo also contains other important places to troubleshoot such as:<ul> <li><code>plugins</code>: Contains a list of all the plugins installed on your Jenkins instance</li> <li><code>updates</code>: This contains a list of all the updates that happened</li> <li><code>logs</code>: Houses the log files on the server</li> </ul> </li> </ul>"},{"location":"topics/jenkins/#setting-up-a-python-build-with-a-github-repo","title":"Setting up a Python Build with a GitHub Repo","text":"<ol> <li> <p>Go through the same steps as above when it comes to creating a simple freestyle project but the main thing here is you want to put the GitHub URL in the <code>Source Code Management</code> section. Note: if the repo is private then you would need to add credentials so Jenkins could clone it but if its public then no credentials are needed</p> </li> <li> <p>Since we want to execute a Python build we need to first make sure Python is installed on our master and Jenkins agents. Can do this by remoting into the server and just running the python command in the shell - <code>python or python3</code></p> </li> <li> <p>Then just run the script from the repo <code>python3 script_name.py</code></p> </li> <li> <p>This is valuable because you can run python jobs via Jenkins anytime you want without having to SSH into servers so if you setup a scheduled build from Jenkins thats linked to a Python script from a repo you can run it fairly smooth</p> </li> </ol>"},{"location":"topics/jenkins/#setting-up-jenkins-agentsworkers","title":"Setting up Jenkins Agents/Workers","text":"<ol> <li> <p>Go to <code>Manage Jenkins/Nodes</code> and this where you will see the Jenkins Master and Nodes/Agents that are setup Configure Cloud is how you build out cloud agents like Docker to use instead of pernanet ones.</p> </li> <li> <p>To create a <code>Docker</code> agent go to <code>Cloud</code> then go to install plugins which will automatically filter <code>Cloud Providers</code> and install Docker then restart Jenkins</p> </li> <li> <p>You can login to the <code>Master Jenkins Node</code> and go to the logs and plugins folder to also verify if the plugin is installed. Also, refreshing the Jenkins page might need to be done as the UI will hang on the refresh portion after installing the plugin</p> </li> <li> <p>Now you can go back to the <code>Configure Cloud</code> option under nodes and add Docker.</p> </li> <li> <p>Setting up Docker:</p> <ul> <li>Need to provide the <code>Docker Host URI</code> which can be another server or if you want to do it locally you can use Docker Desktop and an alpine image in the URI field need to put in tcp://IPAddress that is generated   After entering in the data <code>set it to enabled</code> then <code>test connection</code></li> </ul> </li> <li> <p>Creating the <code>Docker Agent Template</code>:</p> <ul> <li> <p>Go back into the configured Docker agent and then click on the gear icon and then navigate to <code>Docker Agent Templates</code> and then click on Add</p> </li> <li> <p>Key Terms:         - <code>Labels</code>: This is used to help the Master node determine which agent to send the build too.         - <code>Docker Image</code>: This will be the official image that you will be using.         - <code>Instance Capacity</code>: This defines how much instances of the agent will be created.         - <code>Remote File System Root</code>: This defines where the workspace for this agent will be created - default: <code>/home/jenkins</code></p> </li> </ul> </li> </ol>"},{"location":"topics/jenkins/#configuring-jobs-to-agents","title":"Configuring Jobs to Agents","text":"<ol> <li> <p>Go to the Job Name then click on \"Configure\" then under General click on the check box for <code>Restrict where this project can run</code> and now put in the name of the docker agent template then click save</p> </li> <li> <p>Now when you build the job it will use the specific agent that you assigned the docker-alpine label too. Note: It's important that you use the correct image because outdated ones will keep your job in pending state as it won't be able to find a live agent since an incorrect image will lead to provisioning errors</p> </li> <li> <p>For these Docker Jobs on the same screen of the Console Output you will see a tab called <code>Built on Docker</code> which shows the container details. </p> </li> <li> <p>This helps you troubleshoot because some agents might not have the required software like Python3 so if you assign this agent template that does not have python to a build that requires it then it will error out. In this scenario you should create your own Docker Image that has python installed</p> </li> <li> <p>To create another agent you just need to go back to the <code>Docker Agent Template</code> and then create another template.</p> </li> </ol>"},{"location":"topics/jenkins/#_1","title":"Jenkins","text":""},{"location":"topics/jenkins/#adding-jenkins-triggers","title":"Adding Jenkins Triggers","text":"<ol> <li>Build Triggers/<code>Poll SCM</code>: Jenkins Master will periodically check github for any changes and its much easier to manage then setting up webhooks.<ul> <li>It uses cron notation </li> </ul> </li> </ol>"},{"location":"topics/jenkins/#setting-up-jenkin-pipelines","title":"Setting up Jenkin Pipelines","text":"<ol> <li> <p>Similair to how we created freestyle projects you need to go to <code>New Item</code> and then click on <code>Pipeline</code>. You will notice that at the top most of the settings are the same but you have less freedom with pipelines for advanced settings as most of the steps are carried out by the <code>Pipeline Script</code> section.</p> </li> <li> <p>There are two ways to build out the pipeline script and both ways use the <code>Groovy Syntax</code>:</p> <ul> <li><code>Directly in the UI</code></li> <li><code>Jenkinsfile</code></li> </ul> </li> </ol>"},{"location":"topics/jenkins/#jenkin-pipeline-syntax","title":"Jenkin Pipeline Syntax","text":"<ul> <li>Everything is wrapped in a <code>pipeline</code> parameter {}</li> <li>First step in the pipeline is to select the <code>agent</code> that will carry out the job and it is specified by the <code>label</code> parameter</li> <li>Next step is the <code>Stages</code>: This is where you define your stages like building -&gt; testing -&gt; deploying</li> <li>When you create a Pipeline Build you can see a section for Pipleline Overview which will show you all of the stages you built. </li> </ul>"},{"location":"topics/jenkins/#jenkinsfile","title":"Jenkinsfile","text":"<ul> <li> <p>Instead of putting the script directly into the UI of Jenkins you can create a <code>Jenkinsfile</code> within the parent folder of your code repository and then outline all of the deployment steps in this special file</p> </li> <li> <p>Within the Pipeline build in Jenkins, you need to change the pipeline from using Pipeline Script to use <code>Pipeline Script from SCM</code> then add the github repo. Make sure you put the <code>Jenkinsfile</code> path in the <code>Script Path</code></p> </li> <li> <p>When you get the pipeline from SCM it will add a first step where it sees if it can checkout from SCM without any errors. </p> </li> </ul> Standardized Jenkinsfile<pre><code>pipeline {\n    agent any  // Use any available Jenkins agent/node\n\n    // Parameters allow customization without changing the Jenkinsfile\n    parameters {\n        string(name: 'GIT_REPO', defaultValue: 'https://github.com/your-org/your-repo.git', description: 'GitHub repository URL')\n        string(name: 'GIT_BRANCH', defaultValue: 'develop', description: 'Git branch to build')\n        string(name: 'GITHUB_CREDENTIALS_ID', defaultValue: 'github-credentials-id', description: 'Jenkins credentials ID for GitHub access')\n        string(name: 'SONARQUBE_PROJECT_KEY', defaultValue: 'my-app', description: 'SonarQube project key')\n        string(name: 'SONARQUBE_TOKEN', defaultValue: 'sonarqube-token-id', description: 'Jenkins credentials ID for SonarQube token')\n        choice(name: 'BUILD_ENV', choices: ['dev', 'qa', 'prod'], description: 'Deployment environment')\n    }\n\n    environment {\n        // Common environment variables for builds\n        NODE_VERSION = '18'\n        JAVA_VERSION = '17'\n        MAVEN_HOME = tool name: 'Maven 3', type: 'maven'\n        // This assumes Jenkins SonarQube plugin is configured as \"sonarqube\"\n        SONARQUBE_ENV = 'sonarqube'\n    }\n\n    triggers {\n        // Automatically build when there are new commits in the repo\n        pollSCM('H/5 * * * *') // Every 5 minutes\n    }\n\n    stages {\n        stage('Checkout') {\n            steps {\n                // Clone code from GitHub using stored credentials\n                git branch: \"${params.GIT_BRANCH}\",\n                    credentialsId: \"${params.GITHUB_CREDENTIALS_ID}\",\n                    url: \"${params.GIT_REPO}\"\n            }\n        }\n\n        stage('Set Up Node') {\n            steps {\n                // Install Node version for frontend builds\n                sh \"\"\"\n                    echo \"Setting up Node.js ${NODE_VERSION}\"\n                    nvm install ${NODE_VERSION}\n                    nvm use ${NODE_VERSION}\n                \"\"\"\n            }\n        }\n\n        stage('Set Up Java') {\n            steps {\n                // Configure Java for backend builds\n                sh \"java -version\"\n                sh \"javac -version\"\n            }\n        }\n\n        stage('Install Frontend Dependencies') {\n            when { expression { fileExists('frontend/package.json') } }\n            steps {\n                dir('frontend') {\n                    sh \"npm install\"\n                }\n            }\n        }\n\n        stage('Build Backend') {\n            when { expression { fileExists('backend/pom.xml') } }\n            steps {\n                dir('backend') {\n                    sh \"${MAVEN_HOME}/bin/mvn clean install -DskipTests\"\n                }\n            }\n        }\n\n        stage('Run Unit Tests') {\n            parallel {\n                stage('Frontend Tests') {\n                    when { expression { fileExists('frontend/package.json') } }\n                    steps {\n                        dir('frontend') {\n                            sh \"npm test\"\n                        }\n                    }\n                }\n                stage('Backend Tests') {\n                    when { expression { fileExists('backend/pom.xml') } }\n                    steps {\n                        dir('backend') {\n                            sh \"${MAVEN_HOME}/bin/mvn test\"\n                        }\n                    }\n                }\n            }\n        }\n\n        stage('SonarQube Analysis') {\n            steps {\n                withSonarQubeEnv(\"${SONARQUBE_ENV}\") {\n                    withCredentials([string(credentialsId: \"${params.SONARQUBE_TOKEN}\", variable: 'SONAR_TOKEN')]) {\n                        dir('backend') {\n                            // You can run separate Sonar scans for frontend/backend if needed\n                            sh \"\"\"\n                                ${MAVEN_HOME}/bin/mvn sonar:sonar \\\n                                    -Dsonar.projectKey=${params.SONARQUBE_PROJECT_KEY} \\\n                                    -Dsonar.login=$SONAR_TOKEN\n                            \"\"\"\n                        }\n                    }\n                }\n            }\n        }\n\n        stage('Conditional Deployment') {\n            when {\n                anyOf {\n                    branch pattern: \"feature/.*\", comparator: \"REGEXP\"\n                    branch pattern: \"hotfix/.*\", comparator: \"REGEXP\"\n                    branch 'main'\n                    branch 'develop'\n                }\n            }\n            steps {\n                sh \"\"\"\n                    echo \"Deploying to ${params.BUILD_ENV} environment\"\n                    # Add deployment scripts or commands here\n                \"\"\"\n            }\n        }\n    }\n\n    post {\n        success {\n            echo \"Pipeline completed successfully.\"\n        }\n        failure {\n            echo \"Pipeline failed. Check logs for details.\"\n        }\n        always {\n            cleanWs() // Clean workspace after build\n        }\n    }\n}\n</code></pre>"},{"location":"topics/jenkins/#settings-up-jenkins-agents-on-windows-server","title":"Settings up Jenkins Agents on Windows Server","text":"<ul> <li>Need to make sure the version of Java that you install on the Windows Server is the same one that is installed on the Jenkins Controller</li> <li>To find the version, go to <code>Manage Jenkins</code> then go to <code>System Information</code> and search for java.home</li> <li> <p>Adding these Nodes:</p> <ol> <li>Click on Manage Configurations and then go to Nodes and create a new node</li> <li>Provide a name and then click on <code>Permanent</code> this it is not a cloud or dynamic server.</li> <li>The remote root directory will be a path on the server: <code>d:\\tools\\jenkins-agent</code></li> <li>If you leave the <code>label</code> blank it will take on the name of the agent.</li> <li>Usage needs to be set as only when using the label name</li> <li> <p><code>Websocket</code> If you don't select this option then you need to just make sure you open a specific port on the agent so that it can communicate with the master via <code>tcp</code>. When you save this in the Jenkins Master it will save the connection type but will be marked with a red X because you need to connect the agent to the master. If you click on the red X it will give you commands to run on the agent</p> </li> <li> <p>For exectuable services you can leverage <code>WinSW</code> which is a wrapper for any executable so that it can be run as a Windows Service</p> <ul> <li>The way this works is you rename the WinSW ex as your jenkins agent and then need an xml file that defines what it runs</li> <li>The arguments in the xml come from the Jenkins UI when you click on the red X</li> <li>The agent.jar also comes from this locaton after you click on the red X</li> <li>After you start the service then it will connect to the master</li> </ul> </li> </ol> </li> </ul>"},{"location":"topics/jenkins/#jenkins-multibranch-pipelines","title":"Jenkins Multibranch Pipelines","text":"<ol> <li> <p>Creating a <code>GitHub App</code>: Go to GitHub and click on Settings then go to Developer Settings then create the GitHub App and Set the permissions. Then set the subscribe events like push, repository, and other events.</p> </li> <li> <p>To add credentials in Jenkins: Click on <code>Manage Jenkins\\Credentials</code> then click on the Global one and then add credential and add for a GitHub app. PAT (personal access token) gives you a lot less GitHub limits whereas a GitHub app lets you call to GitHub a lot more</p> </li> <li> <p>Create a new item and select <code>Multi-Branch Pipeline</code>:</p> <ul> <li>For branch sources select GitHub</li> <li>If you dont have a Jenkinsfile then there will be no build configuration since you are selecting builds based on this type of file</li> </ul> </li> <li> <p>The way the multibranch pipeline works is that Jenkins scans the repo for every branch name and allows you to see that in the app homepage view:</p> <ul> <li><code>On Different Branches you can modify the Jenkinsfile so that different things can be carried out</code></li> </ul> </li> </ol> <p>For Example, you can create stages in the pipeline based on a specific type of branch <pre><code>stage('fix branch){\n  when{\n    branch \"fix-*\"\n  }\n  steps{\n    sh ```\n    cat README.md\n    ```\n  }\n}\n\nstage('merge pr){\n  when{\n    branch \"pr-*\"\n  }\n  steps{\n    echo \"this is for prs\"\n  }\n}\n</code></pre></p> <ol> <li><code>Dealing with Pull Requests</code>: Now you need to create a PR to merge this branch back into the <code>main</code> which will in this scenario also update the root jenkins file so going forward each type of branch will have a specific pathway<ul> <li>Pull Requests also show up in the Jenkins build page for the multibranch pipeline</li> <li>Any PR or branch that has a strikethough means that it was already <code>merged and then the branch deleted</code> so it no longer exists These go away next time a scan occurs</li> </ul> </li> </ol>"},{"location":"topics/scripts/","title":"Scripts","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nGitHub Runbook Generator\nCreates a formatted Word document runbook based on GitHub repository contents.\n\"\"\"\n\nimport requests\nimport re\nfrom urllib.parse import urlparse\nfrom docx import Document\nfrom docx.shared import Inches\nfrom docx.enum.text import WD_PARAGRAPH_ALIGNMENT\nfrom docx.enum.style import WD_STYLE_TYPE\nimport sys\nimport argparse\nimport urllib3\nimport os\nfrom dotenv import load_dotenv\n\n# Disable SSL warnings (use only if you're okay with less secure connections)\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n# Load environment variables\nload_dotenv()\n\ndef extract_repo_info(github_url):\n    \"\"\"Extract owner and repo name from GitHub URL\"\"\"\n    # Handle different GitHub URL formats\n    patterns = [\n        r'github\\.com/([^/]+)/([^/]+?)(?:\\.git)?/?\n\ndef get_github_files(owner, repo, branch_version):\n    \"\"\"Fetch file names from specified GitHub branch\"\"\"\n    # GitHub API endpoint for repository contents\n    api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents\"\n\n    # First, get all branches to find the one matching our version\n    branches_url = f\"https://api.github.com/repos/{owner}/{repo}/branches\"\n\n    print(f\"Debug: Trying to fetch branches from: {branches_url}\")\n\n    try:\n        # Get GitHub token from environment variable\n        github_token = os.getenv('GITHUB_TOKEN')\n        headers = {}\n        if github_token:\n            headers['Authorization'] = f'token {github_token}'\n            print(\"Debug: Using GitHub token for authentication\")\n        else:\n            print(\"Debug: No GitHub token found - only public repos will work\")\n\n        print(f\"Debug: Making request to GitHub API...\")\n        response = requests.get(branches_url, headers=headers, timeout=30, verify=False)\n        print(f\"Debug: Response status code: {response.status_code}\")\n        if response.status_code == 404:\n            print(\"Debug: 404 error - Repository not found. Possible causes:\")\n            print(\"1. Repository doesn't exist\")\n            print(\"2. Repository is private (requires authentication)\")\n            print(\"3. Repository name is incorrect\")\n            print(f\"Debug: Please verify the repository exists at: https://github.com/{owner}/{repo}\")\n        response.raise_for_status()\n        branches = response.json()\n\n        # Find branch by version (look for exact match or containing the version)\n        target_branch = None\n        for branch in branches:\n            # Try exact match first\n            if branch['name'] == branch_version:\n                target_branch = branch['name']\n                break\n            # Then try if version is contained in branch name\n            elif branch_version in branch['name']:\n                target_branch = branch['name']\n                break\n\n        if not target_branch:\n            # If no branch found with the version, try using the version directly\n            target_branch = branch_version\n\n        # Get files from the target branch - add timeout and SSL handling\n        params = {'ref': target_branch}\n        response = requests.get(api_url, params=params, headers=headers, timeout=30, verify=False)\n        response.raise_for_status()\n\n        files_data = response.json()\n\n        # Extract file names, remove extensions, and filter\n        file_names = set()\n        for item in files_data:\n            if item['type'] == 'file':\n                filename = item['name']\n                # Skip README files\n                if filename.lower().startswith('readme'):\n                    continue\n\n                # Remove extension\n                name_without_ext = filename.split('.')[0]\n                if name_without_ext:  # Only add non-empty names\n                    file_names.add(name_without_ext)\n\n        return list(file_names), target_branch\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching from GitHub API: {e}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Error processing GitHub data: {e}\")\n        sys.exit(1)\n\ndef create_runbook_document(repo_name, file_names):\n    \"\"\"Create a professionally formatted Word document\"\"\"\n    doc = Document()\n\n    # Set up document styles\n    styles = doc.styles\n\n    # Title style\n    title_style = styles.add_style('CustomTitle', WD_STYLE_TYPE.PARAGRAPH)\n    title_style.font.size = Inches(0.25)\n    title_style.font.bold = True\n    title_style.font.name = 'Arial'\n\n    # Section heading style\n    heading_style = styles.add_style('CustomHeading', WD_STYLE_TYPE.PARAGRAPH)\n    heading_style.font.size = Inches(0.18)\n    heading_style.font.bold = True\n    heading_style.font.name = 'Arial'\n\n    # Normal text style\n    normal_style = styles.add_style('CustomNormal', WD_STYLE_TYPE.PARAGRAPH)\n    normal_style.font.size = Inches(0.14)\n    normal_style.font.name = 'Arial'\n\n    # Document Title\n    title = doc.add_paragraph('RUNBOOK', style='CustomTitle')\n    title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n    doc.add_paragraph()  # Add space\n\n    # Section 1: Scope\n    scope_heading = doc.add_paragraph('1. Scope', style='CustomHeading')\n    scope_desc = doc.add_paragraph(\n        'This runbook provides step-by-step procedures for deployment, configuration, '\n        'and maintenance operations. It covers the complete workflow from initial setup '\n        'through final deployment and includes rollback procedures for emergency situations.',\n        style='CustomNormal'\n    )\n    doc.add_paragraph()\n\n    # Add file names list\n    if file_names:\n        doc.add_paragraph('Components included in this runbook:', style='CustomNormal')\n        for name in sorted(file_names):\n            doc.add_paragraph(f'\u2022 {name}', style='CustomNormal')\n    doc.add_paragraph()\n\n    # Section 2: Login\n    login_heading = doc.add_paragraph('2. Login', style='CustomHeading')\n    doc.add_paragraph('ssh test', style='CustomNormal')\n    doc.add_paragraph('sudo ss', style='CustomNormal')\n    doc.add_paragraph()\n\n    # Section 3: Export\n    export_heading = doc.add_paragraph('3. Export', style='CustomHeading')\n    doc.add_paragraph('export test', style='CustomNormal')\n    doc.add_paragraph('export test2', style='CustomNormal')\n    doc.add_paragraph(f'export test/{repo_name}', style='CustomNormal')\n    doc.add_paragraph()\n\n    # Section 4: Download\n    download_heading = doc.add_paragraph('4. Download', style='CustomHeading')\n    doc.add_paragraph('cd test', style='CustomNormal')\n    doc.add_paragraph('downlod', style='CustomNormal')  # Keeping the typo as specified\n    doc.add_paragraph()\n\n    # Section 5: Upload\n    upload_heading = doc.add_paragraph('5. Upload', style='CustomHeading')\n    doc.add_paragraph('upload.sh', style='CustomNormal')\n    doc.add_paragraph()\n\n    # Section 6: Release\n    release_heading = doc.add_paragraph('6. Release', style='CustomHeading')\n    if file_names:\n        for name in sorted(file_names):\n            doc.add_paragraph(f'test.sh {name}', style='CustomNormal')\n    doc.add_paragraph()\n\n    # Section 7: Rollback\n    rollback_heading = doc.add_paragraph('7. Rollback', style='CustomHeading')\n    doc.add_paragraph('cd test', style='CustomNormal')\n    doc.add_paragraph('export=1', style='CustomNormal')\n    doc.add_paragraph('cd test2', style='CustomNormal')\n    doc.add_paragraph()\n\n    if file_names:\n        # First list with roll.sh\n        for name in sorted(file_names):\n            doc.add_paragraph(f'roll.sh {name}', style='CustomNormal')\n        doc.add_paragraph()\n\n        # Second list with st.sh\n        for name in sorted(file_names):\n            doc.add_paragraph(f'st.sh {name}', style='CustomNormal')\n\n    return doc\n\ndef main():\n    # OPTION 1: Use command line arguments (recommended)\n    parser = argparse.ArgumentParser(description='Generate runbook from GitHub repository')\n    parser.add_argument('github_url', help='GitHub repository URL')\n    parser.add_argument('branch_version', help='Branch version to process (e.g., 1.11.0)')\n    parser.add_argument('-o', '--output', default='runbook.docx', help='Output filename (default: runbook.docx)')\n\n    args = parser.parse_args()\n\n    # OPTION 2: Hardcode your values here (uncomment and modify these lines, then comment out the argparse section above)\n    # class Args:\n    #     def __init__(self):\n    #         self.github_url = \"https://github.com/your-username/your-repo\"  # PUT YOUR GITHUB URL HERE\n    #         self.branch_version = \"1.11.0\"  # PUT YOUR BRANCH VERSION HERE\n    #         self.output = \"runbook.docx\"\n    # args = Args()\n\n    try:\n        # Extract repository information\n        owner, repo_name = extract_repo_info(args.github_url)\n        print(f\"Processing repository: {owner}/{repo_name}\")\n\n        # Get files from GitHub\n        print(f\"Fetching files from branch {args.branch_version}...\")\n        file_names, branch_used = get_github_files(owner, repo_name, args.branch_version)\n        print(f\"Found {len(file_names)} files in branch '{branch_used}'\")\n\n        if file_names:\n            print(\"Files found:\", \", \".join(sorted(file_names)))\n        else:\n            print(\"No files found (excluding README files)\")\n\n        # Create the Word document\n        print(\"Creating runbook document...\")\n        doc = create_runbook_document(repo_name, file_names)\n\n        # Save the document\n        doc.save(args.output)\n        print(f\"Runbook saved as: {args.output}\")\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    # If running without command line arguments, show usage\n    if len(sys.argv) == 1:\n        print(\"GitHub Runbook Generator\")\n        print(\"Usage: python script.py &lt;github_url&gt; &lt;branch_version&gt; [-o output_file]\")\n        print()\n        print(\"Example:\")\n        print(\"python script.py https://github.com/user/repo 1.11.0\")\n        print(\"python script.py https://github.com/user/repo v2.5.3 -o my_runbook.docx\")\n        sys.exit(1)\n\n    main()\n,\n        r'github\\.com/([^/]+)/([^/]+?)/.*',\n    ]\n\n    for pattern in patterns:\n        match = re.search(pattern, github_url)\n        if match:\n            owner = match.group(1)\n            repo = match.group(2)\n            print(f\"Debug: Extracted owner='{owner}', repo='{repo}' from URL: {github_url}\")\n            return owner, repo\n\n    raise ValueError(\"Invalid GitHub URL format\")\n\ndef get_github_files(owner, repo, branch_version):\n    \"\"\"Fetch file names from specified GitHub branch\"\"\"\n    # GitHub API endpoint for repository contents\n    api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents\"\n\n    # First, get all branches to find the one matching our version\n    branches_url = f\"https://api.github.com/repos/{owner}/{repo}/branches\"\n\n    try:\n        # Get branches - add timeout and SSL handling\n        response = requests.get(branches_url, timeout=30, verify=False)\n        response.raise_for_status()\n        branches = response.json()\n\n        # Find branch by version (look for exact match or containing the version)\n        target_branch = None\n        for branch in branches:\n            # Try exact match first\n            if branch['name'] == branch_version:\n                target_branch = branch['name']\n                break\n            # Then try if version is contained in branch name\n            elif branch_version in branch['name']:\n                target_branch = branch['name']\n                break\n\n        if not target_branch:\n            # If no branch found with the version, try using the version directly\n            target_branch = branch_version\n\n        # Get files from the target branch - add timeout and SSL handling\n        params = {'ref': target_branch}\n        response = requests.get(api_url, params=params, timeout=30, verify=False)\n        response.raise_for_status()\n\n        files_data = response.json()\n\n        # Extract file names, remove extensions, and filter\n        file_names = set()\n        for item in files_data:\n            if item['type'] == 'file':\n                filename = item['name']\n                # Skip README files\n                if filename.lower().startswith('readme'):\n                    continue\n\n                # Remove extension\n                name_without_ext = filename.split('.')[0]\n                if name_without_ext:  # Only add non-empty names\n                    file_names.add(name_without_ext)\n\n        return list(file_names), target_branch\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching from GitHub API: {e}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Error processing GitHub data: {e}\")\n        sys.exit(1)\n\ndef create_runbook_document(repo_name, file_names):\n    \"\"\"Create a professionally formatted Word document\"\"\"\n    doc = Document()\n\n    # Set up document styles\n    styles = doc.styles\n\n    # Title style\n    title_style = styles.add_style('CustomTitle', WD_STYLE_TYPE.PARAGRAPH)\n    title_style.font.size = Inches(0.25)\n    title_style.font.bold = True\n    title_style.font.name = 'Arial'\n\n    # Section heading style\n    heading_style = styles.add_style('CustomHeading', WD_STYLE_TYPE.PARAGRAPH)\n    heading_style.font.size = Inches(0.18)\n    heading_style.font.bold = True\n    heading_style.font.name = 'Arial'\n\n    # Normal text style\n    normal_style = styles.add_style('CustomNormal', WD_STYLE_TYPE.PARAGRAPH)\n    normal_style.font.size = Inches(0.14)\n    normal_style.font.name = 'Arial'\n\n    # Document Title\n    title = doc.add_paragraph('RUNBOOK', style='CustomTitle')\n    title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n    doc.add_paragraph()  # Add space\n\n    # Section 1: Scope\n    scope_heading = doc.add_paragraph('1. Scope', style='CustomHeading')\n    scope_desc = doc.add_paragraph(\n        'This runbook provides step-by-step procedures for deployment, configuration, '\n        'and maintenance operations. It covers the complete workflow from initial setup '\n        'through final deployment and includes rollback procedures for emergency situations.',\n        style='CustomNormal'\n    )\n    doc.add_paragraph()\n\n    # Add file names list\n    if file_names:\n        doc.add_paragraph('Components included in this runbook:', style='CustomNormal')\n        for name in sorted(file_names):\n            doc.add_paragraph(f'\u2022 {name}', style='CustomNormal')\n    doc.add_paragraph()\n\n    # Section 2: Login\n    login_heading = doc.add_paragraph('2. Login', style='CustomHeading')\n    doc.add_paragraph('ssh test', style='CustomNormal')\n    doc.add_paragraph('sudo ss', style='CustomNormal')\n    doc.add_paragraph()\n\n    # Section 3: Export\n    export_heading = doc.add_paragraph('3. Export', style='CustomHeading')\n    doc.add_paragraph('export test', style='CustomNormal')\n    doc.add_paragraph('export test2', style='CustomNormal')\n    doc.add_paragraph(f'export test/{repo_name}', style='CustomNormal')\n    doc.add_paragraph()\n\n    # Section 4: Download\n    download_heading = doc.add_paragraph('4. Download', style='CustomHeading')\n    doc.add_paragraph('cd test', style='CustomNormal')\n    doc.add_paragraph('downlod', style='CustomNormal')  # Keeping the typo as specified\n    doc.add_paragraph()\n\n    # Section 5: Upload\n    upload_heading = doc.add_paragraph('5. Upload', style='CustomHeading')\n    doc.add_paragraph('upload.sh', style='CustomNormal')\n    doc.add_paragraph()\n\n    # Section 6: Release\n    release_heading = doc.add_paragraph('6. Release', style='CustomHeading')\n    if file_names:\n        for name in sorted(file_names):\n            doc.add_paragraph(f'test.sh {name}', style='CustomNormal')\n    doc.add_paragraph()\n\n    # Section 7: Rollback\n    rollback_heading = doc.add_paragraph('7. Rollback', style='CustomHeading')\n    doc.add_paragraph('cd test', style='CustomNormal')\n    doc.add_paragraph('export=1', style='CustomNormal')\n    doc.add_paragraph('cd test2', style='CustomNormal')\n    doc.add_paragraph()\n\n    if file_names:\n        # First list with roll.sh\n        for name in sorted(file_names):\n            doc.add_paragraph(f'roll.sh {name}', style='CustomNormal')\n        doc.add_paragraph()\n\n        # Second list with st.sh\n        for name in sorted(file_names):\n            doc.add_paragraph(f'st.sh {name}', style='CustomNormal')\n\n    return doc\n\ndef main():\n    # OPTION 1: Use command line arguments (recommended)\n    parser = argparse.ArgumentParser(description='Generate runbook from GitHub repository')\n    parser.add_argument('github_url', help='GitHub repository URL')\n    parser.add_argument('branch_version', help='Branch version to process (e.g., 1.11.0)')\n    parser.add_argument('-o', '--output', default='runbook.docx', help='Output filename (default: runbook.docx)')\n\n    args = parser.parse_args()\n\n    # OPTION 2: Hardcode your values here (uncomment and modify these lines, then comment out the argparse section above)\n    # class Args:\n    #     def __init__(self):\n    #         self.github_url = \"https://github.com/your-username/your-repo\"  # PUT YOUR GITHUB URL HERE\n    #         self.branch_version = \"1.11.0\"  # PUT YOUR BRANCH VERSION HERE\n    #         self.output = \"runbook.docx\"\n    # args = Args()\n\n    try:\n        # Extract repository information\n        owner, repo_name = extract_repo_info(args.github_url)\n        print(f\"Processing repository: {owner}/{repo_name}\")\n\n        # Get files from GitHub\n        print(f\"Fetching files from branch {args.branch_version}...\")\n        file_names, branch_used = get_github_files(owner, repo_name, args.branch_version)\n        print(f\"Found {len(file_names)} files in branch '{branch_used}'\")\n\n        if file_names:\n            print(\"Files found:\", \", \".join(sorted(file_names)))\n        else:\n            print(\"No files found (excluding README files)\")\n\n        # Create the Word document\n        print(\"Creating runbook document...\")\n        doc = create_runbook_document(repo_name, file_names)\n\n        # Save the document\n        doc.save(args.output)\n        print(f\"Runbook saved as: {args.output}\")\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    # If running without command line arguments, show usage\n    if len(sys.argv) == 1:\n        print(\"GitHub Runbook Generator\")\n        print(\"Usage: python script.py &lt;github_url&gt; &lt;branch_version&gt; [-o output_file]\")\n        print()\n        print(\"Example:\")\n        print(\"python script.py https://github.com/user/repo 1.11.0\")\n        print(\"python script.py https://github.com/user/repo v2.5.3 -o my_runbook.docx\")\n        sys.exit(1)\n\n    main()\n</code></pre>"}]}