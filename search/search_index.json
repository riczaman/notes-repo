{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcda My Technical Notes","text":"<p>Welcome to my personal knowledge base. Here, I document new technologies, tools, and concepts I\u2019m learning to grow as a software engineer.</p>"},{"location":"#purpose","title":"Purpose","text":"<ul> <li>Keep a clear, organized record of what I learn.</li> <li>Create a quick reference I can revisit anytime.</li> <li>Share knowledge in a structured, accessible way.</li> </ul>"},{"location":"#topics-covered","title":"Topics Covered","text":"<ul> <li>Programming languages and frameworks</li> <li>DevOps tools and automation</li> <li>Cloud platforms</li> <li>Software design and architecture</li> </ul>"},{"location":"#how-to-navigate","title":"How to Navigate","text":"<p>Use the menu on the left to explore topics. Each section contains: - Overview of the technology - Step-by-step examples - Screenshots or diagrams - Key takeaways</p>"},{"location":"#how-i-learn","title":"How I learn","text":""},{"location":"#first-topic","title":"First Topic","text":"<p>Start with Jenkins \u2014 an automation server for building, testing, and deploying software.</p>"},{"location":"topics/actions/","title":"GitHub Actions","text":"<p>Platform to automate Developer Workflows. CI/CD pipelines is just one of the many workflows that GitHub Actions can be used for. - Any GitHub event that occurs can trigger a workflow which is an automated execution of tasks = GitHub Action</p>"},{"location":"topics/actions/#cicd-pipelines","title":"CI&amp;CD Pipelines","text":"<ol> <li>commit code</li> <li>test code</li> <li>build code</li> <li>push artifact</li> <li>deploy artifact on server</li> </ol>"},{"location":"topics/actions/#github-action-cicd-example","title":"GitHub Action CI/CD Example","text":"<ul> <li> <p>GitHub Actions already has a bunch of workflow templates for applications. 3 types of workflows: Deployment, continious integration build and test, and continious integration publish workflows </p> </li> <li> <p>If you go to <code>Actions</code> on your repo and then select a <code>workflow</code> as a template than it will automatically create the following:</p> </li> <li><code>.github/workflows</code>: Creates the folder that holds the Action workflow</li> <li><code>*.yml</code>: The yaml file that contains the instuctions</li> </ul>"},{"location":"topics/actions/#workflow-syntax","title":"Workflow Syntax","text":"<p>Main Parameters include: <pre><code>   - `name`: This is just the name of the Action\n   - `on`: This is the section that describes the event you can also do multiple events like *on and pull request*\n        `push`: \n            `branches`: [Master]\n\n    - `jobs`: A set of actions that get executed \n\n        -  `runs-on`: ubuntu-latest\n\n        - `steps`: \n           - `uses`: actions/checkout@v2 - This is a predefined action that handles your repo checkout that was made my GitHub. If you go to github.com/actions you can see the full list of pre-defined actions \n\n           - name: Set up JDK 1.8\n                uses: actions/setup-java@v1\n                with:\n                    java-version: 1.8\n\n             - name: Grant execute permission for gradlew\n                run: chmod +x gradlew    #Whenver we use an action we use the *uses* keyword but when we need to run a specific command use *run*\n</code></pre>     - When you push to master or make a PR to master that above action will automatically start</p> <pre><code>- The code is run on GitHub servers which means you do not have to manage them. Each new job runs on a new virtual enviornment. In the above example, we only put one job but you can put multiple jobs and they will all run on different virtual machines.\n   - **By default the jobs run in parallel but you can overwrite this by using the keyword needs under the second jobs like `needs: firstjob`**\n\n- The runs on command determines what OS to run the build on (ubuntu, macOS, or Windows). Using the `matrix` keyword is needed when you want to test on multiple OS then you switch the runs-on to use {{matrix.os}}\n</code></pre>"},{"location":"topics/actions/#build-a-docker-image-from-the-artifact-generated","title":"Build a Docker Image from the Artifact Generated","text":"<ul> <li>You define this also within the workflow action as another step</li> </ul> <p><pre><code>    - name: Build and push Docker Image\n        run: You can put all of the commands required to build and then bush the docker image and if you need multiple commands then you use the pipe syntax. ie. |\n        docker login cred\n        docker build\n            - On Ubuntu machines docker is already pre-installed so you don't have to setup docker to use the commands\n            - but instead of using the run command there is an action for the docker build and push and these are found in the *GitHub Actions Marketplace*\n</code></pre> - For credentials you can store them in GitHub as secrets</p>"},{"location":"topics/aiFoundations/","title":"Oracle AI Foundations","text":"<p>The AI stack consists of: Generative AI, Deep Learning, Machine Learning, and Artificial Intelligence</p> <ul> <li> <p>Artificial Intelligence: Programming machines to imitate human intelligence</p> </li> <li> <p>Machine Learning: Subset of AI where alogorithms are used to learn from past data and predict outcomes on new data or identify trends</p> </li> <li> <p>Deep Learning: A subset of machine learning where algorithms are modelled to learn from complex data using neural networks.</p> </li> <li> <p>Generative AI: A type of AI that creates new content</p> </li> </ul>"},{"location":"topics/aiFoundations/#ai-foundations","title":"AI Foundations","text":"<ul> <li>Artificial General Intelligence: Is machines being able to replicate human intelligence capabilities like motor skills, learning, and intelligence. </li> <li> <p>When you apply AGI to specific and narrow objectives then you get Artificial Intelligence</p> </li> <li> <p>2 major reasons why we need AI:</p> </li> <li><code>Automation &amp; Decision Making</code></li> <li> <p><code>Creative Support</code> </p> </li> <li> <p>Commonly Used AI Domains:</p> </li> <li> <p>Language: <code>Text-related AI tasks</code> use text as the input. <code>Generative AI tasks</code> the output text is generated by a model (ChatGPT). </p> <ul> <li>Text as Data: Inherently Sequential = Sentences, multiple words = tokenization, varying sentence legnths = padding, and similair words = dot or cosine similarity and embedding.</li> <li><code>Language AI Models</code> = designed to understand, process, and generate natural language. (NLP)</li> <li>Deep Learning Models that are used for NLP are: Recurrent Neural Networks which process data sequentiall and stores hidden state, Long Short-Term Memory which process data sequentially and can retain the context betther through use of gates, and Transformers which process data in parallel by using concepts of self attention to better understand the context. </li> </ul> </li> <li> <p>Audio &amp; Speech: Can be either <code>Audio-Related</code> or <code>Generative AI</code>. </p> <ul> <li>Audio &amp; Speech as Data: Digitized snapshots in time like a sample rate, sampling rate of 44.1kHz, bit depth is the number of bits in each 44.1kHz of data. </li> <li><code>Audio &amp; Speech AI Models</code> = designed to process and manipulate audio and speech. </li> <li>Deep Learning models are: Recurrent Neural Networks, ==Long Short-Term Memory, and Transformers, Variational Autoencoders, Waveform Models, &amp; Siamese Networks</li> </ul> </li> <li> <p>Vision: Can be <code>Image Related</code> or <code>Generative AI</code>. </p> <ul> <li>Image as Data: Images consist of pixels which can be grey scale or colour. </li> <li><code>Vision AI models</code> = designed to process and understand visual information from images and videos</li> <li>Deep Learning Models: Convolutional Neural Networks which detect patterns in images, learning hierarchial representations of visual features and YOLO which process the image and detects objects within the image, and Generative Adversarial Network which generates real-looking images. </li> </ul> </li> </ul>"},{"location":"topics/aiFoundations/#oci-ai-services","title":"OCI AI Services","text":"<ol> <li><code>Vision AI Services</code> allows us to do the following:</li> <li><code>Image Classification</code>: Upload an image which gets analyzed and labelled with confidence scores. </li> <li><code>Object Detection</code>: Upload the image and then it detects objects with confidence scores.</li> <li><code>Text Detection</code>: upload an image and it extracts all the text from the image. </li> <li> <p><code>Document AI</code>: Upload a document and then it gives you the raw text and then assigns key value pairs and it extracts tables. </p> </li> <li> <p><code>Language AI Services</code>:</p> </li> <li><code>Text Analytics</code>: Analyzes a block of text and provides us language detection, text classfication, extracts entities, key phrase extractions and sentiment analysis. Also personal identifiable information</li> <li><code>Text Translation</code>: Translates text from one language to another. </li> </ol>"},{"location":"topics/aiFoundations/#ai-vs-ml-vs-dl","title":"AI vs ML vs DL","text":"<ul> <li>Machine Learning Types:</li> <li><code>Supervised</code>: Extracting rules from labelled data. For example: Like credit card applications that use a rules engine. <code>Learning from labelled data</code>. </li> <li><code>Unsupervised</code>: Extracting trends from unlabelled data. Grouping similair data into clusters like retail marketing and sales.</li> <li> <p><code>Reinforcement</code>: Solving tasks by trial and error. </p> </li> <li> <p>Deep Learning is used extracting features and rules from data and it uses neural networks with multiple layers. </p> </li> </ul>"},{"location":"topics/aiFoundations/#machine-learning-foundations","title":"Machine Learning Foundations","text":"<ul> <li> <p>ML provides statidstical tools to analyze, visualize, and make predictions from data like Netflix movie suggestions. </p> </li> <li> <p>ML uses <code>input features</code> to describe what the <code>output label</code> should be. Train the model with the input features and then when the model is trained we can apply inference which is the ability to predic the label. </p> </li> <li> <p>Types of Machine Learning:</p> </li> <li>Supervised which uses labeled data, unsupervised where we just understand relationships and reinforcement which make decisions. </li> <li>Supervised examples include: disease detection, weather forecasting, stock price prediction, spam detection</li> <li>Unsupervised examples: Fradulent transactions, outlier detection and targeted marketing campaigns </li> <li>Reinforcement: automated robots, autonomous cars, healthcare and video games</li> </ul>"},{"location":"topics/aiFoundations/#supervised-learning-classification","title":"Supervised Learning: Classification","text":"<ul> <li>There are 2 types of output labels:</li> <li><code>Continous</code> - This leads to <code>Regression</code></li> <li> <p><code>Categorical</code> - This leads to <code>Classification</code> and these can be <code>binary</code> or <code>multi-class</code> (different types of the same thing like 3 different species of a flower)</p> </li> <li> <p>Classification = a supervised mL technique used to categorize or assign data points into predefined classes based on their features or attributes. </p> </li> <li>They train using a labelled data set</li> </ul> <p>Machine Learning Algorithm for classfication is: Logistic Regression: helps in predicting if something is true or false. Logistic regression uses an S-sjaped curve (sigmoid) for the data as opposed to linear regression.</p>"},{"location":"topics/aiFoundations/#supervised-learning-regression","title":"Supervised Learning: Regression","text":"<ul> <li>Independent features are the labeled input and the dependent feature is the output label</li> <li>Uses linear regression </li> <li>loss is a number indicating how far the predicted value is from the actual value</li> </ul>"},{"location":"topics/aiFoundations/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<ul> <li><code>Anaconda</code> is an open source python and R for data science and machine learning. It helps with package management and deployment. Within Anaconda is <code>Jupyter Notebooks</code> which is an IDE that allows you to share documents. </li> <li> <p>This opens up a terminal view on localhost of your files. </p> </li> <li> <p>Machine Level Process consists of: </p> </li> <li>Loading Data</li> <li>Preprocessing - this involves creating features and labels</li> <li>Training a Model</li> <li>Evaluating the Model</li> <li> <p>Making Predictions </p> </li> <li> <p>Important ML Libraries in Python from <code>Sklearn</code></p> </li> <li><code>train_test_split</code> - this module is used to split the data into 2 sets one where you can train the model and other set to test</li> <li><code>StandardScalar</code> - Process of transforming data so it has a mean of 0 and a standard deviation of 1 and it make sures all features use the same scale. if. square foot vs number of beds when predicting the price of a house. </li> <li> <p><code>accuracy_score</code>- gives you an prediction of how strong the prediction is (classification)</p> </li> <li> <p><code>Unsupervised Learning</code>:</p> </li> <li>There are no labelled outputs </li> <li> <p>Algorithms learn the patterns in the data and group similair data items together. </p> </li> <li> <p>Clustering: is the grouping of simialir data items</p> </li> <li> <p>Similarity: is how close two data points are to each other and is a value between 0 and 1 and this determines what cluster objects belong too.</p> </li> <li> <p>Unsupervised Workflow:</p> <ol> <li>Prepare the data (remove missing values and normalize)</li> <li>Create similarity metrics</li> <li>Run the clustering algorithm (parition, density, hierarchial and distribution)</li> <li>Interpret results and adjust clusternig </li> </ol> </li> <li> <p><code>Reinforcement Learning</code>:</p> </li> <li>type of machine learning that enables an agent to learn from its interactions within the enviornment.</li> <li>agent = interacts with the enviornment and takes action and learns from feedback</li> <li>enviornment = external systems with which the agent interacts</li> <li>state = representation of the current situation of the enviornment</li> <li>action = possible moves or decisions that the agent can take</li> <li> <p>policy = mapping that the agent uses to devide which action to take</p> </li> <li> <p><code>optimal policy</code> = finding the policy that yields a lot of rewards. The algorithms used are: Q learning or Deep Q learning</p> </li> </ul>"},{"location":"topics/aiFoundations/#deep-learning-foundations","title":"Deep Learning Foundations","text":"<ul> <li> <p>A subset of machine learning that focuses on training Artificial Neural Networks (ANNs) with multiple layers </p> </li> <li> <p>ML needs us to specify features wheres in <code>Deep Learning extracts features from raw and complex data and DL algorithms allow parallel processing of data so it has better scalability and performance</code>. </p> </li> <li> <p>The use of GPUs were needed for this complex learning and machine algorithms</p> </li> <li> <p>Types of Deep Learning algorithms can be broken down into two types: <code>Data</code> (Images, videos, text, audio) and the <code>applications</code> (image classification, face detection, NLP) </p> </li> <li> <p>Deep Learning Alorithm for <code>images</code> is Convolutional Neural Networks (CNN)</p> </li> <li> <p>For <code>text</code> we use Transformers, Long-Short Term Memory (LSTM) or Recurrent Neural Networks (RNN)</p> </li> <li> <p>For <code>images, audio and text generation</code> we can use: Transformers, Difussion models, and Generative Adversarial networks (GAN)</p> </li> <li> <p>Building Blocks of ANN:</p> </li> <li><code>Layers</code> which are inputs and hidden layers </li> <li><code>Neurons</code> are computantional units that accept input and produce an output and applies ther activation function to generate output</li> <li><code>Weights</code> which determine the strength of connection between neurons</li> <li><code>Activation Function</code> work on the weighted sum of inputs to a neuron to produce an output</li> <li> <p><code>Bias</code>: is additional input to a neuron that allows a certain degree of flexibility </p> </li> <li> <p>ANNs are trained using Backpropgation Algorithm which is guessing and comparing and then measuring the error. Then the weights are adjusted and then weights are updated. Repeat and Learng Model training method</p> </li> <li> <p>Deep Learning Models for <code>Sequence Models</code></p> </li> <li>Sequence models are input data in the form of sequences and the goal is to find patterns and make predictions. Like NLP, speech recognition, gesture recognition, etc</li> </ul> <p>-Recurrent Neural Networks (RNN) handle sequential data and there is a feedback loop and it can maintain a hidden state or memory and it updates as each element in the sequence is processed. So it can capture dependencies        -Architecture:          1. <code>One to One</code>: used for non sequential data          2. <code>One to Many</code>: music generation or sequence generation          3. <code>Many to one</code>: sentiment analysis           4. <code>Many to Many</code>: machine translation and entity recognition. </p> <pre><code>-==**Long Short Term Memory**==: Works by using a specialized memory cell and gating mechanisms to capture long term dependencies which RNN is not good at. \n- Input processing at step 1, then it recieves the previous memory hidden state value then there is a gating mechanism (input gate, forget gate, and output gate) then it updates the memory and then it produced output generation.\n\n-==**Convolutional Neural Networks**==: \n   -Deep Learning Models: \n       1. `Feed Forward Neural Networks (FNN)` - multi layer perception MLP (simplest)\n       2. `Convolutional Neural Networks (CNN)` - good for image and video \n       3. `Recurrent Neural Networks (RNN)` - good  for time series and sequential data\n       4. `Autoencoders` - are unsupervised learning models used for feature extraction.\n       5. `Long Short Term Memory` - specilized RNN for long term dependencies\n       6. Generative Adversarial Network` (GAN) producing images and content \n       7. `Transformers` which is used for language processing\n\n    - CNN: processes grid like data like images and videos. CNN works good with 2D data by reducing images into an easier to process form.\n\n    - CNN Layers:\n       1. Input Layer \n       2. Feature Extraction layers: This layer is to automatically learn and extract patterns from the input images\n          -convolutional layers (uses small filters kernels)\n          -activation function allows the network to learn more complex non linear data\n          -pooling layer reduces computational complexity\n          -fully connected layer\n          -softmax layer\n          -dropout layer\n       3. Classification Layers\n\n    - Limitations of CNN:\n       - Computation: needs alot of data and compute \n       - Overfitting: happens with limited traning data\n       -Interpretiability: black box models\n       -Sensitivity: sensitive to input variations\n\n    - Main use of CNN is image classification, object detection, image segmentation, face recognition.\n</code></pre>"},{"location":"topics/aiFoundations/#generative-ai-llm-foundations","title":"Generative AI &amp; LLM Foundations","text":""},{"location":"topics/aiFoundations/#intro-to-genai","title":"Intro to GenAI","text":"<ul> <li>Subset of Deep learning where the models are trained to generate output on their own.</li> <li>GenAI models learn the underlying patterns in a given data set and uses that knowledge to create a new data set. </li> </ul> <p>-ML identifies patterns to recognize and classify patterns. -<code>inference</code> is the ability to predict based on the training that was done before.  -ML focuses on learning the relationship between data and the label -In GenAI it learns patterns in an unstructured content and it doesn't need labelled data to train.  -The output of ML is a label whereas in GenAI it is New content. </p> <ul> <li>2 types of Gen AI Models</li> <li><code>Text-Based</code>: models generate text, code, dialogue and they learn from large collections of text data</li> <li><code>Multimodal</code>: process multiple modalities like text, images, videos, etc. </li> </ul>"},{"location":"topics/aiFoundations/#intro-to-llm","title":"Intro to LLM","text":"<ul> <li>A language model (LM) is a probalistic model of text. </li> <li>it helps determine what the next word will be in a sentence it gives a probabilitity to every word in its vocab. </li> <li>Large in LLM stands for the nuumber of parameters. -<code>EOS</code> stands for end of sentence or end of sequence. </li> </ul> <p>LLM Features:    - Based on Transformers this allows them to play attention to specific parts and gives them enhanced contextual understanding.     - Deep Neural networks that are trained on a large set of text    - Paremeters are adustable weights in the models neural network.     - Model size is the memory required to store the models parameters</p> <p>Transformers - <code>Recurrent Neural Network</code> handle sequential data like a sentence and they have a feedback loop that allows them to store and maintain a hidden state but RNN has trouble with <code>Long-range dependencies</code>.     - As the length of the sentence grows it leads to <code>Vanishing Gradient</code> which means it loses context of the entire sequence. </p> <ul> <li>Transformers Architecture: </li> <li>They understand the relationship between all the words in a sentence at the same time and understand how they relate to each other. </li> <li> <p>Attention Mechanism (Self Attention) is used by transformers that adds context to the text and this also helps with long range dependencies </p> </li> <li> <p>Transformer has 2 main parts:</p> </li> <li>Encoder: reads the input text aand encodes it into meanings using attention mechanism. Used for semantic search. </li> <li>Decoder uses these embeddings to generate the output text of the next word (token). Decoders only generate a single token at a time. Used for text generation.</li> <li> <p><code>tokens</code>: LLM understand tokens instead of workds. Tokens can be a part of a word, an entire word or a punctuation</p> </li> <li> <p><code>Embeddings</code>: numerical representation of a piece of text converted to number sequences. They can also be used in semantic search in a vector database</p> </li> <li> <p>Words get converted into tokens then into embeddings (vector data)</p> </li> <li> <p><code>Encoder-Decoder</code>: encoder encodes a sequence of words to a set of vectors and the decoder generates the output sequence from the set of vectors. Here the decoder has a self referential loop as it keeps generating all of the tokens in the sequence. <code>Used for machine translation</code></p> </li> </ul>"},{"location":"topics/aiFoundations/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>Prompt: the input or the initial text provided to the model </li> <li>Prompt Engineering: the process of iteratively refining a prompt for the purpose of eliciting a particular style of a response. </li> <li><code>instruction tuning</code> is a critical step in LLM alignment and it involves fine tuning a pre trained LLM on a varied set of instructions, each paired iwth a desired output.</li> <li> <p>Reinforcement Learning From Human Feedback is used to fine tune LLMs to follow a broad class of written instructions. </p> </li> <li> <p><code>In-context Learning</code>: prompting an LLM with instructions and or demonstrations of the task it is meant to complete</p> </li> <li> <p><code>k-shot prompting</code>: explicitly providing k examples of the intended task in the prompt </p> </li> <li> <p><code>Chain of Thought Prompting</code>: provide examples in a prompt to show responses that include a reasoning step and describes the calculation logic to get to the final answer before giving the final answer. </p> </li> <li> <p><code>Hallucination</code>: model generated text that is non factual and ungrounded.     -<code>retrieval-augmentation</code> have less hallucination then 0-shot prompting. </p> </li> </ul>"},{"location":"topics/aiFoundations/#customizing-llms-with-your-data","title":"Customizing LLMs with your Data","text":"<ol> <li><code>Prompt engineering</code> is the easiest to start</li> <li>If you need more context than use <code>Retrieval-Augmented Generation (RAG)</code></li> <li>More instructions require <code>Fine-tuning</code></li> <li> <p>Usually need to use all of them</p> </li> <li> <p>Retrieval Augmented Generation (RAG): languge queries enterprise knowledge bases (databases, wikis, vector database) to provide grounded responses. RAG does not require fine-tuning </p> </li> <li> <p>Augmented Generation = providing a more concrete answer using the acquired information. </p> </li> <li> <p>LLM Fine-tuning &amp; Inference: take pre-trained foundational model and provide additional training using custom data    -<code>inference</code>: model recieves new text as input and generates output based on what it learning during pre-training and fine tuning.     -benefits: model performance on specific tasks, and improve model efficiency </p> </li> <li> <p>Generative AI creates new content without making predictions</p> </li> <li>Sequence models are indeed well-suited for tasks involving sequentially ordered data points or events, such as time series analysis, natural language processing, speech recognition, and language translation. However, for image classification and object recognition, traditional machine learning models and convolutional neural networks (CNNs) are more commonly used.</li> </ol>"},{"location":"topics/aiFoundations/#oci-ai-portfolio","title":"OCI AI Portfolio","text":"<ul> <li>Data -&gt; Infrastructure -&gt; AI Services -&gt; SaaS Apps</li> <li> <p>No infrastructure needs to be managed. </p> </li> <li> <p>Ways to access Oracle Cloud Infra:</p> </li> <li><code>OCI Console</code> is a browser based for all the features needed for data science likes notebook</li> <li><code>Rest API</code> </li> <li><code>Language SDKs</code>: Provides programming language SDKs</li> <li><code>Command Line Interface</code>: provides quick access and full functionality without scripting</li> </ul>"},{"location":"topics/aiFoundations/#overview-of-ai-services","title":"Overview of AI Services:","text":"<ol> <li><code>Language</code>: Text analysis at scale using pretrained models and custom models </li> <li><code>Vision</code>: upload images to detect and classify object using pretained and custom models</li> <li><code>Speech</code>: convert media files into readable text</li> <li><code>Document Understanding</code>: upload documents to detect and classify text.</li> <li><code>Digital Assistant</code>: Platform used to create and deploy digital assistants using natural language conversations.</li> </ol>"},{"location":"topics/aiFoundations/#overview-of-ml-services","title":"Overview of ML Services:","text":"<ul> <li>3 core principles of OCI Data Science:</li> <li><code>Accelerated</code>: allows data scientists to work the way they want without needed to manage the infra</li> <li><code>Collaborative</code>: allows them to work together using <code>Projects</code> (notebook sessions) and uses the <code>Conda</code> enviornments </li> <li> <p><code>Enterprise-Grade</code>: Fully managed infra and updates and security</p> </li> <li> <p>OCI Data science is used to build, train, and deploy ML models and it serves Data Scientists. </p> </li> <li> <p>Accelerated Data Science (ADS) SDK are given to data scientst that give them libraries to help data scientists </p> </li> <li> <p><code>Model Catalog</code> is a centralized repo where model artifacts are stored. </p> </li> <li>Model Deployments (to an HTTP Web app) -&gt; Jobs</li> </ul>"},{"location":"topics/aiFoundations/#ai-infrastructure","title":"AI Infrastructure","text":"<ul> <li><code>GPU</code>: Hardware that performs simple operations and allows many processes to run with parallel computing.</li> </ul>"},{"location":"topics/aiFoundations/#gpu-superclusters-in-oci","title":"GPU &amp; Superclusters in OCI","text":"<ul> <li><code>RDMA</code>: Remote Direct Memory Access - Its a technology that allows for network communication without any cpu interference which allows GPUs to communicate with low latency and this is the core that their database services are built upon</li> <li><code>ROCKY</code> = RDMA converged ethernet </li> <li>The partnered with NVDIA because there is a high demand for high compute GPUs that can run within a single RDMA network (ie. a Supercluster)</li> </ul> <p><code>RDMA Supercluster</code> which is designed to support a large number of GPUs.    -GPU node connects to the network fabrics and any GPU can talk to any GPU through the fabric    - Supercluster = it just means its much larger than a typical cluster and it has 2 blocks. One block uses a <code>Clos Fabric</code> and it uses a 3-tier network using silicon chips and buffers they counteract the latency that might occur with a large number of GPUs supercluster = Lossless    - Using <code>placement</code> they are able to balance scalability and latency    - OCI AI Superclusters are specifically designed to handle demanding AI workloads that require significant computational power and scalability. They are optimized to provide high performance for complex tasks like training large machine learning models, deep learning, and other compute-intensive AI tasks.    - Dedicated AI Clusters provide GPU-based compute resources required to fine-tune a pre-trained model for specific tasks like customer support.</p>"},{"location":"topics/aiFoundations/#responsible-ai","title":"Responsible AI","text":"<ul> <li>Guiding Principles for AI to be trustworthy:</li> <li>AI should follow applicable laws</li> <li>AI should be ethical: Human Ethics and AI Ethics - used to help humans, prevent harm, and fairness and explicable</li> <li>AI should be robust </li> </ul> <p>-Responsible AI Requirements;    1. Set up goverance    2. Develop policies and procedures    3. Ensure compliance  - roles: developers, deployers, and end users </p>"},{"location":"topics/aiFoundations/#oci-generative-ai-services","title":"OCI Generative AI Services","text":"<ul> <li>Fully mangaged service that provides a set of customizable LLM available through APIs</li> <li><code>Choice of Models</code> - high performing models from Meta and Cohere</li> <li><code>Flexible Fine-Tuning</code> to create custom models by fine tuning models using your own data set</li> <li> <p><code>Dedicated AI Clusters</code> that host your workloads</p> </li> <li> <p>2 types of Pretrained Foundational Models:    1.Chat Models: Such as Command-r-plus, command-r-16k, and llamma 3-70b-instruct: Llama is made by meta and the first two by cohere. R plus is more expensive and its used for more complex scenarios. They work by asking questions and get <code>conversational repsonses</code> aka go through <code>instruction tuning</code></p> </li> <li> <p>Embedding Models such as: embed-english-v3.0, embed-multilingual-v3.0: Text converted to <code>vector embeddings</code> used for semantic search and allows for multilingual models.</p> </li> <li> <p>Fine tuning is used when a pretrained model isn't working or if you want to teach it something new.</p> </li> <li><code>T-Few Fine Tuning</code> is what Cohere uses and it enables fast and efficient customizations - it introduces new base layers and only updates a fraction of the model so you dont have to fine tune everything which takes longer and costs more </li> </ul> <p>-Preamble just changs the behaviour of the model but it is not finetuning </p>"},{"location":"topics/aiFoundations/#vector-search","title":"Vector Search","text":"<ul> <li>AI Vector search is built into the <code>Oracle Database 23ai</code></li> <li>Works on structured and unstructured data</li> <li>Uses SQL support for vector generation, Vector Data type, indexes, and uses syntax similair to SQL.</li> <li> <p>Process: Load images as blob -&gt; Vector Embededding -&gt; store in DB -&gt; Vector Search for similair matches </p> </li> <li> <p><code>Vector Datatype</code>: You can use the dimension format or not this is optional (ie. int, float, etc).  The VECTOR datatype in Oracle Database 23ai is specifically designed to store embeddings for AI Vector Search. This datatype allows efficient storage and retrieval of high-dimensional numerical representations of data, enabling similarity searches for AI and machine learning applications.</p> </li> <li><code>Vector Distance Function</code> shows the similairty between vectors. Vectors that have a small distance are more similair. </li> <li><code>Vector Search SQL</code> used to find top k closes matches to a given query item that uses (vector_distance)</li> <li><code>Vector Index</code> are used not only for performance but it also controls the accuracy using the <code>organization</code> and <code>distance</code> parameters. Organization is if it will fit in memory. If it will fit in memory use <code>inmemory neighbour graph</code> and if it doesnt use <code>neighbour partitions</code></li> <li><code>Target accuracy</code> is a clause added to indicate the default accuracy the index should provide for similairty queries </li> <li><code>Approximate</code> keyword indicates that the user wants to perform a similarity search using a vector index. </li> <li>You can also perform similarity search over joins</li> <li>Allows you to efficiently orchestrate Gen-AI pipelines.</li> <li>Model endpoints allow deployed models to be accessed via an API for real-time inference, making them available for AI applications.</li> </ul>"},{"location":"topics/aiFoundations/#select-ai","title":"Select AI","text":"<ul> <li>use your language to query the data (autonomous database) you dont need to know where the data is or how to access the database</li> <li>It takes the natural language question to form a SQL query </li> </ul>"},{"location":"topics/aiFoundations/#oci-ai-services_1","title":"OCI AI Services","text":"<ul> <li> <p><code>OCI Language</code> detects the language of your text, identifies entities in your text, identifies setinment for each aspect of text, identifies key phrases that represent important ideas or subjects, and it classifies general topic from list of 600 categories</p> </li> <li> <p><code>OCI Speech</code> converts speech to text using deep learning techniques. It also uses SRT closed caption support. </p> </li> <li>It also normalizes text to more concise versions of the text. (words to numbers) </li> <li> <p>Has profanity filtering: removing, mask (removes but leaves the first letter) and tags.</p> </li> <li> <p><code>OCI Vision</code> works on images and provides image analysis and document AI    Image Analysis: <code>Object Detection</code> where it detects objects inside an image with a bounding box and a label. It can also detect text. <code>Image Classification</code> labels the scene and you can retrain data for specific needs. </p> </li> <li> <p><code>Document Understanding</code> its used for understanding document images    -features: Text recognition (OCR) from images, document classification based on visual appearance, language detection, table extraction, and key value extraction </p> </li> </ul>"},{"location":"topics/aiFoundations/#additional-notes","title":"Additional Notes","text":"<ul> <li>Model training = establishing a relationship between input and output parameters</li> <li>Gen AI aims to understand underlying data distribution and creates new examples. </li> </ul>"},{"location":"topics/docker/","title":"Docker","text":"<p>Docker is an open platform designed for developing, shipping, and running applications using containerization. At a high level, it enables users to package an application and all its dependencies (code, libraries, system tools, runtime, and configurations) into a standardized unit called a container. </p>"},{"location":"topics/docker/#deployment-before-containers","title":"Deployment before Containers","text":"<ul> <li>Each developer needs to install the services and dependencies locally on their OS on their laptop (Redis, Database, Messaging, etc)</li> <li>Installation on different OS (Mac vs Windows) is different</li> <li>Prone to error as the setup can be difficult especially if your application is complex</li> </ul>"},{"location":"topics/docker/#deployment-with-containers","title":"Deployment with Containers","text":"<ul> <li>With containers like Docker you don't have to install all of the services independently on your OS instead these services come packaged with the application in an isolated enviornment</li> <li>Docker also allows you to run different versions of an application without any conflicts (ie. Redis 4.1, Redis 4.2, Redis 4.3, etc)</li> </ul>"},{"location":"topics/docker/#vm-vs-docker","title":"VM vs Docker","text":"<ul> <li>Docker only contains and deals with the OS application layer</li> <li>VM has the container layer and the OS Kernel</li> <li>As a result Docker packages are a lot smaller since they only have to implement one layer of the OS and they are faster to start up since VMS have to boot up the OS Kernel</li> <li>VMs are compatible with all OS but Docker is only compatible with Linux OS for example if you have Linux Docker file it can run on a Windows machine because the Windows machine has the Windos OS Kernel</li> <li><code>Docker Desktop</code> now lets Linux Containers work with Mac or Windows OS. Docker Desktop uses a hypervisor layer which has the Linux kernel that is needed. </li> </ul>"},{"location":"topics/docker/#docker-images-vs-docker-containers","title":"Docker Images vs Docker Containers","text":"<ul> <li> <p><code>Docker image</code> is the artifact that we create that has the application and all of it envionrment dependencies that we can then upload to an artifact repository so another server/person can download and run it &amp; it is executable</p> </li> <li> <p><code>Docker Container</code> is a running instance of an image. Basically the server or host or machine that runs the image becomes the Docker container              - You can run multiple containers from one image</p> </li> <li> <p><code>docker images</code>: Command to see all the images that you have in your docker instance</p> </li> <li><code>docker ps</code>: See your running containers</li> </ul>"},{"location":"topics/docker/#docker-registry","title":"Docker Registry","text":"<ul> <li>Registries are places that have premade docker images stored</li> <li><code>Docker Hub</code> is Docker's officially registry: https://hub.docker.com/</li> </ul>"},{"location":"topics/docker/#image-versioning","title":"Image Versioning","text":"<ul> <li>New versions of the application require new versions of the docker image and these versions are called Tags</li> <li>All images have a tag called <code>latest</code> which indicates it the newest version of that image. If you don't choose a specific version you get latest by default</li> </ul>"},{"location":"topics/docker/#getting-images","title":"Getting Images","text":"<ol> <li>Search for the image (package) that you need from a Registry</li> <li>Then run <code>docker pull {name}:tag</code></li> <li>You didn't have to specify the registry here because DockerHub is the default location where it searches. </li> </ol>"},{"location":"topics/docker/#running-images","title":"Running Images","text":"<ol> <li>To run images you just need to use this commmand: <code>docker run imageName:tagnNumber</code></li> <li> <p>Running the container using this command runs it in the foreground which means your terminal is now blocked</p> </li> <li> <p>Using <code>-d</code> flag will run the container in the background and it will give you the full process ID</p> </li> <li> <p><code>docker run -d name:tag</code> </p> </li> <li> <p>To see Logs while running containers in the backround: <code>docker logs containerID</code> (you get this from docker ps)</p> </li> <li> <p>You can skip the docker pull command and just run the docker run for an image that you don't have locally as long as that image is found in DockerHub</p> </li> </ol>"},{"location":"topics/docker/#port-binding","title":"Port Binding","text":"<ul> <li>When you run a container like the above without mentioning a port then you can't access it since its running on the enclosed Docker network </li> <li> <p>We need to expose the container port to the host that is running the container </p> </li> <li> <p><code>Port Binding</code>: is when you bind the containers port to the host port so that you can access the container. For example, nginx runs on port 80 and that is standard for this application.</p> </li> <li> <p>Stopping Containers = <code>docker stop containerID</code></p> </li> <li> <p>To bind the port: <code>docker run -d -p hostPort:containerPort name:tag</code></p> </li> <li>the -p is what is used to bind the port and it uses {host port: container port}. Now if you go to localhost for example on the host port you define you will see the running container </li> <li> <p>Only one service can be tied to a port if you try to run another docker run for a different service on the same port you will get an error </p> </li> <li> <p>To stop a container you run: <code>docker stop containerID</code></p> </li> </ul> <p>-You should match the host port to the same one that the container uses</p>"},{"location":"topics/docker/#all-containers","title":"All Containers","text":"<ul> <li>Even though you stopped containers they still exist and <code>docker run</code> does not reuse containers it creates a new one to see a list of all containers running and not running you can run: <code>docker ps -a</code></li> <li> <p>the -a flag means all</p> </li> <li> <p>Restarting a Container that was previously stopped:</p> </li> <li> <p><code>docker start containerID</code></p> </li> <li> <p>Using Container Names - When running these docker commands we reference the docker ID but we can also use the <code>container name</code> which is usually auto-generated by docker but you can change this value:</p> </li> <li>to manually name the container you need to run <code>docker run --name giveName -d -p 8080:80 name:tag</code></li> <li>the --name is what you use to give it a specific name</li> </ul>"},{"location":"topics/docker/#private-docker-registeries","title":"Private Docker Registeries","text":"<ul> <li>These are images that are created in-house by companies which they do not want to share with people outside of the company</li> <li> <p>You need to <code>authenticate</code> before you can access the registry and all of the big cloud providers have their own docker registries</p> </li> <li> <p>Docker Registry versus Docker Repository</p> </li> <li>A registry is a service for providing storage and its a collection of repositories </li> <li>A repository is a collection of related images with the same name but different versions </li> </ul>"},{"location":"topics/docker/#building-custom-docker-images","title":"Building Custom Docker Images","text":"<ul> <li> <p><code>Dockerfile</code> is the file that contains all the commands used to assemble an image from your app</p> </li> <li> <p>In the root of the application create a new file called: <code>Dockerfile</code></p> </li> <li> <p>Anything that you need to run your app is what you also need to include as a dependency or env variable in your dockerfile. For example, you have a backend server file that needs node js for example to run it locally you need to do node server.js then node is something that your app depends on</p> </li> <li> <p>So each dockerfile starts off with a Base Image and you choose the base image based on the tools that your application needs             - node based app you should use a node base image             - python based app you should use a python base image</p> </li> </ul> <p>Structure of a Docker file</p> <ul> <li> <p>You can pull base images from public or private registeries so in this example we are going to use a base image from the Dockerhub and we get base images by using the FROM keyword</p> </li> <li> <p>Basically you need to map all the commands you need to run the app locally in your docker file so that this image when it goes to another OS can also be built successfully. </p> </li> <li>For example: If you have package.json that means you have node packages that need to install so you need to define this in your dockerfile</li> <li>You can run any commands and since alpine is a linux based image you can run linux command but you need to prefix it with the <code>RUN</code> keyword.</li> <li>You also need to copy all of the files that are required to run the application to the container so you need a <code>COPY</code> directive </li> <li>To change the directory of where you want to run commands in your docker container you use <code>WORKDIR</code></li> <li>The last command that is run in a dockerfile uses the command <code>CMD</code> and it uses [] where you provide the command and the parameters</li> </ul>"},{"location":"topics/docker/#from-node19-alpine-copy-packagejson-app-copy-src-app-workdir-app-run-npm-install-cmd-node-serverjs","title":"<pre><code>FROM node:19-alpine\n\nCOPY package.json /app/\nCOPY src /app/\n\nWORKDIR /app\n\nRUN npm install\n\nCMD [\"node\", \"server.js\"]\n</code></pre>","text":""},{"location":"topics/docker/#building-the-docker-image","title":"Building the Docker Image","text":"<ol> <li>Once the dockerfile is complete then inside the project where the dockerfile resides we can build the image:</li> <li>the -t is the tag parameter that lets you name the image using the {name: version}</li> <li>the last parameter is the location of the dockerfile</li> <li>` docker build - t web-app:1.0 . <ul> <li>the dot represents the current folder </li> </ul> </li> </ol>"},{"location":"topics/jenkins/","title":"Jenkins","text":"<p>Automation platform that lets you build, test, and deploy automations using pipelines.</p>"},{"location":"topics/jenkins/#jenkins-infrastructure","title":"Jenkins Infrastructure","text":"<ul> <li>Master Server = Controls the pipelines &amp; schedules builds</li> <li>Agents/Minions Servers = Run the builds </li> </ul>"},{"location":"topics/jenkins/#jenkins-build-types","title":"Jenkins Build Types","text":"<ol> <li>Freestyle Builds - shell scripts that are run on servers based on specific events.</li> <li>Pipeline Builds - Use Jenkins files to define declaritively on how to deploy the build in different stages.</li> </ol>"},{"location":"topics/jenkins/#jenkins-gui","title":"Jenkins GUI","text":"<ul> <li>Manage Jenkins: This contains all of the settings that you need for your Jenins instance such as plugins, global settings, etc.</li> <li><code>System Configuration/Configure System</code></li> <li><code>System Configuration/Manage Plugins</code></li> <li><code>System Configuration/Manage Nodes &amp; Cloud</code>: This is where you setup agents</li> <li><code>Security/Manage Credentials</code>: This where you store <code>SSH keys</code> or <code>API tokens</code></li> <li><code>Tools and Actions/Prepare for Shutdown</code>: You need to use this when you are performing and upgrade or maintanence if you just shutdown the server without doing this step then you will interrupt jobs that are running. </li> </ul>"},{"location":"topics/jenkins/#setting-up-freestyle-jenkins-projects","title":"Setting up Freestyle Jenkins Projects","text":"<ol> <li> <p>Go to the Jenkins Dashboard then click on  New Item. The two most popular types of projects are <code>freestyle</code> and <code>pipelines</code></p> </li> <li> <p>Pick the type of job and give it a name. Make sure to not to put spaces in the name</p> </li> <li> <p>From the build options: </p> <ul> <li> <p><code>Source Code Management</code> is usually always <code>Git</code> and Jenkins will pull that repo that is specified here. You also will mention any branches if you need specific ones. </p> </li> <li> <p>Then we have <code>Build Triggers</code>: You would usually using GitHub webhooks but you need to make sure the firewall or port is open on the Jenkins server so that it can work with the webhook. <code>Build Periodically</code> is used to build jobs on a schedule using cron jobs. </p> </li> <li> <p><code>Build Enviornments</code>: Good to select the 'Delete Workspace' option to clean up any artifacts from previous runs</p> </li> </ul> </li> <li> <p><code>Build</code>:</p> <ul> <li>Most common option is <code>Execute Shell</code></li> </ul> </li> <li> <p><code>Post-Build</code>: Like email notifications</p> </li> <li> <p>After clicking and saving on the build - from the job dashboard you can click <code>Build Now</code></p> </li> </ol> <p></p> <ol> <li> <p>Click on <code>Configure</code> from the Build Homepage to change any settings</p> </li> <li> <p><code>Enviornment Variable</code>:</p> <ul> <li>To see what env variables your build has access to go to <code>Configure</code> on the build and then scroll to Build Steps and check the <code>Execute Shell</code> step and click on \"See the list of available environment variables\"</li> <li><code>Main Enviornment Variables</code>:         - <code>BUILD_ID</code>: Gives you the current build ID and you can use this for docker images         - <code>BUILD_URL</code></li> <li>To see use enviornment variables <code>${VAR_NAME}</code> in your shell script</li> </ul> </li> <li> <p>Reading Console Output:</p> <ul> <li>Any line in the console that is prefixed with a plus sign <code>+</code> means that is a command that is being run</li> </ul> </li> <li> <p><code>Workspace</code>: If there are files that are created and managed by your build they will show up in Jenkins on the Project homepage under the Workspace folder</p> </li> </ol>"},{"location":"topics/jenkins/#jenkins-filesystem","title":"Jenkins Filesystem","text":"<ul> <li><code>cd /var/jenkins_home</code> this will take you to the home directory of Jenkins on the Master Server</li> <li>Within this folder you can navigate to the <code>workspace</code> directory and this is the directory that will contain all of your builds with their job names as the folder name. Within the build folders will be any artifacts of the build.</li> <li>Within the jenkins_home repo also contains other important places to troubleshoot such as:<ul> <li><code>plugins</code>: Contains a list of all the plugins installed on your Jenkins instance</li> <li><code>updates</code>: This contains a list of all the updates that happened</li> <li><code>logs</code>: Houses the log files on the server</li> </ul> </li> </ul>"},{"location":"topics/jenkins/#setting-up-a-python-build-with-a-github-repo","title":"Setting up a Python Build with a GitHub Repo","text":"<ol> <li> <p>Go through the same steps as above when it comes to creating a simple freestyle project but the main thing here is you want to put the GitHub URL in the <code>Source Code Management</code> section. Note: if the repo is private then you would need to add credentials so Jenkins could clone it but if its public then no credentials are needed</p> </li> <li> <p>Since we want to execute a Python build we need to first make sure Python is installed on our master and Jenkins agents. Can do this by remoting into the server and just running the python command in the shell - <code>python or python3</code></p> </li> <li> <p>Then just run the script from the repo <code>python3 script_name.py</code></p> </li> <li> <p>This is valuable because you can run python jobs via Jenkins anytime you want without having to SSH into servers so if you setup a scheduled build from Jenkins thats linked to a Python script from a repo you can run it fairly smooth</p> </li> </ol>"},{"location":"topics/jenkins/#setting-up-jenkins-agentsworkers","title":"Setting up Jenkins Agents/Workers","text":"<ol> <li> <p>Go to <code>Manage Jenkins/Nodes</code> and this where you will see the Jenkins Master and Nodes/Agents that are setup Configure Cloud is how you build out cloud agents like Docker to use instead of pernanet ones.</p> </li> <li> <p>To create a <code>Docker</code> agent go to <code>Cloud</code> then go to install plugins which will automatically filter <code>Cloud Providers</code> and install Docker then restart Jenkins</p> </li> <li> <p>You can login to the <code>Master Jenkins Node</code> and go to the logs and plugins folder to also verify if the plugin is installed. Also, refreshing the Jenkins page might need to be done as the UI will hang on the refresh portion after installing the plugin</p> </li> <li> <p>Now you can go back to the <code>Configure Cloud</code> option under nodes and add Docker.</p> </li> <li> <p>Setting up Docker:</p> <ul> <li>Need to provide the <code>Docker Host URI</code> which can be another server or if you want to do it locally you can use Docker Desktop and an alpine image in the URI field need to put in tcp://IPAddress that is generated   After entering in the data <code>set it to enabled</code> then <code>test connection</code></li> </ul> </li> <li> <p>Creating the <code>Docker Agent Template</code>:</p> <ul> <li> <p>Go back into the configured Docker agent and then click on the gear icon and then navigate to <code>Docker Agent Templates</code> and then click on Add</p> </li> <li> <p>Key Terms:         - <code>Labels</code>: This is used to help the Master node determine which agent to send the build too.         - <code>Docker Image</code>: This will be the official image that you will be using.         - <code>Instance Capacity</code>: This defines how much instances of the agent will be created.         - <code>Remote File System Root</code>: This defines where the workspace for this agent will be created - default: <code>/home/jenkins</code></p> </li> </ul> </li> </ol>"},{"location":"topics/jenkins/#configuring-jobs-to-agents","title":"Configuring Jobs to Agents","text":"<ol> <li> <p>Go to the Job Name then click on \"Configure\" then under General click on the check box for <code>Restrict where this project can run</code> and now put in the name of the docker agent template then click save</p> </li> <li> <p>Now when you build the job it will use the specific agent that you assigned the docker-alpine label too. Note: It's important that you use the correct image because outdated ones will keep your job in pending state as it won't be able to find a live agent since an incorrect image will lead to provisioning errors</p> </li> <li> <p>For these Docker Jobs on the same screen of the Console Output you will see a tab called <code>Built on Docker</code> which shows the container details. </p> </li> <li> <p>This helps you troubleshoot because some agents might not have the required software like Python3 so if you assign this agent template that does not have python to a build that requires it then it will error out. In this scenario you should create your own Docker Image that has python installed</p> </li> <li> <p>To create another agent you just need to go back to the <code>Docker Agent Template</code> and then create another template.</p> </li> </ol>"},{"location":"topics/jenkins/#_1","title":"Jenkins","text":""},{"location":"topics/jenkins/#adding-jenkins-triggers","title":"Adding Jenkins Triggers","text":"<ol> <li>Build Triggers/<code>Poll SCM</code>: Jenkins Master will periodically check github for any changes and its much easier to manage then setting up webhooks.<ul> <li>It uses cron notation </li> </ul> </li> </ol>"},{"location":"topics/jenkins/#setting-up-jenkin-pipelines","title":"Setting up Jenkin Pipelines","text":"<ol> <li> <p>Similair to how we created freestyle projects you need to go to <code>New Item</code> and then click on <code>Pipeline</code>. You will notice that at the top most of the settings are the same but you have less freedom with pipelines for advanced settings as most of the steps are carried out by the <code>Pipeline Script</code> section.</p> </li> <li> <p>There are two ways to build out the pipeline script and both ways use the <code>Groovy Syntax</code>:</p> <ul> <li><code>Directly in the UI</code></li> <li><code>Jenkinsfile</code></li> </ul> </li> </ol>"},{"location":"topics/jenkins/#jenkin-pipeline-syntax","title":"Jenkin Pipeline Syntax","text":"<ul> <li>Everything is wrapped in a <code>pipeline</code> parameter {}</li> <li>First step in the pipeline is to select the <code>agent</code> that will carry out the job and it is specified by the <code>label</code> parameter</li> <li>Next step is the <code>Stages</code>: This is where you define your stages like building -&gt; testing -&gt; deploying</li> <li>When you create a Pipeline Build you can see a section for Pipleline Overview which will show you all of the stages you built. </li> </ul>"},{"location":"topics/jenkins/#jenkinsfile","title":"Jenkinsfile","text":"<ul> <li> <p>Instead of putting the script directly into the UI of Jenkins you can create a <code>Jenkinsfile</code> within the parent folder of your code repository and then outline all of the deployment steps in this special file</p> </li> <li> <p>Within the Pipeline build in Jenkins, you need to change the pipeline from using Pipeline Script to use <code>Pipeline Script from SCM</code> then add the github repo. Make sure you put the <code>Jenkinsfile</code> path in the <code>Script Path</code></p> </li> <li> <p>When you get the pipeline from SCM it will add a first step where it sees if it can checkout from SCM without any errors. </p> </li> </ul> Standardized Jenkinsfile<pre><code>pipeline {\n    agent any  // Use any available Jenkins agent/node\n\n    // Parameters allow customization without changing the Jenkinsfile\n    parameters {\n        string(name: 'GIT_REPO', defaultValue: 'https://github.com/your-org/your-repo.git', description: 'GitHub repository URL')\n        string(name: 'GIT_BRANCH', defaultValue: 'develop', description: 'Git branch to build')\n        string(name: 'GITHUB_CREDENTIALS_ID', defaultValue: 'github-credentials-id', description: 'Jenkins credentials ID for GitHub access')\n        string(name: 'SONARQUBE_PROJECT_KEY', defaultValue: 'my-app', description: 'SonarQube project key')\n        string(name: 'SONARQUBE_TOKEN', defaultValue: 'sonarqube-token-id', description: 'Jenkins credentials ID for SonarQube token')\n        choice(name: 'BUILD_ENV', choices: ['dev', 'qa', 'prod'], description: 'Deployment environment')\n    }\n\n    environment {\n        // Common environment variables for builds\n        NODE_VERSION = '18'\n        JAVA_VERSION = '17'\n        MAVEN_HOME = tool name: 'Maven 3', type: 'maven'\n        // This assumes Jenkins SonarQube plugin is configured as \"sonarqube\"\n        SONARQUBE_ENV = 'sonarqube'\n    }\n\n    triggers {\n        // Automatically build when there are new commits in the repo\n        pollSCM('H/5 * * * *') // Every 5 minutes\n    }\n\n    stages {\n        stage('Checkout') {\n            steps {\n                // Clone code from GitHub using stored credentials\n                git branch: \"${params.GIT_BRANCH}\",\n                    credentialsId: \"${params.GITHUB_CREDENTIALS_ID}\",\n                    url: \"${params.GIT_REPO}\"\n            }\n        }\n\n        stage('Set Up Node') {\n            steps {\n                // Install Node version for frontend builds\n                sh \"\"\"\n                    echo \"Setting up Node.js ${NODE_VERSION}\"\n                    nvm install ${NODE_VERSION}\n                    nvm use ${NODE_VERSION}\n                \"\"\"\n            }\n        }\n\n        stage('Set Up Java') {\n            steps {\n                // Configure Java for backend builds\n                sh \"java -version\"\n                sh \"javac -version\"\n            }\n        }\n\n        stage('Install Frontend Dependencies') {\n            when { expression { fileExists('frontend/package.json') } }\n            steps {\n                dir('frontend') {\n                    sh \"npm install\"\n                }\n            }\n        }\n\n        stage('Build Backend') {\n            when { expression { fileExists('backend/pom.xml') } }\n            steps {\n                dir('backend') {\n                    sh \"${MAVEN_HOME}/bin/mvn clean install -DskipTests\"\n                }\n            }\n        }\n\n        stage('Run Unit Tests') {\n            parallel {\n                stage('Frontend Tests') {\n                    when { expression { fileExists('frontend/package.json') } }\n                    steps {\n                        dir('frontend') {\n                            sh \"npm test\"\n                        }\n                    }\n                }\n                stage('Backend Tests') {\n                    when { expression { fileExists('backend/pom.xml') } }\n                    steps {\n                        dir('backend') {\n                            sh \"${MAVEN_HOME}/bin/mvn test\"\n                        }\n                    }\n                }\n            }\n        }\n\n        stage('SonarQube Analysis') {\n            steps {\n                withSonarQubeEnv(\"${SONARQUBE_ENV}\") {\n                    withCredentials([string(credentialsId: \"${params.SONARQUBE_TOKEN}\", variable: 'SONAR_TOKEN')]) {\n                        dir('backend') {\n                            // You can run separate Sonar scans for frontend/backend if needed\n                            sh \"\"\"\n                                ${MAVEN_HOME}/bin/mvn sonar:sonar \\\n                                    -Dsonar.projectKey=${params.SONARQUBE_PROJECT_KEY} \\\n                                    -Dsonar.login=$SONAR_TOKEN\n                            \"\"\"\n                        }\n                    }\n                }\n            }\n        }\n\n        stage('Conditional Deployment') {\n            when {\n                anyOf {\n                    branch pattern: \"feature/.*\", comparator: \"REGEXP\"\n                    branch pattern: \"hotfix/.*\", comparator: \"REGEXP\"\n                    branch 'main'\n                    branch 'develop'\n                }\n            }\n            steps {\n                sh \"\"\"\n                    echo \"Deploying to ${params.BUILD_ENV} environment\"\n                    # Add deployment scripts or commands here\n                \"\"\"\n            }\n        }\n    }\n\n    post {\n        success {\n            echo \"Pipeline completed successfully.\"\n        }\n        failure {\n            echo \"Pipeline failed. Check logs for details.\"\n        }\n        always {\n            cleanWs() // Clean workspace after build\n        }\n    }\n}\n</code></pre>"},{"location":"topics/jenkins/#settings-up-jenkins-agents-on-windows-server","title":"Settings up Jenkins Agents on Windows Server","text":"<ul> <li>Need to make sure the version of Java that you install on the Windows Server is the same one that is installed on the Jenkins Controller</li> <li>To find the version, go to <code>Manage Jenkins</code> then go to <code>System Information</code> and search for java.home</li> <li> <p>Adding these Nodes:</p> <ol> <li>Click on Manage Configurations and then go to Nodes and create a new node</li> <li>Provide a name and then click on <code>Permanent</code> this it is not a cloud or dynamic server.</li> <li>The remote root directory will be a path on the server: <code>d:\\tools\\jenkins-agent</code></li> <li>If you leave the <code>label</code> blank it will take on the name of the agent.</li> <li>Usage needs to be set as only when using the label name</li> <li> <p><code>Websocket</code> If you don't select this option then you need to just make sure you open a specific port on the agent so that it can communicate with the master via <code>tcp</code>. When you save this in the Jenkins Master it will save the connection type but will be marked with a red X because you need to connect the agent to the master. If you click on the red X it will give you commands to run on the agent</p> </li> <li> <p>For exectuable services you can leverage <code>WinSW</code> which is a wrapper for any executable so that it can be run as a Windows Service</p> <ul> <li>The way this works is you rename the WinSW ex as your jenkins agent and then need an xml file that defines what it runs</li> <li>The arguments in the xml come from the Jenkins UI when you click on the red X</li> <li>The agent.jar also comes from this locaton after you click on the red X</li> <li>After you start the service then it will connect to the master</li> </ul> </li> </ol> </li> </ul>"},{"location":"topics/jenkins/#jenkins-multibranch-pipelines","title":"Jenkins Multibranch Pipelines","text":"<ol> <li> <p>Creating a <code>GitHub App</code>: Go to GitHub and click on Settings then go to Developer Settings then create the GitHub App and Set the permissions. Then set the subscribe events like push, repository, and other events.</p> </li> <li> <p>To add credentials in Jenkins: Click on <code>Manage Jenkins\\Credentials</code> then click on the Global one and then add credential and add for a GitHub app. PAT (personal access token) gives you a lot less GitHub limits whereas a GitHub app lets you call to GitHub a lot more</p> </li> <li> <p>Create a new item and select <code>Multi-Branch Pipeline</code>:</p> <ul> <li>For branch sources select GitHub</li> <li>If you dont have a Jenkinsfile then there will be no build configuration since you are selecting builds based on this type of file</li> </ul> </li> <li> <p>The way the multibranch pipeline works is that Jenkins scans the repo for every branch name and allows you to see that in the app homepage view:</p> <ul> <li><code>On Different Branches you can modify the Jenkinsfile so that different things can be carried out</code></li> </ul> </li> </ol> <p>For Example, you can create stages in the pipeline based on a specific type of branch <pre><code>stage('fix branch){\n  when{\n    branch \"fix-*\"\n  }\n  steps{\n    sh ```\n    cat README.md\n    ```\n  }\n}\n\nstage('merge pr){\n  when{\n    branch \"pr-*\"\n  }\n  steps{\n    echo \"this is for prs\"\n  }\n}\n</code></pre></p> <ol> <li><code>Dealing with Pull Requests</code>: Now you need to create a PR to merge this branch back into the <code>main</code> which will in this scenario also update the root jenkins file so going forward each type of branch will have a specific pathway<ul> <li>Pull Requests also show up in the Jenkins build page for the multibranch pipeline</li> <li>Any PR or branch that has a strikethough means that it was already <code>merged and then the branch deleted</code> so it no longer exists These go away next time a scan occurs</li> </ul> </li> </ol>"}]}